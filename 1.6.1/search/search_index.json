{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to Fides Fides [pronounced /fee-dhez/ , from Latin: Fid\u0113s] is an open-source privacy as code (PaC) tool by Ethyca that allows you to easily declare your systems' privacy characteristics, track privacy related changes to systems & data in version control, and enforce policies in both your source code and your runtime infrastructure. This includes support for major privacy regulations (e.g. GDPR , CCPA and LGPD ) and standards like ISO 19944 by default. Fides can manage both enforcement of privacy in your CI pipeline and orchestration of data privacy requests in your runtime environment. Why is it called Fides? Fides was the goddess of trust and good faith in Roman paganism. Fides represented everything that was required for \"honor and credibility\" in every aspect of Roman life. In addition to this, Fides means \"reliability\": reliability between two parties, which is always reciprocal . As we considered naming conventions, Fides stood out for her embodiment of this project's philosophy - to provide developers with a powerful tool to make privacy a default feature of any software. If you'd like a brief Roman mythology lesson, check out Fides on Wikipedia . Key Features Privacy as Code You describe your datasets and code using Fides' high-level description language in human-readable, declarative manifest files. This allows you to create a consistent, versioned definition of privacy characteristics in your code to automate reporting, evaluate risk and execute policies against. Automated Privacy Checks Fides integrates with git using the fidesctl tool to allow you to automate privacy checks in your CI pipeline and evalute changes against your privacy policies on each commit. This allows you to review changes and assure they meet your privacy policies before deployment. Support all Privacy Standards Fides ships with a comprehensive taxonomy that allows you to efficiently describe the privacy behaviors of your system for major regulations, including GDPR , CCPA and LGPD as well as major standards like ISO 19944 . Extensible Taxonomy Fides' taxonomy can be easily extended, allowing teams to add support for system specific concepts or data types while inheriting concepts to ensure compliance with global privacy regulations. Automate Privacy Reporting Fides' declarations can be configurd to automatically generate privacy review reports suitable for privacy and legal team review. This allows developers to focus on implementation while providing privacy teams with greater insight into the software's behavior. Data Privacy Rights Automation Fides' data orchestration capabilities mean you can use declarations to generate complex data rights automated processes that execute automatically against user's privacy rights requests. This allows you to easily configure automated, API driven privacy requests for access, erasure and de-identification of data. Next Steps To start learning how Fides works, visit the Tutorial page to walkthrough using the taxonomy, annotating datasets and systems, writing and evaluating policies, and more. Welcome!","title":"What is Fides?"},{"location":"#introduction-to-fides","text":"Fides [pronounced /fee-dhez/ , from Latin: Fid\u0113s] is an open-source privacy as code (PaC) tool by Ethyca that allows you to easily declare your systems' privacy characteristics, track privacy related changes to systems & data in version control, and enforce policies in both your source code and your runtime infrastructure. This includes support for major privacy regulations (e.g. GDPR , CCPA and LGPD ) and standards like ISO 19944 by default. Fides can manage both enforcement of privacy in your CI pipeline and orchestration of data privacy requests in your runtime environment.","title":"Introduction to Fides"},{"location":"#why-is-it-called-fides","text":"Fides was the goddess of trust and good faith in Roman paganism. Fides represented everything that was required for \"honor and credibility\" in every aspect of Roman life. In addition to this, Fides means \"reliability\": reliability between two parties, which is always reciprocal . As we considered naming conventions, Fides stood out for her embodiment of this project's philosophy - to provide developers with a powerful tool to make privacy a default feature of any software. If you'd like a brief Roman mythology lesson, check out Fides on Wikipedia .","title":"Why is it called Fides?"},{"location":"#key-features","text":"","title":"Key Features"},{"location":"#privacy-as-code","text":"You describe your datasets and code using Fides' high-level description language in human-readable, declarative manifest files. This allows you to create a consistent, versioned definition of privacy characteristics in your code to automate reporting, evaluate risk and execute policies against.","title":"Privacy as Code"},{"location":"#automated-privacy-checks","text":"Fides integrates with git using the fidesctl tool to allow you to automate privacy checks in your CI pipeline and evalute changes against your privacy policies on each commit. This allows you to review changes and assure they meet your privacy policies before deployment.","title":"Automated Privacy Checks"},{"location":"#support-all-privacy-standards","text":"Fides ships with a comprehensive taxonomy that allows you to efficiently describe the privacy behaviors of your system for major regulations, including GDPR , CCPA and LGPD as well as major standards like ISO 19944 .","title":"Support all Privacy Standards"},{"location":"#extensible-taxonomy","text":"Fides' taxonomy can be easily extended, allowing teams to add support for system specific concepts or data types while inheriting concepts to ensure compliance with global privacy regulations.","title":"Extensible Taxonomy"},{"location":"#automate-privacy-reporting","text":"Fides' declarations can be configurd to automatically generate privacy review reports suitable for privacy and legal team review. This allows developers to focus on implementation while providing privacy teams with greater insight into the software's behavior.","title":"Automate Privacy Reporting"},{"location":"#data-privacy-rights-automation","text":"Fides' data orchestration capabilities mean you can use declarations to generate complex data rights automated processes that execute automatically against user's privacy rights requests. This allows you to easily configure automated, API driven privacy requests for access, erasure and de-identification of data.","title":"Data Privacy Rights Automation"},{"location":"#next-steps","text":"To start learning how Fides works, visit the Tutorial page to walkthrough using the taxonomy, annotating datasets and systems, writing and evaluating policies, and more. Welcome!","title":"Next Steps"},{"location":"cli/","text":"CLI These docs reflect the latest PyPI release. fidesctl The parent group for the Fidesctl CLI. Usage: 1 fidesctl [OPTIONS] COMMAND [ARGS]... Options: 1 2 3 4 5 6 7 8 9 10 --version Show the version and exit. -f, --config-path TEXT Path to a configuration file. Use 'fidesctl view- config' to print the config. Not compatible with the 'fidesctl webserver' subcommand. --local Run in 'local_mode'. This mode doesn't make API calls and can be used without the API server/database. --help Show this message and exit. fidesctl annotate Annotate fidesctl resource types Usage: 1 fidesctl annotate [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit. fidesctl annotate dataset Guided flow for annotating datasets. The dataset file will be edited in-place. Usage: 1 fidesctl annotate dataset [OPTIONS] INPUT_FILENAME Options: 1 2 3 -a, --all-members Annotate all dataset members, not just fields -v, --validate Strictly validate annotation inputs. --help Show this message and exit. fidesctl apply Validate local manifest files and persist any changes via the API server. Usage: 1 fidesctl apply [OPTIONS] [MANIFESTS_DIR] Options: 1 2 3 4 5 --dry Prevent the persistance of any changes. --diff Include any changes between server and local resources in the command output --help Show this message and exit. fidesctl db Database utility commands Usage: 1 fidesctl db [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit. fidesctl db init Initialize the Fidesctl database. Usage: 1 fidesctl db init [OPTIONS] Options: 1 --help Show this message and exit. fidesctl db reset Wipes all user-created data and resets the database back to its freshly initialized state. Usage: 1 fidesctl db reset [OPTIONS] Options: 1 2 -y, --yes Automatically responds 'yes' to any prompts. --help Show this message and exit. fidesctl delete Delete a resource on the server. Usage: 1 2 3 fidesctl delete [OPTIONS] [data_category|data_qualifier|data_subject|data_use| dataset|organization|policy|registry|system|evaluation] FIDES_KEY Options: 1 --help Show this message and exit. fidesctl evaluate Compare your System's Privacy Declarations with your Organization's Policy Rules. All local resources are applied to the server before evaluation. If your policy evaluation fails, it is expected that you will need to either adjust your Privacy Declarations, Datasets, or Policies before trying again. Usage: 1 fidesctl evaluate [OPTIONS] [MANIFESTS_DIR] Options: 1 2 3 4 5 6 7 8 -k, --fides-key TEXT The fides_key of the single policy that you wish to evaluate. -m, --message TEXT A message that you can supply to describe the context of this evaluation. --dry Prevent the persistance of any changes. --help Show this message and exit. fidesctl export Export fidesctl resource types Usage: 1 fidesctl export [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit. fidesctl export datamap Export a formatted data map to excel using template The csv flag can be used to output data as csv instead Usage: 1 fidesctl export datamap [OPTIONS] [MANIFESTS_DIR] Options: 1 2 3 --dry Prevent the persistance of any changes. --csv Export using csv format --help Show this message and exit. fidesctl export dataset Export a dataset in a data map format. Usage: 1 fidesctl export dataset [OPTIONS] [MANIFESTS_DIR] Options: 1 2 --dry Prevent the persistance of any changes. --help Show this message and exit. fidesctl export organization Export an organization in a data map format. Usage: 1 fidesctl export organization [OPTIONS] [MANIFESTS_DIR] Options: 1 2 --dry Prevent the persistance of any changes. --help Show this message and exit. fidesctl export system Export a system in a data map format. Usage: 1 fidesctl export system [OPTIONS] [MANIFESTS_DIR] Options: 1 2 --dry Prevent the persistance of any changes. --help Show this message and exit. fidesctl generate Generate fidesctl resource types Usage: 1 fidesctl generate [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit. fidesctl generate dataset Generate fidesctl Dataset resources Usage: 1 fidesctl generate dataset [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit. fidesctl generate dataset db Connect to a database directly via a SQLAlchemy-style connection string and generate a dataset manifest file that consists of every schema/table/field. This is a one-time operation that does not track the state of the database. It will need to be run again if the database schema changes. Usage: 1 fidesctl generate dataset db [OPTIONS] CONNECTION_STRING OUTPUT_FILENAME Options: 1 2 --include-null Includes attributes that would otherwise be null. --help Show this message and exit. fidesctl generate dataset okta Generates datasets for your Okta applications. Connect to an Okta admin account by providing an organization url. Auth token can be supplied by setting the environment variable OKTA_CLIENT_TOKEN. This is a one-time operation that does not track the state of the okta resources. It will need to be run again if the tracked resources change. Usage: 1 fidesctl generate dataset okta [OPTIONS] ORG_URL OUTPUT_FILENAME Options: 1 2 --include-null Includes attributes that would otherwise be null. --help Show this message and exit. fidesctl generate system Generate fidesctl System resources Usage: 1 fidesctl generate system [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit. fidesctl generate system aws Connect to an aws account by leveraging a boto3 environment variable configuration and generate a system manifest file that consists of every tracked resource. Tracked resources: [Redshift, RDS] This is a one-time operation that does not track the state of the aws resources. It will need to be run again if the tracked resources change. Usage: 1 fidesctl generate system aws [OPTIONS] OUTPUT_FILENAME Options: 1 2 3 --include-null Includes attributes that would otherwise be null. -o, --organization TEXT --help Show this message and exit. fidesctl get View a resource from the server as a JSON object. Usage: 1 2 fidesctl get [OPTIONS] [data_category|data_qualifier|data_subject|data_use|dat aset|organization|policy|registry|system|evaluation] FIDES_KEY Options: 1 --help Show this message and exit. fidesctl init Initializes a Fidesctl instance, creating the default directory ( .fides/ ) and the configuration file ( fidesctl.toml ) if necessary. Additionally, requests the ability to respectfully collect anonymous usage data. Usage: 1 fidesctl init [OPTIONS] [FIDES_DIRECTORY_LOCATION] Options: 1 --help Show this message and exit. fidesctl ls Get a list of all resources of this type from the server and display them as JSON. Usage: 1 2 fidesctl ls [OPTIONS] [data_category|data_qualifier|data_subject|data_use|data set|organization|policy|registry|system|evaluation] Options: 1 --help Show this message and exit. fidesctl parse Reads the resource files that are stored in MANIFESTS_DIR and its subdirectories to verify the validity of all manifest files. If the taxonomy is invalid, this command prints the error messages and triggers a non-zero exit code. Usage: 1 fidesctl parse [OPTIONS] [MANIFESTS_DIR] Options: 1 2 -v, --verbose Enable verbose output. --help Show this message and exit. fidesctl scan Scan external resource coverage against fidesctl resources Usage: 1 fidesctl scan [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit. fidesctl scan dataset Scan fidesctl Dataset resources Usage: 1 fidesctl scan dataset [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit. fidesctl scan dataset db Connect to a database directly via a SQLAlchemy-style connection string and compare the database objects to existing datasets. If there are fields within the database that aren't listed and categorized within one of the datasets, this counts as lacking coverage. Outputs missing fields and has a non-zero exit if coverage is under the stated threshold. Usage: 1 fidesctl scan dataset db [OPTIONS] CONNECTION_STRING [MANIFESTS_DIR] Options: 1 2 -c, --coverage-threshold INTEGER RANGE --help Show this message and exit. fidesctl scan dataset okta Scans your existing datasets and compares them to found Okta applications. Connect to an Okta admin account by providing an organization url. Auth token can be supplied by setting the environment variable OKTA_CLIENT_TOKEN. Outputs missing resources and has a non-zero exit if coverage is under the stated threshold. Usage: 1 fidesctl scan dataset okta [OPTIONS] ORG_URL [MANIFESTS_DIR] Options: 1 2 -c, --coverage-threshold INTEGER RANGE --help Show this message and exit. fidesctl scan system Scan fidesctl System resources Usage: 1 fidesctl scan system [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit. fidesctl scan system aws Connect to an aws account by leveraging a valid boto3 environment varible configuration and compares tracked resources to existing systems. Tracked resources: [Redshift, RDS] Outputs missing resources and has a non-zero exit if coverage is under the stated threshold. Usage: 1 fidesctl scan system aws [OPTIONS] [MANIFESTS_DIR] Options: 1 2 3 -o, --organization TEXT -c, --coverage-threshold INTEGER RANGE --help Show this message and exit. fidesctl status Sends a request to the Fidesctl API healthcheck endpoint and prints the response. Usage: 1 fidesctl status [OPTIONS] Options: 1 --help Show this message and exit. fidesctl view View various resources types. Usage: 1 fidesctl view [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit. fidesctl view config Prints the fidesctl configuration values. Usage: 1 fidesctl view config [OPTIONS] Options: 1 2 --exclude-unset Only print configuration values explicitly set by the user. --help Show this message and exit. fidesctl webserver Starts the fidesctl API server using Uvicorn on port 8080. Usage: 1 fidesctl webserver [OPTIONS] Options: 1 --help Show this message and exit.","title":"CLI"},{"location":"cli/#cli","text":"These docs reflect the latest PyPI release.","title":"CLI"},{"location":"cli/#fidesctl","text":"The parent group for the Fidesctl CLI. Usage: 1 fidesctl [OPTIONS] COMMAND [ARGS]... Options: 1 2 3 4 5 6 7 8 9 10 --version Show the version and exit. -f, --config-path TEXT Path to a configuration file. Use 'fidesctl view- config' to print the config. Not compatible with the 'fidesctl webserver' subcommand. --local Run in 'local_mode'. This mode doesn't make API calls and can be used without the API server/database. --help Show this message and exit.","title":"fidesctl"},{"location":"cli/#fidesctl-annotate","text":"Annotate fidesctl resource types Usage: 1 fidesctl annotate [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit.","title":"annotate"},{"location":"cli/#fidesctl-annotate-dataset","text":"Guided flow for annotating datasets. The dataset file will be edited in-place. Usage: 1 fidesctl annotate dataset [OPTIONS] INPUT_FILENAME Options: 1 2 3 -a, --all-members Annotate all dataset members, not just fields -v, --validate Strictly validate annotation inputs. --help Show this message and exit.","title":"dataset"},{"location":"cli/#fidesctl-apply","text":"Validate local manifest files and persist any changes via the API server. Usage: 1 fidesctl apply [OPTIONS] [MANIFESTS_DIR] Options: 1 2 3 4 5 --dry Prevent the persistance of any changes. --diff Include any changes between server and local resources in the command output --help Show this message and exit.","title":"apply"},{"location":"cli/#fidesctl-db","text":"Database utility commands Usage: 1 fidesctl db [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit.","title":"db"},{"location":"cli/#fidesctl-db-init","text":"Initialize the Fidesctl database. Usage: 1 fidesctl db init [OPTIONS] Options: 1 --help Show this message and exit.","title":"init"},{"location":"cli/#fidesctl-db-reset","text":"Wipes all user-created data and resets the database back to its freshly initialized state. Usage: 1 fidesctl db reset [OPTIONS] Options: 1 2 -y, --yes Automatically responds 'yes' to any prompts. --help Show this message and exit.","title":"reset"},{"location":"cli/#fidesctl-delete","text":"Delete a resource on the server. Usage: 1 2 3 fidesctl delete [OPTIONS] [data_category|data_qualifier|data_subject|data_use| dataset|organization|policy|registry|system|evaluation] FIDES_KEY Options: 1 --help Show this message and exit.","title":"delete"},{"location":"cli/#fidesctl-evaluate","text":"Compare your System's Privacy Declarations with your Organization's Policy Rules. All local resources are applied to the server before evaluation. If your policy evaluation fails, it is expected that you will need to either adjust your Privacy Declarations, Datasets, or Policies before trying again. Usage: 1 fidesctl evaluate [OPTIONS] [MANIFESTS_DIR] Options: 1 2 3 4 5 6 7 8 -k, --fides-key TEXT The fides_key of the single policy that you wish to evaluate. -m, --message TEXT A message that you can supply to describe the context of this evaluation. --dry Prevent the persistance of any changes. --help Show this message and exit.","title":"evaluate"},{"location":"cli/#fidesctl-export","text":"Export fidesctl resource types Usage: 1 fidesctl export [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit.","title":"export"},{"location":"cli/#fidesctl-export-datamap","text":"Export a formatted data map to excel using template The csv flag can be used to output data as csv instead Usage: 1 fidesctl export datamap [OPTIONS] [MANIFESTS_DIR] Options: 1 2 3 --dry Prevent the persistance of any changes. --csv Export using csv format --help Show this message and exit.","title":"datamap"},{"location":"cli/#fidesctl-export-dataset","text":"Export a dataset in a data map format. Usage: 1 fidesctl export dataset [OPTIONS] [MANIFESTS_DIR] Options: 1 2 --dry Prevent the persistance of any changes. --help Show this message and exit.","title":"dataset"},{"location":"cli/#fidesctl-export-organization","text":"Export an organization in a data map format. Usage: 1 fidesctl export organization [OPTIONS] [MANIFESTS_DIR] Options: 1 2 --dry Prevent the persistance of any changes. --help Show this message and exit.","title":"organization"},{"location":"cli/#fidesctl-export-system","text":"Export a system in a data map format. Usage: 1 fidesctl export system [OPTIONS] [MANIFESTS_DIR] Options: 1 2 --dry Prevent the persistance of any changes. --help Show this message and exit.","title":"system"},{"location":"cli/#fidesctl-generate","text":"Generate fidesctl resource types Usage: 1 fidesctl generate [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit.","title":"generate"},{"location":"cli/#fidesctl-generate-dataset","text":"Generate fidesctl Dataset resources Usage: 1 fidesctl generate dataset [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit.","title":"dataset"},{"location":"cli/#fidesctl-generate-dataset-db","text":"Connect to a database directly via a SQLAlchemy-style connection string and generate a dataset manifest file that consists of every schema/table/field. This is a one-time operation that does not track the state of the database. It will need to be run again if the database schema changes. Usage: 1 fidesctl generate dataset db [OPTIONS] CONNECTION_STRING OUTPUT_FILENAME Options: 1 2 --include-null Includes attributes that would otherwise be null. --help Show this message and exit.","title":"db"},{"location":"cli/#fidesctl-generate-dataset-okta","text":"Generates datasets for your Okta applications. Connect to an Okta admin account by providing an organization url. Auth token can be supplied by setting the environment variable OKTA_CLIENT_TOKEN. This is a one-time operation that does not track the state of the okta resources. It will need to be run again if the tracked resources change. Usage: 1 fidesctl generate dataset okta [OPTIONS] ORG_URL OUTPUT_FILENAME Options: 1 2 --include-null Includes attributes that would otherwise be null. --help Show this message and exit.","title":"okta"},{"location":"cli/#fidesctl-generate-system","text":"Generate fidesctl System resources Usage: 1 fidesctl generate system [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit.","title":"system"},{"location":"cli/#fidesctl-generate-system-aws","text":"Connect to an aws account by leveraging a boto3 environment variable configuration and generate a system manifest file that consists of every tracked resource. Tracked resources: [Redshift, RDS] This is a one-time operation that does not track the state of the aws resources. It will need to be run again if the tracked resources change. Usage: 1 fidesctl generate system aws [OPTIONS] OUTPUT_FILENAME Options: 1 2 3 --include-null Includes attributes that would otherwise be null. -o, --organization TEXT --help Show this message and exit.","title":"aws"},{"location":"cli/#fidesctl-get","text":"View a resource from the server as a JSON object. Usage: 1 2 fidesctl get [OPTIONS] [data_category|data_qualifier|data_subject|data_use|dat aset|organization|policy|registry|system|evaluation] FIDES_KEY Options: 1 --help Show this message and exit.","title":"get"},{"location":"cli/#fidesctl-init","text":"Initializes a Fidesctl instance, creating the default directory ( .fides/ ) and the configuration file ( fidesctl.toml ) if necessary. Additionally, requests the ability to respectfully collect anonymous usage data. Usage: 1 fidesctl init [OPTIONS] [FIDES_DIRECTORY_LOCATION] Options: 1 --help Show this message and exit.","title":"init"},{"location":"cli/#fidesctl-ls","text":"Get a list of all resources of this type from the server and display them as JSON. Usage: 1 2 fidesctl ls [OPTIONS] [data_category|data_qualifier|data_subject|data_use|data set|organization|policy|registry|system|evaluation] Options: 1 --help Show this message and exit.","title":"ls"},{"location":"cli/#fidesctl-parse","text":"Reads the resource files that are stored in MANIFESTS_DIR and its subdirectories to verify the validity of all manifest files. If the taxonomy is invalid, this command prints the error messages and triggers a non-zero exit code. Usage: 1 fidesctl parse [OPTIONS] [MANIFESTS_DIR] Options: 1 2 -v, --verbose Enable verbose output. --help Show this message and exit.","title":"parse"},{"location":"cli/#fidesctl-scan","text":"Scan external resource coverage against fidesctl resources Usage: 1 fidesctl scan [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit.","title":"scan"},{"location":"cli/#fidesctl-scan-dataset","text":"Scan fidesctl Dataset resources Usage: 1 fidesctl scan dataset [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit.","title":"dataset"},{"location":"cli/#fidesctl-scan-dataset-db","text":"Connect to a database directly via a SQLAlchemy-style connection string and compare the database objects to existing datasets. If there are fields within the database that aren't listed and categorized within one of the datasets, this counts as lacking coverage. Outputs missing fields and has a non-zero exit if coverage is under the stated threshold. Usage: 1 fidesctl scan dataset db [OPTIONS] CONNECTION_STRING [MANIFESTS_DIR] Options: 1 2 -c, --coverage-threshold INTEGER RANGE --help Show this message and exit.","title":"db"},{"location":"cli/#fidesctl-scan-dataset-okta","text":"Scans your existing datasets and compares them to found Okta applications. Connect to an Okta admin account by providing an organization url. Auth token can be supplied by setting the environment variable OKTA_CLIENT_TOKEN. Outputs missing resources and has a non-zero exit if coverage is under the stated threshold. Usage: 1 fidesctl scan dataset okta [OPTIONS] ORG_URL [MANIFESTS_DIR] Options: 1 2 -c, --coverage-threshold INTEGER RANGE --help Show this message and exit.","title":"okta"},{"location":"cli/#fidesctl-scan-system","text":"Scan fidesctl System resources Usage: 1 fidesctl scan system [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit.","title":"system"},{"location":"cli/#fidesctl-scan-system-aws","text":"Connect to an aws account by leveraging a valid boto3 environment varible configuration and compares tracked resources to existing systems. Tracked resources: [Redshift, RDS] Outputs missing resources and has a non-zero exit if coverage is under the stated threshold. Usage: 1 fidesctl scan system aws [OPTIONS] [MANIFESTS_DIR] Options: 1 2 3 -o, --organization TEXT -c, --coverage-threshold INTEGER RANGE --help Show this message and exit.","title":"aws"},{"location":"cli/#fidesctl-status","text":"Sends a request to the Fidesctl API healthcheck endpoint and prints the response. Usage: 1 fidesctl status [OPTIONS] Options: 1 --help Show this message and exit.","title":"status"},{"location":"cli/#fidesctl-view","text":"View various resources types. Usage: 1 fidesctl view [OPTIONS] COMMAND [ARGS]... Options: 1 --help Show this message and exit.","title":"view"},{"location":"cli/#fidesctl-view-config","text":"Prints the fidesctl configuration values. Usage: 1 fidesctl view config [OPTIONS] Options: 1 2 --exclude-unset Only print configuration values explicitly set by the user. --help Show this message and exit.","title":"config"},{"location":"cli/#fidesctl-webserver","text":"Starts the fidesctl API server using Uvicorn on port 8080. Usage: 1 fidesctl webserver [OPTIONS] Options: 1 --help Show this message and exit.","title":"webserver"},{"location":"ethyca/","text":"About Ethyca The mission of Ethyca is to make Internet-scale technology respectful and ethical. We're a venture-backed privacy technology team headquartered in New York, but working as a distributed team across the US to solve what we believe is the most important problem in technology today: the human right to privacy in vastly complex data-driven systems. What is Fides? Fides is a universally understandable, open-source language that can be used to describe privacy within tech infrastructure. Our existing tools ( Fidesctl and Fidesops ) use this language to power a low friction set of developer tools that integrate with your existing CI pipelines, making privacy a feature of your tech stack. With Fides, we hope everyone can build better tools for privacy in the next decade and beyond. What we Believe Data privacy is a human right that should be a native feature of any respectful technology. Today building great privacy as a feature in software is friction-filled and complicated. We're building open-source privacy tools for the developer community because we believe the only way to achieve a respectful internet is to make privacy an easy-to-implement layer of any tech stack. The Future We've been working on this problem since 2018 and have a clear view of our next five years. We're excited about the roadmap of features we'll add to Fides in order to make it the comprehensive tool for addressing the major challenges of privacy in both the code management and runtime environments. This means building solutions for automated privacy analysis, context rich data classification, automated data orchestration for privacy rights, semantic access control models, and more. We'd love you to contribute to Fides and you can do this directly as part of the open-source community. If you're interested in solving some of the toughest and most important problems facing internet scale data-driven software, join us now and get paid to work on this problem too! Your Participation Fides' success is predicated on your participation -- Privacy as Code can only become a reality if we ensure it's easy to understand, implement and an interopable standard for wide adoption. Your feedback, contributions and improvements are encouraged as we work towards building a community with the sole objective of building more repsectful software for everyone on the internet.","title":"About Ethyca"},{"location":"ethyca/#about-ethyca","text":"The mission of Ethyca is to make Internet-scale technology respectful and ethical. We're a venture-backed privacy technology team headquartered in New York, but working as a distributed team across the US to solve what we believe is the most important problem in technology today: the human right to privacy in vastly complex data-driven systems.","title":"About Ethyca"},{"location":"ethyca/#what-is-fides","text":"Fides is a universally understandable, open-source language that can be used to describe privacy within tech infrastructure. Our existing tools ( Fidesctl and Fidesops ) use this language to power a low friction set of developer tools that integrate with your existing CI pipelines, making privacy a feature of your tech stack. With Fides, we hope everyone can build better tools for privacy in the next decade and beyond.","title":"What is Fides?"},{"location":"ethyca/#what-we-believe","text":"Data privacy is a human right that should be a native feature of any respectful technology. Today building great privacy as a feature in software is friction-filled and complicated. We're building open-source privacy tools for the developer community because we believe the only way to achieve a respectful internet is to make privacy an easy-to-implement layer of any tech stack.","title":"What we Believe"},{"location":"ethyca/#the-future","text":"We've been working on this problem since 2018 and have a clear view of our next five years. We're excited about the roadmap of features we'll add to Fides in order to make it the comprehensive tool for addressing the major challenges of privacy in both the code management and runtime environments. This means building solutions for automated privacy analysis, context rich data classification, automated data orchestration for privacy rights, semantic access control models, and more. We'd love you to contribute to Fides and you can do this directly as part of the open-source community. If you're interested in solving some of the toughest and most important problems facing internet scale data-driven software, join us now and get paid to work on this problem too!","title":"The Future"},{"location":"ethyca/#your-participation","text":"Fides' success is predicated on your participation -- Privacy as Code can only become a reality if we ensure it's easy to understand, implement and an interopable standard for wide adoption. Your feedback, contributions and improvements are encouraged as we work towards building a community with the sole objective of building more repsectful software for everyone on the internet.","title":"Your Participation"},{"location":"license/","text":"License 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2021- Ethyca, Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"license/#license","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2021- Ethyca, Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"api/","text":"Fidesctl API These docs represent the cutting-edge, and may contain endpoints/features that have yet to be released in stable versions. const ui = SwaggerUIBundle({ url: 'openapi.json', dom_id: '#swagger-ui', }) /* If there is an anchor tag, reload it after the page loads to scroll to * that section, since the Swagger UI takes some time to render. */ if (location.hash) { setTimeout(function() { location.href = location.href }, 200); }","title":"API"},{"location":"api/#fidesctl-api","text":"These docs represent the cutting-edge, and may contain endpoints/features that have yet to be released in stable versions. const ui = SwaggerUIBundle({ url: 'openapi.json', dom_id: '#swagger-ui', }) /* If there is an anchor tag, reload it after the page loads to scroll to * that section, since the Swagger UI takes some time to render. */ if (location.hash) { setTimeout(function() { location.href = location.href }, 200); }","title":"Fidesctl API"},{"location":"cicd/examples/","text":"Example Integrations The following code snippets are meant as simple example implementations, and illustrate how you might integrate fidesctl using various popular CI pipline tools. Always inspect, understand, and test your production CI configuration files. GitHub Actions GitLab CI Jenkins CircleCI Azure Pipelines GitHub Actions .github/workflows/fidesctl_ci.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 name : Fidesctl CI # Only check on Pull Requests that target main on : pull_request : branches : - main paths : # Only run checks when the resource files change or the workflow file changes - .fides/** - .github/workflows/fidesctl_ci.yml jobs : fidesctl_ci : runs-on : ubuntu-latest container : image : ethyca/fidesctl:latest steps : - name : Dry Evaluation uses : actions/checkout@v2 run : fidesctl evaluate --dry .fides/ env : FIDESCTL__CLI__SERVER_HOST : \"fidesctl.privacyco.com\" .github/workflows/fidesctl_cd.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 name : Fidesctl CD # Run the check every time a new commit hits the default branch on : push : branches : - main tags : - \"*\" jobs : fidesctl_cd : runs-on : ubuntu-latest container : image : ethyca/fidesctl:latest steps : - name : Evaluation uses : actions/checkout@v2 run : fidesctl evaluate .fides/ env : FIDESCTL__CLI__SERVER_HOST : \"fidesctl.privacyco.com\" GitLab CI .gitlab-ci.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 stages : - test - deploy variables : &global-variables FIDESCTL__CLI__SERVER_HOST : \"fidesctl.privacyco.com\" fidesctl-ci : stage : test image : ethyca/fidesctl script : fidesctl evaluate --dry .fides/ only : if : '$CI_PIPELINE_SOURCE = merge_request_event' changes : - .fides/** - .gitlab-ci.yml variables : << : *global-variables fidesctl-cd : stage : deploy image : ethyca/fidesctl script : fidesctl evaluate .fides/ if : '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH' variables : << : *global-variables Jenkins Jenkinsfile (Declarative Syntax) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 pipeline { agent { docker { image 'ethyca/fidesctl:latest' } } stages { stage ( 'test' ){ environment { FIDESCTL__CLI__SERVER_HOST: 'fidesctl.privacyco.com' } steps { sh 'fidesctl evaluate --dry .fides/' } when { anyOf { changeset '.fides/**' changeset 'Jenkinsfile' } changeRequest () } } stage ( 'deploy' ) { environment { FIDESCTL__CLI__SERVER_HOST: 'fidesctl.privacyco.com' } steps { sh 'fidesctl evaluate .fides/' } when { branch 'main' } } } } CircleCI .circleci/config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 version : 2.1 executors : fidesctl : docker : - image : ethyca/fidesctl:latest environment : FIDESCTL__CLI__SERVER_HOST : 'fidesctl.privacyco.com' jobs : fidesctl-evaluate-dry : executor : fidesctl steps : - run : fidesctl evaluate --dry .fides/ fidesctl-evaluate : executor : fidesctl steps : - run : fidesctl evaluate .fides/ workflows : version : 2 test : jobs : - fidesctl-evaluate-dry : filters : branches : ignore : main deploy : jobs : - fidesctl-evaluate : filters : branches : only : main Azure Pipelines .azure-pipelines.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Trigger a dry run of the evaluate job on pull requests that target main pr : - main jobs : - job : \"fidesctl_evaluate_dry\" pool : vmImage : ubuntu-latest container : image : ethyca/fidesctl:latest steps : - checkout : self - script : fidesctl evaluate --dry .fides/ displayName : \"Fidesctl Dry Evaluation\" # Trigger the evaluate job on commits to the default branch trigger : - main jobs : - job : \"fidesctl_evaluate\" pool : vmImage : ubuntu-latest container : image : ethyca/fidesctl:latest steps : - checkout : self - script : fidesctl evaluate .fides/ displayName : \"Fidesctl Evaluation\"","title":"Example Integrations"},{"location":"cicd/examples/#example-integrations","text":"The following code snippets are meant as simple example implementations, and illustrate how you might integrate fidesctl using various popular CI pipline tools. Always inspect, understand, and test your production CI configuration files. GitHub Actions GitLab CI Jenkins CircleCI Azure Pipelines","title":"Example Integrations"},{"location":"cicd/examples/#github-actions","text":".github/workflows/fidesctl_ci.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 name : Fidesctl CI # Only check on Pull Requests that target main on : pull_request : branches : - main paths : # Only run checks when the resource files change or the workflow file changes - .fides/** - .github/workflows/fidesctl_ci.yml jobs : fidesctl_ci : runs-on : ubuntu-latest container : image : ethyca/fidesctl:latest steps : - name : Dry Evaluation uses : actions/checkout@v2 run : fidesctl evaluate --dry .fides/ env : FIDESCTL__CLI__SERVER_HOST : \"fidesctl.privacyco.com\" .github/workflows/fidesctl_cd.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 name : Fidesctl CD # Run the check every time a new commit hits the default branch on : push : branches : - main tags : - \"*\" jobs : fidesctl_cd : runs-on : ubuntu-latest container : image : ethyca/fidesctl:latest steps : - name : Evaluation uses : actions/checkout@v2 run : fidesctl evaluate .fides/ env : FIDESCTL__CLI__SERVER_HOST : \"fidesctl.privacyco.com\"","title":"GitHub Actions"},{"location":"cicd/examples/#gitlab-ci","text":".gitlab-ci.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 stages : - test - deploy variables : &global-variables FIDESCTL__CLI__SERVER_HOST : \"fidesctl.privacyco.com\" fidesctl-ci : stage : test image : ethyca/fidesctl script : fidesctl evaluate --dry .fides/ only : if : '$CI_PIPELINE_SOURCE = merge_request_event' changes : - .fides/** - .gitlab-ci.yml variables : << : *global-variables fidesctl-cd : stage : deploy image : ethyca/fidesctl script : fidesctl evaluate .fides/ if : '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH' variables : << : *global-variables","title":"GitLab CI"},{"location":"cicd/examples/#jenkins","text":"Jenkinsfile (Declarative Syntax) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 pipeline { agent { docker { image 'ethyca/fidesctl:latest' } } stages { stage ( 'test' ){ environment { FIDESCTL__CLI__SERVER_HOST: 'fidesctl.privacyco.com' } steps { sh 'fidesctl evaluate --dry .fides/' } when { anyOf { changeset '.fides/**' changeset 'Jenkinsfile' } changeRequest () } } stage ( 'deploy' ) { environment { FIDESCTL__CLI__SERVER_HOST: 'fidesctl.privacyco.com' } steps { sh 'fidesctl evaluate .fides/' } when { branch 'main' } } } }","title":"Jenkins"},{"location":"cicd/examples/#circleci","text":".circleci/config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 version : 2.1 executors : fidesctl : docker : - image : ethyca/fidesctl:latest environment : FIDESCTL__CLI__SERVER_HOST : 'fidesctl.privacyco.com' jobs : fidesctl-evaluate-dry : executor : fidesctl steps : - run : fidesctl evaluate --dry .fides/ fidesctl-evaluate : executor : fidesctl steps : - run : fidesctl evaluate .fides/ workflows : version : 2 test : jobs : - fidesctl-evaluate-dry : filters : branches : ignore : main deploy : jobs : - fidesctl-evaluate : filters : branches : only : main","title":"CircleCI"},{"location":"cicd/examples/#azure-pipelines","text":".azure-pipelines.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Trigger a dry run of the evaluate job on pull requests that target main pr : - main jobs : - job : \"fidesctl_evaluate_dry\" pool : vmImage : ubuntu-latest container : image : ethyca/fidesctl:latest steps : - checkout : self - script : fidesctl evaluate --dry .fides/ displayName : \"Fidesctl Dry Evaluation\" # Trigger the evaluate job on commits to the default branch trigger : - main jobs : - job : \"fidesctl_evaluate\" pool : vmImage : ubuntu-latest container : image : ethyca/fidesctl:latest steps : - checkout : self - script : fidesctl evaluate .fides/ displayName : \"Fidesctl Evaluation\"","title":"Azure Pipelines"},{"location":"cicd/overview/","text":"CI/CD Overview Fidesctl is primarily designed to integrate with your existing CI pipeline configurations. Common implementations include: Github Actions Gitlab CI Jenkins CircleCI Azure Pipelines AWS CodePipeline Bitbucket Bamboo Team City Implementing Fidesctl is possible in nearly any CI pipeline, including those not listed. Implementation To integrate Fidesctl with your CI pipeline, you should plan to implement at least two CI actions: fidesctl evaluate --dry <resource_dir> evaluate --dry checks if code changes will be accepted without applying those changes to the fidesctl server. Run this against the latest commit on code changesets (pull requests, merge requests, etc). fidesctl evaluate <resource_dir> evaluate synchronizes the latest changes to the fidesctl server. Run this against commits representing merges into the default branch. Implementation examples are also available for a variety of CI tools.","title":"Overview"},{"location":"cicd/overview/#cicd-overview","text":"","title":"CI/CD Overview"},{"location":"cicd/overview/#_1","text":"Fidesctl is primarily designed to integrate with your existing CI pipeline configurations. Common implementations include: Github Actions Gitlab CI Jenkins CircleCI Azure Pipelines AWS CodePipeline Bitbucket Bamboo Team City Implementing Fidesctl is possible in nearly any CI pipeline, including those not listed.","title":""},{"location":"cicd/overview/#implementation","text":"To integrate Fidesctl with your CI pipeline, you should plan to implement at least two CI actions: fidesctl evaluate --dry <resource_dir> evaluate --dry checks if code changes will be accepted without applying those changes to the fidesctl server. Run this against the latest commit on code changesets (pull requests, merge requests, etc). fidesctl evaluate <resource_dir> evaluate synchronizes the latest changes to the fidesctl server. Run this against commits representing merges into the default branch. Implementation examples are also available for a variety of CI tools.","title":"Implementation"},{"location":"community/code_of_conduct/","text":"Fides Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct that could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the Fides core team at fides@ethyca.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"community/code_of_conduct/#fides-code-of-conduct","text":"","title":"Fides Code of Conduct"},{"location":"community/code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"community/code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct that could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"community/code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"community/code_of_conduct/#scope","text":"This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"community/code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the Fides core team at fides@ethyca.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"community/code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"community/hints_tips/","text":"Community The Fides project welcomes issues, contributions and discussion from all users, regardless of background or experience level. In order to create a positive and welcoming environment, all interactions are governed by the Fides Code of Conduct . Guidelines Whether it's giving us feedback, raising a question, or showing your Fides-related work, we are looking forward to hearing from you. The Fides community is vibrant because of the quality of its members and the discussions they bring. To keep the workspace inviting and helpful for everyone, there are a few guidelines that we ask all members to follow. Rule 1: Assume Positive Intent Being nice is the most important pillar of the Fides community. We are considerate to each other's effort and time. It's also easy to misinterpret people through Slack, so we make an extra effort to chat in a positive tone. We assume that you are here to learn and exchange ideas, and we ask that you contribute to making a welcoming community. If someone is helping you, be mindful of the effort they are putting in. While we are always happy to help users, we can not help users with step-by-step debugging. Use your professional judgment in discerning whether requests are unreasonable. The Fides team always tries to listen to the community. Please be understanding if your issue or feature request is not deemed an immediate priority. Rule 2: Use threads for larger messages Because of the size of our community and frequency of posts, it's easy for large messages to drown out smaller messages. Using threads helps people see more messages on their screen. Larger code blocks should be posted in threads. Rule 3: Avoid posting sensitive information Community members sometimes need to post code snippets as they ask for help. Be sure to remove sensitive information from posts to the public channels. If your Fides account information is needed to help you, we will ask you to direct message such information. Be cautious of anyone asking for information through direct messages. Rule 4: Write high quality questions The Fides community is here to support you. That said, it is significantly easier to answer well-researched and clearly-written questions. Even adding potentially relevant links to a post helps tremendously. Informative Slack threads are archived by our resident bot Marvin. Having well-written threads helps future users encountering the same problem. Oftentimes your question may have been answered somewhere else; some good resources to start looking before asking a question: Fides Documentation GitHub Issues StackOverflow Rule 5: Don't abuse tagging users Requests for help will be seen by the Fides team, and will be directed to the appropriate person. Tagging individual users is highly discouraged unless it is in the context of a conversation thread. Rule 6: Avoid using DMs to ask for help Fides employees should not be sent questions in DMs unless we specifically ask you to send us private information. There are times when it makes sense to directly message another community member experiencing a similar issue, or working with similar technologies. Just be aware that some people may not want to be messaged. It also helps other people if you post your question publicly. Similar to above, informative Slack threads are archived. Having conversations in public channels drives better quality discussions that can be referenced in the future. Rule 7: Don't advertise material unrelated to Fides Our community is in the channel to learn more about Fides. Showing us Fides-related stuff that you're working on is highly encouraged. Advertising products and events unrelated to Fides will be removed.","title":"Community Hints & Tips"},{"location":"community/hints_tips/#community","text":"The Fides project welcomes issues, contributions and discussion from all users, regardless of background or experience level. In order to create a positive and welcoming environment, all interactions are governed by the Fides Code of Conduct .","title":"Community"},{"location":"community/hints_tips/#guidelines","text":"Whether it's giving us feedback, raising a question, or showing your Fides-related work, we are looking forward to hearing from you. The Fides community is vibrant because of the quality of its members and the discussions they bring. To keep the workspace inviting and helpful for everyone, there are a few guidelines that we ask all members to follow.","title":"Guidelines"},{"location":"community/hints_tips/#rule-1-assume-positive-intent","text":"Being nice is the most important pillar of the Fides community. We are considerate to each other's effort and time. It's also easy to misinterpret people through Slack, so we make an extra effort to chat in a positive tone. We assume that you are here to learn and exchange ideas, and we ask that you contribute to making a welcoming community. If someone is helping you, be mindful of the effort they are putting in. While we are always happy to help users, we can not help users with step-by-step debugging. Use your professional judgment in discerning whether requests are unreasonable. The Fides team always tries to listen to the community. Please be understanding if your issue or feature request is not deemed an immediate priority.","title":"Rule 1: Assume Positive Intent"},{"location":"community/hints_tips/#rule-2-use-threads-for-larger-messages","text":"Because of the size of our community and frequency of posts, it's easy for large messages to drown out smaller messages. Using threads helps people see more messages on their screen. Larger code blocks should be posted in threads.","title":"Rule 2: Use threads for larger messages"},{"location":"community/hints_tips/#rule-3-avoid-posting-sensitive-information","text":"Community members sometimes need to post code snippets as they ask for help. Be sure to remove sensitive information from posts to the public channels. If your Fides account information is needed to help you, we will ask you to direct message such information. Be cautious of anyone asking for information through direct messages.","title":"Rule 3: Avoid posting sensitive information"},{"location":"community/hints_tips/#rule-4-write-high-quality-questions","text":"The Fides community is here to support you. That said, it is significantly easier to answer well-researched and clearly-written questions. Even adding potentially relevant links to a post helps tremendously. Informative Slack threads are archived by our resident bot Marvin. Having well-written threads helps future users encountering the same problem. Oftentimes your question may have been answered somewhere else; some good resources to start looking before asking a question: Fides Documentation GitHub Issues StackOverflow","title":"Rule 4: Write high quality questions"},{"location":"community/hints_tips/#rule-5-dont-abuse-tagging-users","text":"Requests for help will be seen by the Fides team, and will be directed to the appropriate person. Tagging individual users is highly discouraged unless it is in the context of a conversation thread.","title":"Rule 5: Don't abuse tagging users"},{"location":"community/hints_tips/#rule-6-avoid-using-dms-to-ask-for-help","text":"Fides employees should not be sent questions in DMs unless we specifically ask you to send us private information. There are times when it makes sense to directly message another community member experiencing a similar issue, or working with similar technologies. Just be aware that some people may not want to be messaged. It also helps other people if you post your question publicly. Similar to above, informative Slack threads are archived. Having conversations in public channels drives better quality discussions that can be referenced in the future.","title":"Rule 6: Avoid using DMs to ask for help"},{"location":"community/hints_tips/#rule-7-dont-advertise-material-unrelated-to-fides","text":"Our community is in the channel to learn more about Fides. Showing us Fides-related stuff that you're working on is highly encouraged. Advertising products and events unrelated to Fides will be removed.","title":"Rule 7: Don't advertise material unrelated to Fides"},{"location":"community/overview/","text":"Join the Fides Community We believe the only way to solve privacy is as a community of likeminded developers that care to solve these problems for the rest of the world. To make it easier to connect and share ideas, we've created a set of community channels which we'd love to hear from you on: Start Contributing \u2003 Chat on Slack \u2003 Chat on Discord","title":"Github, Slack & Discord"},{"location":"community/overview/#join-the-fides-community","text":"We believe the only way to solve privacy is as a community of likeminded developers that care to solve these problems for the rest of the world. To make it easier to connect and share ideas, we've created a set of community channels which we'd love to hear from you on: Start Contributing \u2003 Chat on Slack \u2003 Chat on Discord","title":"Join the Fides Community"},{"location":"development/code_style/","text":"Code Style General Docstrings Docstrings are required for every function, class and method. No specific style is required or encouraged, as we expect that most of the relevant information can be gleaned from both the function signature's type-hints as well as descriptive parameter names. The docstring should serve to give additional context/flavour beyond that which can be gained from the code itself. Docstring Example 1 2 3 4 5 6 7 8 9 10 11 12 # Bad def execute_evaluation ( taxonomy : Taxonomy ) -> Evaluation : \"\"\" Execute an evaluation. \"\"\" # Good def execute_evaluation ( taxonomy : Taxonomy ) -> Evaluation : \"\"\" Check the stated constraints of each Privacy Policy's rules against each system's privacy declarations. \"\"\" Variable/Parameter Naming Variable and parameter names should be as self-describing as possible. Brevity is not a concern here. Here are some common examples for writing good self-documenting code: Single Letter Variable Names 1 2 3 4 5 6 7 8 9 10 11 12 13 # Incorrect s = 726 # Correct elapsed_time_seconds = 726 # Incorrect for n in nodes : print ( n ) # Correct for node in nodes : print ( node ) Abbreviated Variable Names 1 2 3 4 5 6 7 8 # Incorrect r = requests . get ( url ) # Incorrect resp = reqeusts . get ( url ) # Correct response = requests . get ( url ) Type Ambiguous Variable Names 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Incorrect food = [ \"apple\" , \"banana\" ] # Incorrect foods = [ \"apple\" , \"banana\" ] # Correct # Use type annotations if the name is somewhat ambiguous foods : List [ str ] = [ \"apple\" , \"banana\" ] # Correct # The type is contained in the name foods_list = [ \"apple\" , \"banana\" ] # Correct # Both of the above styles foods_list : List [ str ] = [ \"apple\" , \"banana\" ] Pre-Commit Hooks Fidesctl includes a .pre-commit-config.yaml to facilitate running CI checks before pushing up to a PR. The pre-commit package is included in the dev-requirements.txt . Once that is installed, follow these steps to get up and running: pre-commit install - This is a one-time setup step to create the git pre-commit hooks. These pre-commit hooks will now run automatically. However you can also use pre-commit run to run them manually once all of your changes have been staged. NOTE : A Python interpreter must be available from wherever the git commands are being run, as this is required to run the pre-commit package. CI Checks CI checks are stored as targets within the Makefile, and can be run from the top-level fides directory with the following pattern: Pattern 1 make <lowercased_name> Examples 1 2 3 make black make mypy make xenon Black formatting Fidesctl's code is formatted using the black style. This style is checked in a CI step, and merges to master are prevented if code does not conform. A number of extensions are available for popular editors that will automatically apply black to your code. Pylint Fidesctl's code is linted using pylint . Linter checks run as part of a CI step and merges to master are prevented if code does not conform. Mypy Fidesctl's code is statically-typed using mypy . Type checking is validated as a CI step, and merges to master are prevented if code does not pass type checks. As a general rule, mypy typing requires all function arguments and return values to be annotated. Xenon Fidesctl's code is checked for its cyclomatic-complexity by Xenon. If a single logical piece of code is deemed too complex, then a CI step will fail, at which point the focus should be on breaking up said complex function/method/class.","title":"Code Style"},{"location":"development/code_style/#code-style","text":"","title":"Code Style"},{"location":"development/code_style/#general","text":"","title":"General"},{"location":"development/code_style/#docstrings","text":"Docstrings are required for every function, class and method. No specific style is required or encouraged, as we expect that most of the relevant information can be gleaned from both the function signature's type-hints as well as descriptive parameter names. The docstring should serve to give additional context/flavour beyond that which can be gained from the code itself. Docstring Example 1 2 3 4 5 6 7 8 9 10 11 12 # Bad def execute_evaluation ( taxonomy : Taxonomy ) -> Evaluation : \"\"\" Execute an evaluation. \"\"\" # Good def execute_evaluation ( taxonomy : Taxonomy ) -> Evaluation : \"\"\" Check the stated constraints of each Privacy Policy's rules against each system's privacy declarations. \"\"\"","title":"Docstrings"},{"location":"development/code_style/#variableparameter-naming","text":"Variable and parameter names should be as self-describing as possible. Brevity is not a concern here. Here are some common examples for writing good self-documenting code: Single Letter Variable Names 1 2 3 4 5 6 7 8 9 10 11 12 13 # Incorrect s = 726 # Correct elapsed_time_seconds = 726 # Incorrect for n in nodes : print ( n ) # Correct for node in nodes : print ( node ) Abbreviated Variable Names 1 2 3 4 5 6 7 8 # Incorrect r = requests . get ( url ) # Incorrect resp = reqeusts . get ( url ) # Correct response = requests . get ( url ) Type Ambiguous Variable Names 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Incorrect food = [ \"apple\" , \"banana\" ] # Incorrect foods = [ \"apple\" , \"banana\" ] # Correct # Use type annotations if the name is somewhat ambiguous foods : List [ str ] = [ \"apple\" , \"banana\" ] # Correct # The type is contained in the name foods_list = [ \"apple\" , \"banana\" ] # Correct # Both of the above styles foods_list : List [ str ] = [ \"apple\" , \"banana\" ]","title":"Variable/Parameter Naming"},{"location":"development/code_style/#pre-commit-hooks","text":"Fidesctl includes a .pre-commit-config.yaml to facilitate running CI checks before pushing up to a PR. The pre-commit package is included in the dev-requirements.txt . Once that is installed, follow these steps to get up and running: pre-commit install - This is a one-time setup step to create the git pre-commit hooks. These pre-commit hooks will now run automatically. However you can also use pre-commit run to run them manually once all of your changes have been staged. NOTE : A Python interpreter must be available from wherever the git commands are being run, as this is required to run the pre-commit package.","title":"Pre-Commit Hooks"},{"location":"development/code_style/#ci-checks","text":"CI checks are stored as targets within the Makefile, and can be run from the top-level fides directory with the following pattern: Pattern 1 make <lowercased_name> Examples 1 2 3 make black make mypy make xenon","title":"CI Checks"},{"location":"development/code_style/#black-formatting","text":"Fidesctl's code is formatted using the black style. This style is checked in a CI step, and merges to master are prevented if code does not conform. A number of extensions are available for popular editors that will automatically apply black to your code.","title":"Black formatting"},{"location":"development/code_style/#pylint","text":"Fidesctl's code is linted using pylint . Linter checks run as part of a CI step and merges to master are prevented if code does not conform.","title":"Pylint"},{"location":"development/code_style/#mypy","text":"Fidesctl's code is statically-typed using mypy . Type checking is validated as a CI step, and merges to master are prevented if code does not pass type checks. As a general rule, mypy typing requires all function arguments and return values to be annotated.","title":"Mypy"},{"location":"development/code_style/#xenon","text":"Fidesctl's code is checked for its cyclomatic-complexity by Xenon. If a single logical piece of code is deemed too complex, then a CI step will fail, at which point the focus should be on breaking up said complex function/method/class.","title":"Xenon"},{"location":"development/database_migration/","text":"Database Migration Changes to fidesctl could require a change to the database model. This includes scenarios where you want to persist a new field or replace an existing field. Changes made to the fidesctl database are done through alembic migration scripts. Migrations can be found in the following direcotry: fidesctl/src/fidesapi/migrations/versions To create a new migration we use the alembic revision command: 1 2 cd fidesctl/src/fidesapi alembic revision --autogenerate -m \"migration message\" The autogenerated script should be verified and could require some manual changes. Migrations will run on the fidesctl server startup.","title":"Database Migration"},{"location":"development/database_migration/#database-migration","text":"Changes to fidesctl could require a change to the database model. This includes scenarios where you want to persist a new field or replace an existing field. Changes made to the fidesctl database are done through alembic migration scripts. Migrations can be found in the following direcotry: fidesctl/src/fidesapi/migrations/versions To create a new migration we use the alembic revision command: 1 2 cd fidesctl/src/fidesapi alembic revision --autogenerate -m \"migration message\" The autogenerated script should be verified and could require some manual changes. Migrations will run on the fidesctl server startup.","title":"Database Migration"},{"location":"development/documentation/","text":"Documentation Documentation is incredibly important to fidesctl, both for explaining its concepts to general audiences and describing its usage to developers. Concepts Fidesctl includes a great deal of \"concept\" documentation, which covers features, tutorials, guides, and examples separately from the auto-generated API reference. This page is part of the concept documentation for development! To write concept docs, add Markdown files to the docs/fides/docs/ directory (or one of its subdirectories). To ensure that your page is displayed in the navigation, edit mkdocs.yml to include a reference to it. Semantics Capitalization Concepts that refer to proper nouns or are trademarked should always be capitalized. The exception here is fidesctl and fidesops, which are lowercased as a stylistic choice. Other Fides terms, like \"Data Category\" or \"System\", should also be capitalized to be clear about the fact that a Fides resource is being referenced. When a System is applied, it is either created or updated through the fidesctl api. The System model requires a field called fides_key . Previewing docs locally Documentation (including both concepts and API references) is built and deployed with every merge to Fides's master branch. If you're using VS Code Dev Containers, the docs will automatically be available at localhost:8000 , otherwise you'll need to run the following command: 1 make docs-serve You'll see a status update as the docs build, and then an announcement that they are available on http://127.0.0.1:8000 .","title":"Documentation"},{"location":"development/documentation/#documentation","text":"Documentation is incredibly important to fidesctl, both for explaining its concepts to general audiences and describing its usage to developers.","title":"Documentation"},{"location":"development/documentation/#concepts","text":"Fidesctl includes a great deal of \"concept\" documentation, which covers features, tutorials, guides, and examples separately from the auto-generated API reference. This page is part of the concept documentation for development! To write concept docs, add Markdown files to the docs/fides/docs/ directory (or one of its subdirectories). To ensure that your page is displayed in the navigation, edit mkdocs.yml to include a reference to it.","title":"Concepts"},{"location":"development/documentation/#semantics","text":"","title":"Semantics"},{"location":"development/documentation/#capitalization","text":"Concepts that refer to proper nouns or are trademarked should always be capitalized. The exception here is fidesctl and fidesops, which are lowercased as a stylistic choice. Other Fides terms, like \"Data Category\" or \"System\", should also be capitalized to be clear about the fact that a Fides resource is being referenced. When a System is applied, it is either created or updated through the fidesctl api. The System model requires a field called fides_key .","title":"Capitalization"},{"location":"development/documentation/#previewing-docs-locally","text":"Documentation (including both concepts and API references) is built and deployed with every merge to Fides's master branch. If you're using VS Code Dev Containers, the docs will automatically be available at localhost:8000 , otherwise you'll need to run the following command: 1 make docs-serve You'll see a status update as the docs build, and then an announcement that they are available on http://127.0.0.1:8000 .","title":"Previewing docs locally"},{"location":"development/overview/","text":"Development Overview Thanks for contributing to Fidesctl! This section of the docs is designed to help you become familiar with how we work, the standards we apply, and how to ensure your contribution is successful. If you're stuck, don't be shy about asking for help on GitHub . Getting Started The first step is to clone the Fidesctl repo for development: 1 git clone https://github.com/ethyca/fides Once that's complete, there are a few different ways to spin up the project and get coding! Developer Workflows There are a few different ways to develop Fidesctl, they are listed below in order of how strongly they are recommended! If you're using VS Code, the recommended way to work on Fidesctl is by leveraging the Dev Containers feature. The repo has a .devcontainer/devcontainer.json file already included that will set up a complete environment in VS Code, including the suggested VS Code extensions and settings. Follow these steps to get started: Install the Remote-Containers extension for VS Code. Open the command window (the F1 key will open it, by default) and select the Remote-Containers: Open Folder in Container... command. Click \"Open\", without having selected any specific folder. The containers will now spin up and VS Code will be running inside of the containers. The bottom left of the IDE will now say Dev Container: Fidesctl . When you open a new VS Code shell, it will be inside of the fidesctl container, and you'll have access to all of the fidesctl commands as well as any Python commands like pytest , black , mypy , etc. If you're using an editor besides VS Code, then the next best way to work on Fidesctl is by utilizing the Makefile commands: Make sure that you have docker , docker-compose and make installed. Once you have everything set up, run make cli to spin up a shell within the fidesctl container. You can and should run all of your various development commands from within this shell, such as pytest , black , etc. Finally, the least-recommended method would be to install the project in your local environment and develop directly. Write your code We have no doubt you can write amazing code! However, we want to help you ensure your code plays nicely with the rest of the Fidesctl ecosystem. Many projects describe code style and documentation as a suggestion; in Fidesctl it's a CI-checked requirement. To learn how to style your code, see the style guide . To learn how to migrate the database schema, see the database migration guide . To learn how to document your code, see the docs guide . To learn how to test your code, see the tests guide . To learn what format your PR should follow, make sure to follow the pull request guidelines . Submit your code In order to submit code to Fidesctl, please: Fork the Fidesctl repository Create a new branch on your fork Open a Pull Request once your work is ready for review Once automated tests have passed, a maintainer will review your PR and provide feedback on any changes it requires to be approved. Once approved, your PR will be merged into Fidesctl. Congratulations You're a Fidesctl contributor - welcome to the team! \ud83c\udf89","title":"Overview"},{"location":"development/overview/#development-overview","text":"Thanks for contributing to Fidesctl! This section of the docs is designed to help you become familiar with how we work, the standards we apply, and how to ensure your contribution is successful. If you're stuck, don't be shy about asking for help on GitHub .","title":"Development Overview"},{"location":"development/overview/#getting-started","text":"The first step is to clone the Fidesctl repo for development: 1 git clone https://github.com/ethyca/fides Once that's complete, there are a few different ways to spin up the project and get coding!","title":"Getting Started"},{"location":"development/overview/#developer-workflows","text":"There are a few different ways to develop Fidesctl, they are listed below in order of how strongly they are recommended! If you're using VS Code, the recommended way to work on Fidesctl is by leveraging the Dev Containers feature. The repo has a .devcontainer/devcontainer.json file already included that will set up a complete environment in VS Code, including the suggested VS Code extensions and settings. Follow these steps to get started: Install the Remote-Containers extension for VS Code. Open the command window (the F1 key will open it, by default) and select the Remote-Containers: Open Folder in Container... command. Click \"Open\", without having selected any specific folder. The containers will now spin up and VS Code will be running inside of the containers. The bottom left of the IDE will now say Dev Container: Fidesctl . When you open a new VS Code shell, it will be inside of the fidesctl container, and you'll have access to all of the fidesctl commands as well as any Python commands like pytest , black , mypy , etc. If you're using an editor besides VS Code, then the next best way to work on Fidesctl is by utilizing the Makefile commands: Make sure that you have docker , docker-compose and make installed. Once you have everything set up, run make cli to spin up a shell within the fidesctl container. You can and should run all of your various development commands from within this shell, such as pytest , black , etc. Finally, the least-recommended method would be to install the project in your local environment and develop directly.","title":"Developer Workflows"},{"location":"development/overview/#write-your-code","text":"We have no doubt you can write amazing code! However, we want to help you ensure your code plays nicely with the rest of the Fidesctl ecosystem. Many projects describe code style and documentation as a suggestion; in Fidesctl it's a CI-checked requirement. To learn how to style your code, see the style guide . To learn how to migrate the database schema, see the database migration guide . To learn how to document your code, see the docs guide . To learn how to test your code, see the tests guide . To learn what format your PR should follow, make sure to follow the pull request guidelines .","title":"Write your code"},{"location":"development/overview/#submit-your-code","text":"In order to submit code to Fidesctl, please: Fork the Fidesctl repository Create a new branch on your fork Open a Pull Request once your work is ready for review Once automated tests have passed, a maintainer will review your PR and provide feedback on any changes it requires to be approved. Once approved, your PR will be merged into Fidesctl.","title":"Submit your code"},{"location":"development/overview/#congratulations","text":"You're a Fidesctl contributor - welcome to the team! \ud83c\udf89","title":"Congratulations"},{"location":"development/pull_requests/","text":"Pull Requests Pull requests are the primary unit of work within the Fidesctl project. All code changes are expected to be submitted via a PR, and as such here are a few requirements for submitting PRs: Completely fill out the provided pull request template. PRs should be in a draft state until they are ready for a final review + merge. A non-draft PR signals to the community that the author believes the PR is ready to ship! If you need early feedback on your PR, feel free to ask for it directly while your PR is in a draft state. Make sure that all checks are passing and all boxes have been checked before taking the PR out of a draft state. PR reviews require other people to spend their time, so please be courteous and double check your work before passing it to a reviewer. If you're unsure about a potential feature implementation or there is anything else that needs discussing, feel free to ask for an early review/feedback in the comments of the draft PR. PRs should be focused, reflecting a single logical change or feature. Generally, it is better to have multiple smaller PRs than one big one. This reduces the merge risk and shortens review time.","title":"Pull Requests"},{"location":"development/pull_requests/#pull-requests","text":"Pull requests are the primary unit of work within the Fidesctl project. All code changes are expected to be submitted via a PR, and as such here are a few requirements for submitting PRs: Completely fill out the provided pull request template. PRs should be in a draft state until they are ready for a final review + merge. A non-draft PR signals to the community that the author believes the PR is ready to ship! If you need early feedback on your PR, feel free to ask for it directly while your PR is in a draft state. Make sure that all checks are passing and all boxes have been checked before taking the PR out of a draft state. PR reviews require other people to spend their time, so please be courteous and double check your work before passing it to a reviewer. If you're unsure about a potential feature implementation or there is anything else that needs discussing, feel free to ask for an early review/feedback in the comments of the draft PR. PRs should be focused, reflecting a single logical change or feature. Generally, it is better to have multiple smaller PRs than one big one. This reduces the merge risk and shortens review time.","title":"Pull Requests"},{"location":"development/releases/","text":"Releases Versioning Fidesctl uses semantic versioning. Due to the rapid development of the project, some minor versions may also contain minor breaking changes. Best practice is always pinning versions and carefully testing before bumping to a new version. Patch versions will never cause breaking changes, and are only used to hotfix critical bugs. Release Schedule Fidesctl does not follow a set release schedule, but instead ships versions based on the addition of features/functionality. Each release, with the exception of hotfixes, will contain at least one substantial new feature. Planning For each release a corresponding GitHub Project is created. These projects can be found here . Issues are then added to release projects as a way to organize what will be included in each release. Once a release project is complete and the core team signs off on the readiness of the release, a new version is cut using GitHub releases. You can see all fidesctl releases here . Each new release triggers a GitHub Action that pushes the new version to PyPI as well as pushes a clean version to DockerHub. The release project is then marked as closed . Hotfixes are an exception to this and can be added and pushed as patch versions when needed. Branching Fidesctl uses continuous delivery with a single main branch. All code changes get merged into this branch. For releases, a new tag is created and the release process kicks off automatically. In the case of patches, a branch is created from the relevant tag and commits are then cherry-picked into it and a new patch version tag is created. Release Steps We use GitHub\u2019s release feature to tag releases that then get automatically deployed to DockerHub & PyPi via GitHub Actions pipelines. We also use a CHANGELOG.md to make sure that our users are never surprised about an upcoming change and can plan upgrades accordingly. The release steps are as follows: Open a PR that is titled the version of the release (i.e. 1.6.0 ) Rename the Unreleased section of CHANGELOG.md to the new version number and put a date next to it Update the compare links for both the new version and for the new Unreleased section Once approved, merge the PR Create a new release, ensuring that the last PR to get merged is the aforementioned CHANGELOG.md update PR Add the new version as the tag (i.e. 1.6.0 ) Make the title the version with a v in front of it (i.e. v1.6.0 ) Publish the release Close the accompanying project in GitHub (i.e. 1.6.0 )","title":"Releases"},{"location":"development/releases/#releases","text":"","title":"Releases"},{"location":"development/releases/#versioning","text":"Fidesctl uses semantic versioning. Due to the rapid development of the project, some minor versions may also contain minor breaking changes. Best practice is always pinning versions and carefully testing before bumping to a new version. Patch versions will never cause breaking changes, and are only used to hotfix critical bugs.","title":"Versioning"},{"location":"development/releases/#release-schedule","text":"Fidesctl does not follow a set release schedule, but instead ships versions based on the addition of features/functionality. Each release, with the exception of hotfixes, will contain at least one substantial new feature.","title":"Release Schedule"},{"location":"development/releases/#planning","text":"For each release a corresponding GitHub Project is created. These projects can be found here . Issues are then added to release projects as a way to organize what will be included in each release. Once a release project is complete and the core team signs off on the readiness of the release, a new version is cut using GitHub releases. You can see all fidesctl releases here . Each new release triggers a GitHub Action that pushes the new version to PyPI as well as pushes a clean version to DockerHub. The release project is then marked as closed . Hotfixes are an exception to this and can be added and pushed as patch versions when needed.","title":"Planning"},{"location":"development/releases/#branching","text":"Fidesctl uses continuous delivery with a single main branch. All code changes get merged into this branch. For releases, a new tag is created and the release process kicks off automatically. In the case of patches, a branch is created from the relevant tag and commits are then cherry-picked into it and a new patch version tag is created.","title":"Branching"},{"location":"development/releases/#release-steps","text":"We use GitHub\u2019s release feature to tag releases that then get automatically deployed to DockerHub & PyPi via GitHub Actions pipelines. We also use a CHANGELOG.md to make sure that our users are never surprised about an upcoming change and can plan upgrades accordingly. The release steps are as follows: Open a PR that is titled the version of the release (i.e. 1.6.0 ) Rename the Unreleased section of CHANGELOG.md to the new version number and put a date next to it Update the compare links for both the new version and for the new Unreleased section Once approved, merge the PR Create a new release, ensuring that the last PR to get merged is the aforementioned CHANGELOG.md update PR Add the new version as the tag (i.e. 1.6.0 ) Make the title the version with a v in front of it (i.e. v1.6.0 ) Publish the release Close the accompanying project in GitHub (i.e. 1.6.0 )","title":"Release Steps"},{"location":"development/testing/","text":"Testing Fidesctl loves tests! There are a few important reasons to write tests: Make sure your code works Tests ensure that your code does the thing you intend it to do. If you have a function that adds two numbers, you'll want to test that it does, in fact, return their sum. If behavior depends on a configuration setting, ensure that changing that setting changes the behavior. In short, if you wrote a line of code, you should test that line works as expected. Make sure your code doesn't not work It may seem silly, but another important reason to write tests is to ensure that your code behaves as expected even when it's broken . This is especially important for a project like Fidesctl, which is focused on helping engineers when something unexpected happens to their code. For example, you could write tests about what you expect to happen if your function is called with incorrect (or no) arguments, or to ensure that any errors are properly trapped and handled. Tests are documentation Ultimately, your tests are the best documentation for your code. Another developer should be able to look at your tests and understand what your code does, how to invoke it, and what edge cases it contains. Therefore, try to write short, self-explanatory tests with descriptive titles. Help future developers As Fidesctl grows, your code will be reused in more and more places, by developers who may not be familiar with the details of your implementation. Therefore, your tests are an opportunity to ensure that your code is used correctly in the future. For example, if your code needs to be used in a certain way, or expects a certain configuration, or is always expected to return a certain output, or has any other details that might impact its ability to be used in the framework, write a test for it! At minimum, you'll help a future developer understand that you consciously chose to design your code a certain way. Writing tests Fidesctl's tests are stored in the tests directory. Tests should have descriptive names that make it clear what you're testing. If necessary, add a docstring or comment to explain why you're testing this specific thing. 1 2 3 4 5 6 7 # Good test name def test_dry_evaluate_system_fail ( server_url , resources_dict ): ... # Bad test name def test_dry_evaluate (): ... Fidesctl has a few pytest fixtures available for testing; see conftest.py for details. Integration tests vs. Mocked tests Generally, tests that include mocking are discouraged. Mocking can create a false sense of security and obfuscate possible errors in the code that only present themselves when integration tested. Running tests Fidesctl uses pytest for unit testing. To run tests, invoke pytest from the /fides/fidesctl/ directory: 1 2 cd fidesctl pytest Running specific tests To run a subset of tests, provide a filename or directory; to match a specific test name, use the -k flag: 1 2 # run all tests in the tests/integration directory that contain the word \"api\" in their title pytest tests/integration/ -k api The --sw flag will exit pytest the first time it encounters an error; subsequent runs with the same flag will skip any tests that succeeded and run the failed test first. For more information on available Pytest invocation options, see the documentation here . Excluding external tests Integration tests also test integration with external services like Snowflake which require internet access and authentication. It is possible to skip these tests by excluding the external mark. 1 2 # run all tests except external ones pytest -m \"not external\" CI Workflows CI will run automatically against any PR you open. Please run your tests locally first to avoid \"debugging in CI\", as this takes up resources that could be used by other contributors and is generally an inefficient usage of your time!","title":"Testing"},{"location":"development/testing/#testing","text":"Fidesctl loves tests! There are a few important reasons to write tests: Make sure your code works Tests ensure that your code does the thing you intend it to do. If you have a function that adds two numbers, you'll want to test that it does, in fact, return their sum. If behavior depends on a configuration setting, ensure that changing that setting changes the behavior. In short, if you wrote a line of code, you should test that line works as expected. Make sure your code doesn't not work It may seem silly, but another important reason to write tests is to ensure that your code behaves as expected even when it's broken . This is especially important for a project like Fidesctl, which is focused on helping engineers when something unexpected happens to their code. For example, you could write tests about what you expect to happen if your function is called with incorrect (or no) arguments, or to ensure that any errors are properly trapped and handled. Tests are documentation Ultimately, your tests are the best documentation for your code. Another developer should be able to look at your tests and understand what your code does, how to invoke it, and what edge cases it contains. Therefore, try to write short, self-explanatory tests with descriptive titles. Help future developers As Fidesctl grows, your code will be reused in more and more places, by developers who may not be familiar with the details of your implementation. Therefore, your tests are an opportunity to ensure that your code is used correctly in the future. For example, if your code needs to be used in a certain way, or expects a certain configuration, or is always expected to return a certain output, or has any other details that might impact its ability to be used in the framework, write a test for it! At minimum, you'll help a future developer understand that you consciously chose to design your code a certain way.","title":"Testing"},{"location":"development/testing/#writing-tests","text":"Fidesctl's tests are stored in the tests directory. Tests should have descriptive names that make it clear what you're testing. If necessary, add a docstring or comment to explain why you're testing this specific thing. 1 2 3 4 5 6 7 # Good test name def test_dry_evaluate_system_fail ( server_url , resources_dict ): ... # Bad test name def test_dry_evaluate (): ... Fidesctl has a few pytest fixtures available for testing; see conftest.py for details.","title":"Writing tests"},{"location":"development/testing/#integration-tests-vs-mocked-tests","text":"Generally, tests that include mocking are discouraged. Mocking can create a false sense of security and obfuscate possible errors in the code that only present themselves when integration tested.","title":"Integration tests vs. Mocked tests"},{"location":"development/testing/#running-tests","text":"Fidesctl uses pytest for unit testing. To run tests, invoke pytest from the /fides/fidesctl/ directory: 1 2 cd fidesctl pytest","title":"Running tests"},{"location":"development/testing/#running-specific-tests","text":"To run a subset of tests, provide a filename or directory; to match a specific test name, use the -k flag: 1 2 # run all tests in the tests/integration directory that contain the word \"api\" in their title pytest tests/integration/ -k api The --sw flag will exit pytest the first time it encounters an error; subsequent runs with the same flag will skip any tests that succeeded and run the failed test first. For more information on available Pytest invocation options, see the documentation here .","title":"Running specific tests"},{"location":"development/testing/#excluding-external-tests","text":"Integration tests also test integration with external services like Snowflake which require internet access and authentication. It is possible to skip these tests by excluding the external mark. 1 2 # run all tests except external ones pytest -m \"not external\"","title":"Excluding external tests"},{"location":"development/testing/#ci-workflows","text":"CI will run automatically against any PR you open. Please run your tests locally first to avoid \"debugging in CI\", as this takes up resources that could be used by other contributors and is generally an inefficient usage of your time!","title":"CI Workflows"},{"location":"guides/extending_taxonomy/","text":"Extending the Default Taxonomy Fides' default taxonomy can be extended to ensure interoperability inside and outside your organization. Extending the existing categories allows the use of attribution when exporting data from Fides, and when adding context or clarity for legal teams. If you have suggestions for core categories that should ship with the taxonomy, requests can be submitted on the Fides Github . Implementing a custom Data Use A Data Use is a label that denotes the way data is used in your system. The following is an example of extending the default Data Use taxonomy : data_use.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 data_use : - fides_key : third_party_sharing.legal_obligation.payroll name : Payroll description : Legally obliged sharing of payroll information recipients : - HMRC - IRS - NYDTF legal_basis : Legal Obligation special_category : Employment parent_key : third_party_sharing.legal_obligation - fides_key : third_party_sharing.personalized_advertising.direct_marketing name : Direct Marketing description : Consented user information for direct marketing purposes recipients : - Processor - marketing co. legal_basis : Consent special_category : Consent parent_key : third_party_sharing.personalized_advertising The above example uses the existing demo_data_uses.yml from the Fides project . Further details for each field are below: Field Description fides_key Ideally extended from the existing taxonomy using the dot ( . ) separator. A string token that uniquely identifies this Data Use. name A UI-friendly name that will also be surfaced as the Purpose of Processing when exporting data from Fides. description An optional description of the purpose of processing. recipients A list of recipients of personal data for this data use. The Payroll example above has multiple recipients for tax purposes. legal_basis The legal basis category for processing, used as part of exporting data from Fides. Loosely tied to article 6 of the GDPR. special_category The special category associated to processing of personal data. Loosely tied to article 9 of the GDPR. parent_key The parent Data Use fides_key extended from. Implementing a custom Data Subject A Data Subject is a label that describes a segment of individuals whose data you store. The following is an example of extending the Data Subject taxonomy : data_subject.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 data_subject : - fides_key : potential_customer name : Potential Customer description : A prospective individual or other organization that purchases goods or services from the organization. rights : strategy : INCLUDE values : - Informed - Access - Rectification - Erasure - Object automated_decisions_or_profiling : true The above example uses the existing demo_data_subjects.yml from the Fides project . Further details for each field are below: Label Description fides_key Ideally extended from the existing taxonomy using the dot ( . ) separator. A string token that uniquely identifies this Data Use. name A UI-friendly name that will also be surfaced as the Categories of individuals when exporting data from Fides. description An optional description of the data subject. rights A strategy of how to apply data subject rights, along with an optional list to complement the strategy. automated_decisions_or_profiling If automated decision-making or profiling exists for this data subject, set as either true or false. Next Steps Once created, your new Data Subject or Data Use can be referenced as part of a privacy declaration in a system , throughout your policies , and in other Fides resources .","title":"Extending the Taxonomy"},{"location":"guides/extending_taxonomy/#extending-the-default-taxonomy","text":"Fides' default taxonomy can be extended to ensure interoperability inside and outside your organization. Extending the existing categories allows the use of attribution when exporting data from Fides, and when adding context or clarity for legal teams. If you have suggestions for core categories that should ship with the taxonomy, requests can be submitted on the Fides Github .","title":"Extending the Default Taxonomy"},{"location":"guides/extending_taxonomy/#implementing-a-custom-data-use","text":"A Data Use is a label that denotes the way data is used in your system. The following is an example of extending the default Data Use taxonomy : data_use.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 data_use : - fides_key : third_party_sharing.legal_obligation.payroll name : Payroll description : Legally obliged sharing of payroll information recipients : - HMRC - IRS - NYDTF legal_basis : Legal Obligation special_category : Employment parent_key : third_party_sharing.legal_obligation - fides_key : third_party_sharing.personalized_advertising.direct_marketing name : Direct Marketing description : Consented user information for direct marketing purposes recipients : - Processor - marketing co. legal_basis : Consent special_category : Consent parent_key : third_party_sharing.personalized_advertising The above example uses the existing demo_data_uses.yml from the Fides project . Further details for each field are below: Field Description fides_key Ideally extended from the existing taxonomy using the dot ( . ) separator. A string token that uniquely identifies this Data Use. name A UI-friendly name that will also be surfaced as the Purpose of Processing when exporting data from Fides. description An optional description of the purpose of processing. recipients A list of recipients of personal data for this data use. The Payroll example above has multiple recipients for tax purposes. legal_basis The legal basis category for processing, used as part of exporting data from Fides. Loosely tied to article 6 of the GDPR. special_category The special category associated to processing of personal data. Loosely tied to article 9 of the GDPR. parent_key The parent Data Use fides_key extended from.","title":"Implementing a custom Data Use"},{"location":"guides/extending_taxonomy/#implementing-a-custom-data-subject","text":"A Data Subject is a label that describes a segment of individuals whose data you store. The following is an example of extending the Data Subject taxonomy : data_subject.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 data_subject : - fides_key : potential_customer name : Potential Customer description : A prospective individual or other organization that purchases goods or services from the organization. rights : strategy : INCLUDE values : - Informed - Access - Rectification - Erasure - Object automated_decisions_or_profiling : true The above example uses the existing demo_data_subjects.yml from the Fides project . Further details for each field are below: Label Description fides_key Ideally extended from the existing taxonomy using the dot ( . ) separator. A string token that uniquely identifies this Data Use. name A UI-friendly name that will also be surfaced as the Categories of individuals when exporting data from Fides. description An optional description of the data subject. rights A strategy of how to apply data subject rights, along with an optional list to complement the strategy. automated_decisions_or_profiling If automated decision-making or profiling exists for this data subject, set as either true or false.","title":"Implementing a custom Data Subject"},{"location":"guides/extending_taxonomy/#next-steps","text":"Once created, your new Data Subject or Data Use can be referenced as part of a privacy declaration in a system , throughout your policies , and in other Fides resources .","title":"Next Steps"},{"location":"guides/generate_resources/","text":"Generating Resources As an alternative to manually creating resource files like in our tutorial , it is possible to generate these files using the generate CLI command. The CLI will connect to a given resource and automatically generate a non-annotated resource YAML file in the specified location. Once you have created your resources you will need to keep them up to date. The scan command is available to compare your resources and what is defined in your fidesctl server or resource files. The command will exit in error if a coverage threshold is not met. The scan and generate commands work best when used in tandem as they follow an expected resource format. The fidesctl format must be followed in order to be able to track coverage. Working With a Database The generate command can connect to a database and automatically generate resource YAML file based on the database schema. Generating a Dataset Given a database schema with a single users table as follows: 1 2 3 4 5 6 flaskr = # SELECT * FROM users; id | created_at | email | password | first_name | last_name ----+---------------------+-------------------+------------------------------------+------------+----------- 1 | 2020 -01-01 00 :00:00 | admin@example.com | pbkdf2:sha256:260000 $O87nanbSkl ... | Admin | User 2 | 2020 -01-03 00 :00:00 | user@example.com | pbkdf2:sha256:260000 $PGcBy5NzZe ... | Example | User ( 2 rows ) We can invoke the generate command by providing a connection url for this database: 1 2 3 ./venv/bin/fidesctl generate dataset db \\ postgresql://postgres:postgres@localhost:5432/flaskr \\ fides_resources/flaskr_postgres_dataset.yml The result is a resource file with a dataset with collections and fields to represent our schema: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 dataset : - fides_key : public organization_fides_key : default_organization name : public description : 'Fides Generated Description for Schema: public' meta : null data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified collections : - name : public.users description : 'Fides Generated Description for Table: public.users' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified fields : - name : created_at description : 'Fides Generated Description for Column: created_at' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - name : email description : 'Fides Generated Description for Column: email' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - name : first_name description : 'Fides Generated Description for Column: first_name' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - name : id description : 'Fides Generated Description for Column: id' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - name : last_name description : 'Fides Generated Description for Column: last_name' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - name : password description : 'Fides Generated Description for Column: password' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified The resulting file still requires annotating the dataset with data categories to represent what is stored. Scanning the Dataset The scan command can then connect to your database and compare its schema to your already defined datasets: 1 2 3 ./venv/bin/fidesctl scan dataset db \\ postgresql://postgres:postgres@localhost:5432/flaskr \\ fides_resources/flaskr_postgres_dataset.yml The command output confirms our database resource is covered fully: 1 2 3 4 5 6 Loading resource manifests from: dataset.yml Taxonomy successfully created. Successfully scanned the following datasets: public Annotation coverage: 100 % Working With an AWS Account The generate command can connect to an AWS account and automatically generate resource YAML file based on tracked resources. Providing Credentials Authentication is managed through environment variable configuration defined by boto3 . We can define our credentials directly as environment variables: 1 2 3 export AWS_ACCESS_KEY_ID = \"<my_access_key_id>\" export AWS_SECRET_ACCESS_KEY = \"<my_access_key>\" export AWS_DEFAULT_REGION = \"us-east-1\" Or reference a profile through an environment variable: 1 2 export AWS_PROFILE = \"my_profile_1\" export AWS_DEFAULT_REGION = \"us-east-1\" Required Permissions The identity which is authenticated must be allowed to invoke the following actions: * redshift:DescribeClusters * rds:DescribeDBInstances * rds:DescribeDBClusters These can be supplied in an IAM policy: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"redshift:DescribeClusters\" , \"rds:DescribeDBInstances\" , \"rds:DescribeDBClusters\" ], \"Resource\" : \"*\" } ] } Filtering AWS Resources It is possible to filter resources at the organization level by adding a resource filter within fidesctl_meta . The ignore_resource_arn filter can exclude any resources with an exact matching Amazon Resource Name (ARN) and also supports wildcards in individual ARN fields. An empty ARN field in the filter pattern works as a wildcard. The filter can be added to the organization model within your manifest file: 1 2 3 4 5 6 7 organization : - fides_key : default_organization name : default_organization fidesctl_meta : resource_filters : - type : ignore_resource_arn value : 'arn:aws:rds:us-east-1:910934740016:db:database-2' In the above example we explicitly ignore a single rds database but if we wanted to ignore all rds databases we could remove the partition, account id, region and database name ARN fields: 1 2 3 resource_filters : - type : ignore_resource_arn value : 'arn::rds:::db:' Any ARN field can be wildcarded by leaving it empty. Generating Systems Once credentials have been configured we can invoke the generate system aws command: 1 2 ./venv/bin/fidesctl generate system aws \\ fides_resources/aws_systems.yml The result is a resource file with a system that represents a redshift cluster defined in our account: 1 2 3 4 5 6 7 8 9 10 11 system : - fides_key : my_redshift_cluster organization_fides_key : default_organization name : my_redshift_cluster description : 'Fides Generated Description for Cluster: my_redshift_cluster' fidesctl_meta : endpoint_address : my_redshift_cluster.us-east-1.redshift.amazonaws.com endpoint_port : '5439' resource_id : arn:aws:redshift:us-east-1:910934740016:namespace:057d5b0e-7eaa-4012-909c-3957c7149176 system_type : redshift_cluster privacy_declarations : [] Scanning the Systems The scan command can then connect to your AWS account and compare its resources to your already defined systems: 1 2 ./venv/bin/fidesctl scan system aws \\ fides_resources/aws_systems.yml The command output confirms our resources are covered fully: 1 2 3 4 Loading resource manifests from: manifest.yml Taxonomy successfully created. Scanned 1 resource and all were found in taxonomy. Resource coverage: 100 % Working With an Okta Account The generate command can connect to an Okta admin account and automatically generate resource YAML file based on applications your organization integrates with. Providing Credentials Authentication is managed through environment variable configuration defined by the Okta Python SDK . The simplest way to authenticate is by using a client token: 1 export OKTA_CLIENT_TOKEN = \"<my_okta_client_token>\" It is also possible to authenticate using OAuth 2.0: 1 2 3 4 export OKTA_CLIENT_AUTHORIZATIONMODE = \"PrivateKey\" export OKTA_CLIENT_CLIENTID = \"<my_client_id>\" export OKTA_CLIENT_SCOPES = \"<my_scope_1,my_scope_2>\" export OKTA_CLIENT_PRIVATEKEY = \"<my_private_jwk>\" Generating Datasets Once credentials have been configured we can invoke the generate dataset okta command: 1 2 3 ./venv/bin/fidesctl generate dataset okta \\ <my_org_url> \\ fides_resources/okta_datasets.yml The result is a resource file with datasets that represent our application integrations: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 dataset : - fides_key : 0oa4jejqcp74R9MpJ5d7 organization_fides_key : default_organization name : salesforce description : 'Fides Generated Description for Okta Application: Salesforce.com' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified fidesctl_meta : resource_id : 0oa4jejqcp74R9MpJ5d7 retention : No retention or erasure policy collections : [] - fides_key : 0oa4jekd00tpvn5hN5d7 organization_fides_key : default_organization name : google description : 'Fides Generated Description for Okta Application: Google Workspace' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified fidesctl_meta : resource_id : 0oa4jekd00tpvn5hN5d7 retention : No retention or erasure policy collections : [] Scanning the Datasets The scan command can then connect to your Okta account and compare its applications to your already defined datasets: 1 2 3 ./venv/bin/fidesctl scan dataset okta \\ https://dev-78908748.okta.com \\ fides_resources/okta_datasets.yml The command output confirms our resources are covered fully: 1 2 3 4 5 6 7 8 9 10 Loading resource manifests from: manifest.yml Taxonomy successfully created. Successfully scanned the following datasets: saasure ( id = 0oa4h45lj1tcpqU6W5d7 ) okta_enduser ( id = 0oa4h45ln0xLKJnAw5d7 ) okta_browser_plugin ( id = 0oa4h45lnodX7MHJB5d7 ) salesforce ( id = 0oa4jejqcp74R9MpJ5d7 ) google ( id = 0oa4jekd00tpvn5hN5d7 ) Resource coverage: 100 %","title":"Generating Resources"},{"location":"guides/generate_resources/#generating-resources","text":"As an alternative to manually creating resource files like in our tutorial , it is possible to generate these files using the generate CLI command. The CLI will connect to a given resource and automatically generate a non-annotated resource YAML file in the specified location. Once you have created your resources you will need to keep them up to date. The scan command is available to compare your resources and what is defined in your fidesctl server or resource files. The command will exit in error if a coverage threshold is not met. The scan and generate commands work best when used in tandem as they follow an expected resource format. The fidesctl format must be followed in order to be able to track coverage.","title":"Generating Resources"},{"location":"guides/generate_resources/#working-with-a-database","text":"The generate command can connect to a database and automatically generate resource YAML file based on the database schema.","title":"Working With a Database"},{"location":"guides/generate_resources/#generating-a-dataset","text":"Given a database schema with a single users table as follows: 1 2 3 4 5 6 flaskr = # SELECT * FROM users; id | created_at | email | password | first_name | last_name ----+---------------------+-------------------+------------------------------------+------------+----------- 1 | 2020 -01-01 00 :00:00 | admin@example.com | pbkdf2:sha256:260000 $O87nanbSkl ... | Admin | User 2 | 2020 -01-03 00 :00:00 | user@example.com | pbkdf2:sha256:260000 $PGcBy5NzZe ... | Example | User ( 2 rows ) We can invoke the generate command by providing a connection url for this database: 1 2 3 ./venv/bin/fidesctl generate dataset db \\ postgresql://postgres:postgres@localhost:5432/flaskr \\ fides_resources/flaskr_postgres_dataset.yml The result is a resource file with a dataset with collections and fields to represent our schema: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 dataset : - fides_key : public organization_fides_key : default_organization name : public description : 'Fides Generated Description for Schema: public' meta : null data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified collections : - name : public.users description : 'Fides Generated Description for Table: public.users' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified fields : - name : created_at description : 'Fides Generated Description for Column: created_at' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - name : email description : 'Fides Generated Description for Column: email' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - name : first_name description : 'Fides Generated Description for Column: first_name' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - name : id description : 'Fides Generated Description for Column: id' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - name : last_name description : 'Fides Generated Description for Column: last_name' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - name : password description : 'Fides Generated Description for Column: password' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified The resulting file still requires annotating the dataset with data categories to represent what is stored.","title":"Generating a Dataset"},{"location":"guides/generate_resources/#scanning-the-dataset","text":"The scan command can then connect to your database and compare its schema to your already defined datasets: 1 2 3 ./venv/bin/fidesctl scan dataset db \\ postgresql://postgres:postgres@localhost:5432/flaskr \\ fides_resources/flaskr_postgres_dataset.yml The command output confirms our database resource is covered fully: 1 2 3 4 5 6 Loading resource manifests from: dataset.yml Taxonomy successfully created. Successfully scanned the following datasets: public Annotation coverage: 100 %","title":"Scanning the Dataset"},{"location":"guides/generate_resources/#working-with-an-aws-account","text":"The generate command can connect to an AWS account and automatically generate resource YAML file based on tracked resources.","title":"Working With an AWS Account"},{"location":"guides/generate_resources/#providing-credentials","text":"Authentication is managed through environment variable configuration defined by boto3 . We can define our credentials directly as environment variables: 1 2 3 export AWS_ACCESS_KEY_ID = \"<my_access_key_id>\" export AWS_SECRET_ACCESS_KEY = \"<my_access_key>\" export AWS_DEFAULT_REGION = \"us-east-1\" Or reference a profile through an environment variable: 1 2 export AWS_PROFILE = \"my_profile_1\" export AWS_DEFAULT_REGION = \"us-east-1\"","title":"Providing Credentials"},{"location":"guides/generate_resources/#required-permissions","text":"The identity which is authenticated must be allowed to invoke the following actions: * redshift:DescribeClusters * rds:DescribeDBInstances * rds:DescribeDBClusters These can be supplied in an IAM policy: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"redshift:DescribeClusters\" , \"rds:DescribeDBInstances\" , \"rds:DescribeDBClusters\" ], \"Resource\" : \"*\" } ] }","title":"Required Permissions"},{"location":"guides/generate_resources/#filtering-aws-resources","text":"It is possible to filter resources at the organization level by adding a resource filter within fidesctl_meta . The ignore_resource_arn filter can exclude any resources with an exact matching Amazon Resource Name (ARN) and also supports wildcards in individual ARN fields. An empty ARN field in the filter pattern works as a wildcard. The filter can be added to the organization model within your manifest file: 1 2 3 4 5 6 7 organization : - fides_key : default_organization name : default_organization fidesctl_meta : resource_filters : - type : ignore_resource_arn value : 'arn:aws:rds:us-east-1:910934740016:db:database-2' In the above example we explicitly ignore a single rds database but if we wanted to ignore all rds databases we could remove the partition, account id, region and database name ARN fields: 1 2 3 resource_filters : - type : ignore_resource_arn value : 'arn::rds:::db:' Any ARN field can be wildcarded by leaving it empty.","title":"Filtering AWS Resources"},{"location":"guides/generate_resources/#generating-systems","text":"Once credentials have been configured we can invoke the generate system aws command: 1 2 ./venv/bin/fidesctl generate system aws \\ fides_resources/aws_systems.yml The result is a resource file with a system that represents a redshift cluster defined in our account: 1 2 3 4 5 6 7 8 9 10 11 system : - fides_key : my_redshift_cluster organization_fides_key : default_organization name : my_redshift_cluster description : 'Fides Generated Description for Cluster: my_redshift_cluster' fidesctl_meta : endpoint_address : my_redshift_cluster.us-east-1.redshift.amazonaws.com endpoint_port : '5439' resource_id : arn:aws:redshift:us-east-1:910934740016:namespace:057d5b0e-7eaa-4012-909c-3957c7149176 system_type : redshift_cluster privacy_declarations : []","title":"Generating Systems"},{"location":"guides/generate_resources/#scanning-the-systems","text":"The scan command can then connect to your AWS account and compare its resources to your already defined systems: 1 2 ./venv/bin/fidesctl scan system aws \\ fides_resources/aws_systems.yml The command output confirms our resources are covered fully: 1 2 3 4 Loading resource manifests from: manifest.yml Taxonomy successfully created. Scanned 1 resource and all were found in taxonomy. Resource coverage: 100 %","title":"Scanning the Systems"},{"location":"guides/generate_resources/#working-with-an-okta-account","text":"The generate command can connect to an Okta admin account and automatically generate resource YAML file based on applications your organization integrates with.","title":"Working With an Okta Account"},{"location":"guides/generate_resources/#providing-credentials_1","text":"Authentication is managed through environment variable configuration defined by the Okta Python SDK . The simplest way to authenticate is by using a client token: 1 export OKTA_CLIENT_TOKEN = \"<my_okta_client_token>\" It is also possible to authenticate using OAuth 2.0: 1 2 3 4 export OKTA_CLIENT_AUTHORIZATIONMODE = \"PrivateKey\" export OKTA_CLIENT_CLIENTID = \"<my_client_id>\" export OKTA_CLIENT_SCOPES = \"<my_scope_1,my_scope_2>\" export OKTA_CLIENT_PRIVATEKEY = \"<my_private_jwk>\"","title":"Providing Credentials"},{"location":"guides/generate_resources/#generating-datasets","text":"Once credentials have been configured we can invoke the generate dataset okta command: 1 2 3 ./venv/bin/fidesctl generate dataset okta \\ <my_org_url> \\ fides_resources/okta_datasets.yml The result is a resource file with datasets that represent our application integrations: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 dataset : - fides_key : 0oa4jejqcp74R9MpJ5d7 organization_fides_key : default_organization name : salesforce description : 'Fides Generated Description for Okta Application: Salesforce.com' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified fidesctl_meta : resource_id : 0oa4jejqcp74R9MpJ5d7 retention : No retention or erasure policy collections : [] - fides_key : 0oa4jekd00tpvn5hN5d7 organization_fides_key : default_organization name : google description : 'Fides Generated Description for Okta Application: Google Workspace' data_categories : [] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified fidesctl_meta : resource_id : 0oa4jekd00tpvn5hN5d7 retention : No retention or erasure policy collections : []","title":"Generating Datasets"},{"location":"guides/generate_resources/#scanning-the-datasets","text":"The scan command can then connect to your Okta account and compare its applications to your already defined datasets: 1 2 3 ./venv/bin/fidesctl scan dataset okta \\ https://dev-78908748.okta.com \\ fides_resources/okta_datasets.yml The command output confirms our resources are covered fully: 1 2 3 4 5 6 7 8 9 10 Loading resource manifests from: manifest.yml Taxonomy successfully created. Successfully scanned the following datasets: saasure ( id = 0oa4h45lj1tcpqU6W5d7 ) okta_enduser ( id = 0oa4h45ln0xLKJnAw5d7 ) okta_browser_plugin ( id = 0oa4h45lnodX7MHJB5d7 ) salesforce ( id = 0oa4jejqcp74R9MpJ5d7 ) google ( id = 0oa4jekd00tpvn5hN5d7 ) Resource coverage: 100 %","title":"Scanning the Datasets"},{"location":"guides/generating_datamap/","text":"Generating a Data Map Fides is capable of exporting a data map of your resources to generate an Article 30-compliant Record of Processing Activities (RoPA). This guide will walk through generating a mock RoPA using predefined resources included in the Fides repository . To follow along, ensure you have the Fides repository cloned and fidesctl installed. Additional support for running fidesctl locally can be found in the first step of the Quick Start guide . Export the Demo Resources To apply and export the provided demo_resources , run the following commands: Apply and Export Defaults 1 2 $ fidesctl apply demo_resources/ $ fidesctl export datamap demo_resources/ This will export a data map to the demo_resources/ directory. Organization The header block at the top of a data map is composed of properties found in the Organization resource . In a production deployment, this would be composed of publicly available information for your company/organization, but has been pre-populated here to allow exploration. The newly-generated data map is a direct result of the provided Organization resource manifest ( demo_resources/demo_organization.yml ): demo_organization.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 organization : - fides_key : default_organization name : Demo Organization description : An e-commerce organization security_policy : https://ethyca.com/privacy-policy/ controller : name : Con Troller address : 123 demo street, New York, NY, USA email : controller@demo_company.com phone : +1 555 555 5555 data_protection_officer : name : DataPro Tection address : 123 demo street, New York, NY, USA email : dpo@demo_company.com phone : +1 555 555 5555 representative : name : Rep Resentative address : 123 demo street, New York, NY, USA email : representative@demo_company.com phone : +1 555 555 5555 Each of controller , data_protection_officer , and representative are composed of Contact Detail properties populated in the exported data map. Additionally, the link to the security policy of an organization can be populated from the Organization resource's security_policy field. Dataset The Dataset is primarily used to provide a list of Data Categories which populate the data map. Additional properties can optionally be applied for retention and third_country_transfers . The newly-generated data map is a direct result of the provided Dataset resource manifest ( demo_resources/demo_dataset.yml ): demo_dataset.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 dataset : - fides_key : demo_users_dataset organization_fides_key : default_organization name : Demo Users Dataset description : Data collected about users for our analytics system. meta : null data_categories : [] data_qualifiers : - aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified retention : \"30 days after account deletion\" third_country_transfers : - GBR - CAN collections : - name : users description : User information data_categories : [] data_qualifiers : - aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified fields : - name : created_at description : User's creation timestamp data_categories : - system.operations data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified ... data_categories and retention can be set at any/all of the Dataset, DatasetCollection, and DatasetField levels. third_country_transfers should be set at the dataset level. Any Datasets referenced by a System will have this information included as rows of your data map. System The System contains the remainder of the attributes on the initial data map. Each populated property is referenced directly from an associated label in fides_resources/demo_system.yml : Data Map Label Resource Label Description Fides Dataset dataset_references Used to join dataset(s) to the system. Fides System name The name defined at the top level of the system. Department or Business Function administering_department Set at the top level of the system. Purpose of Processing data_use The data_use defined in the privacy_declaration . Categories of Individuals data_subject A data_subject list defined in the privacy_declaration . Categories of Personal Data data_categories Any data_categories set as part of the privacy_declaration (see the output for Demo Marketing System as a clear example). Role or Responsibility data_responsibility_title Set at the top level of the system. Source of the Personal Data dataset_references The Fides dataset name, if referenced by the system. Data Protection Impact Assessment data_protection_impact_assessment All the information related to a Data Protection Impact Assessment, set at the top level of the system. Extend the Default Taxonomy In your initial export, several data map columns are populated with N/A . The default Fides Taxonomy can be extended to replace these empty values with additional data required as part of a Record of Processing Activities. Example manifest updates are included in demo_resources/demo_extended_taxonomy.yml . Data Use Below is an extended Data Use example. Each of these properties is responsible for populating a field on your data map. Extended Data Use 1 2 3 4 5 6 7 8 9 10 data_use : - fides_key : third_party_sharing.personalized_advertising.direct_marketing name : Direct Marketing description : User information for direct marketing purposes recipients : - Processor - marketing co. legal_basis : Legitimate Interests special_category : Vital Interests legitimate_interest_impact_assessment : https://example.org/legitimate_interest_assessment parent_key : third_party_sharing.personalized_advertising You can now apply this Data Use to the Demo Marketing System in demo_system.yml . Replace the Demo Marketing System's Data Use of advertising with the above fides_key of third_party_sharing.personalized_advertising.direct_marketing to include its information in your data map. Data Subject A Data Subject , shown below, can also be extended to populate your data map with additional information. Extended Data Subject 1 2 3 4 5 6 7 8 9 10 11 12 13 data_subject : - fides_key : potential_customer name : Potential Customer description : A prospective individual or other organization that purchases goods or services from the organization. rights : strategy : INCLUDE values : - Informed - Access - Rectification - Erasure - Object automated_decisions_or_profiling : true You can now apply this Data Subject to the Demo Marketing System in demo_system.yml . Replace the Demo Marketing System's Data Subject of customer with the above fides_key of potential_customer to include its information in your data map. Generate a RoPA Now that you have added the additional information around privacy notices and data subject rights, you can export a fresh copy of your data map: Apply and Export Defaults 1 2 $ fidesctl apply demo_resources/ $ fidesctl export datamap demo_resources/ Populated Fields Opening the new data map will show the previously N/A columns are now populated, resulting in an Article 30-compliant RoPA for one of the two systems defined in demo_resources/ . Below is a mapping of the newly populated columns with their respective values: Data Map Label Description Purpose of Processing The name of your newly extended data_use set in a Privacy Declaration. Categories of Individuals The name of your newly extended data_subject set in a Privacy Declaration. Categories of Recipients The recipients defined in your extended Data Use. Article 6 Lawful Basis for Processing Personal Data The legal_basis defined in your extended Data Use. Article 9 Condition for Processing Special Category Data The special_category defined in your extended Data Use. Legitimate Interests for the Processing If the legal_basis is \"Legitimate Interests\" , the Data Use name is used to identify what the legitimate interest data use is. Link to Record of Legitimate Interests Assessment If the legal_basis is \"Legitimate Interests\" , a legitimate interests impact assessment is required and should be set using the legitimate_interest_impact_assessment property. Rights Available to Individuals The rights defined in your extended Data Subject based on the strategy used. Existence of Automated Decision-Making, Including Profiling The boolean value for automated_decisions_or_profiling , defined in your extended Data Subject. Additional Learning The provided demo_system.yml includes a second System, Demo Analytics , which can be enhanced in the same way as the Demo Marketing System. Follow the guide to extend the taxonomy again, this time for the Demo Analytics System, to have both systems fully compliant! Next Steps We hope this was helpful in understanding the additional properties required for generating an Article 30 compliant RoPA! If there are any questions or issues you still have, we would love to hear more from you in our Slack Community or in an issue/PR on GitHub .","title":"Generating a Data Map"},{"location":"guides/generating_datamap/#generating-a-data-map","text":"Fides is capable of exporting a data map of your resources to generate an Article 30-compliant Record of Processing Activities (RoPA). This guide will walk through generating a mock RoPA using predefined resources included in the Fides repository . To follow along, ensure you have the Fides repository cloned and fidesctl installed. Additional support for running fidesctl locally can be found in the first step of the Quick Start guide .","title":"Generating a Data Map"},{"location":"guides/generating_datamap/#export-the-demo-resources","text":"To apply and export the provided demo_resources , run the following commands: Apply and Export Defaults 1 2 $ fidesctl apply demo_resources/ $ fidesctl export datamap demo_resources/ This will export a data map to the demo_resources/ directory.","title":"Export the Demo Resources"},{"location":"guides/generating_datamap/#organization","text":"The header block at the top of a data map is composed of properties found in the Organization resource . In a production deployment, this would be composed of publicly available information for your company/organization, but has been pre-populated here to allow exploration. The newly-generated data map is a direct result of the provided Organization resource manifest ( demo_resources/demo_organization.yml ): demo_organization.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 organization : - fides_key : default_organization name : Demo Organization description : An e-commerce organization security_policy : https://ethyca.com/privacy-policy/ controller : name : Con Troller address : 123 demo street, New York, NY, USA email : controller@demo_company.com phone : +1 555 555 5555 data_protection_officer : name : DataPro Tection address : 123 demo street, New York, NY, USA email : dpo@demo_company.com phone : +1 555 555 5555 representative : name : Rep Resentative address : 123 demo street, New York, NY, USA email : representative@demo_company.com phone : +1 555 555 5555 Each of controller , data_protection_officer , and representative are composed of Contact Detail properties populated in the exported data map. Additionally, the link to the security policy of an organization can be populated from the Organization resource's security_policy field.","title":"Organization"},{"location":"guides/generating_datamap/#dataset","text":"The Dataset is primarily used to provide a list of Data Categories which populate the data map. Additional properties can optionally be applied for retention and third_country_transfers . The newly-generated data map is a direct result of the provided Dataset resource manifest ( demo_resources/demo_dataset.yml ): demo_dataset.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 dataset : - fides_key : demo_users_dataset organization_fides_key : default_organization name : Demo Users Dataset description : Data collected about users for our analytics system. meta : null data_categories : [] data_qualifiers : - aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified retention : \"30 days after account deletion\" third_country_transfers : - GBR - CAN collections : - name : users description : User information data_categories : [] data_qualifiers : - aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified fields : - name : created_at description : User's creation timestamp data_categories : - system.operations data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified ... data_categories and retention can be set at any/all of the Dataset, DatasetCollection, and DatasetField levels. third_country_transfers should be set at the dataset level. Any Datasets referenced by a System will have this information included as rows of your data map.","title":"Dataset"},{"location":"guides/generating_datamap/#system","text":"The System contains the remainder of the attributes on the initial data map. Each populated property is referenced directly from an associated label in fides_resources/demo_system.yml : Data Map Label Resource Label Description Fides Dataset dataset_references Used to join dataset(s) to the system. Fides System name The name defined at the top level of the system. Department or Business Function administering_department Set at the top level of the system. Purpose of Processing data_use The data_use defined in the privacy_declaration . Categories of Individuals data_subject A data_subject list defined in the privacy_declaration . Categories of Personal Data data_categories Any data_categories set as part of the privacy_declaration (see the output for Demo Marketing System as a clear example). Role or Responsibility data_responsibility_title Set at the top level of the system. Source of the Personal Data dataset_references The Fides dataset name, if referenced by the system. Data Protection Impact Assessment data_protection_impact_assessment All the information related to a Data Protection Impact Assessment, set at the top level of the system.","title":"System"},{"location":"guides/generating_datamap/#extend-the-default-taxonomy","text":"In your initial export, several data map columns are populated with N/A . The default Fides Taxonomy can be extended to replace these empty values with additional data required as part of a Record of Processing Activities. Example manifest updates are included in demo_resources/demo_extended_taxonomy.yml .","title":"Extend the Default Taxonomy"},{"location":"guides/generating_datamap/#data-use","text":"Below is an extended Data Use example. Each of these properties is responsible for populating a field on your data map. Extended Data Use 1 2 3 4 5 6 7 8 9 10 data_use : - fides_key : third_party_sharing.personalized_advertising.direct_marketing name : Direct Marketing description : User information for direct marketing purposes recipients : - Processor - marketing co. legal_basis : Legitimate Interests special_category : Vital Interests legitimate_interest_impact_assessment : https://example.org/legitimate_interest_assessment parent_key : third_party_sharing.personalized_advertising You can now apply this Data Use to the Demo Marketing System in demo_system.yml . Replace the Demo Marketing System's Data Use of advertising with the above fides_key of third_party_sharing.personalized_advertising.direct_marketing to include its information in your data map.","title":"Data Use"},{"location":"guides/generating_datamap/#data-subject","text":"A Data Subject , shown below, can also be extended to populate your data map with additional information. Extended Data Subject 1 2 3 4 5 6 7 8 9 10 11 12 13 data_subject : - fides_key : potential_customer name : Potential Customer description : A prospective individual or other organization that purchases goods or services from the organization. rights : strategy : INCLUDE values : - Informed - Access - Rectification - Erasure - Object automated_decisions_or_profiling : true You can now apply this Data Subject to the Demo Marketing System in demo_system.yml . Replace the Demo Marketing System's Data Subject of customer with the above fides_key of potential_customer to include its information in your data map.","title":"Data Subject"},{"location":"guides/generating_datamap/#generate-a-ropa","text":"Now that you have added the additional information around privacy notices and data subject rights, you can export a fresh copy of your data map: Apply and Export Defaults 1 2 $ fidesctl apply demo_resources/ $ fidesctl export datamap demo_resources/","title":"Generate a RoPA"},{"location":"guides/generating_datamap/#populated-fields","text":"Opening the new data map will show the previously N/A columns are now populated, resulting in an Article 30-compliant RoPA for one of the two systems defined in demo_resources/ . Below is a mapping of the newly populated columns with their respective values: Data Map Label Description Purpose of Processing The name of your newly extended data_use set in a Privacy Declaration. Categories of Individuals The name of your newly extended data_subject set in a Privacy Declaration. Categories of Recipients The recipients defined in your extended Data Use. Article 6 Lawful Basis for Processing Personal Data The legal_basis defined in your extended Data Use. Article 9 Condition for Processing Special Category Data The special_category defined in your extended Data Use. Legitimate Interests for the Processing If the legal_basis is \"Legitimate Interests\" , the Data Use name is used to identify what the legitimate interest data use is. Link to Record of Legitimate Interests Assessment If the legal_basis is \"Legitimate Interests\" , a legitimate interests impact assessment is required and should be set using the legitimate_interest_impact_assessment property. Rights Available to Individuals The rights defined in your extended Data Subject based on the strategy used. Existence of Automated Decision-Making, Including Profiling The boolean value for automated_decisions_or_profiling , defined in your extended Data Subject.","title":"Populated Fields"},{"location":"guides/generating_datamap/#additional-learning","text":"The provided demo_system.yml includes a second System, Demo Analytics , which can be enhanced in the same way as the Demo Marketing System. Follow the guide to extend the taxonomy again, this time for the Demo Analytics System, to have both systems fully compliant!","title":"Additional Learning"},{"location":"guides/generating_datamap/#next-steps","text":"We hope this was helpful in understanding the additional properties required for generating an Article 30 compliant RoPA! If there are any questions or issues you still have, we would love to hear more from you in our Slack Community or in an issue/PR on GitHub .","title":"Next Steps"},{"location":"guides/policies/","text":"Example Policies This page gives a few examples of different policies one might want to use in their own organizations. No Third-Party Data Sharing data_sharing_policy.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 policy : - fides_key : data_sharing_policy name : Data Sharing description : The privacy policy that governs sharing of data with third parties. rules : - name : Disallow Third-Party Marketing description : Disallow collecting any user contact info to use for marketing. data_categories : matches : ANY # If any of these data categories are being used values : - account - user data_uses : matches : ANY # And the use of the data is for third-party sharing values : - third_party_sharing data_subjects : matches : ANY # And the data subject is a customer values : - customer # And the data is identifiable, trigger a violation data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified Respecting Employee Data Privacy employee_data_processing_policy.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 policy : - fides_key : employee_data_processing name : Employee Data Processing description : Restrict employee data processing only to that which is required for systematic business functions. rules : - name : Disallow Non-System Use of Employee Data data_categories : matches : ANY # If any of these data categories are being used values : - account - user data_uses : matches : OTHER # And the use of the data is something other than... values : - provide.system.operations - improve.system - collect data_subjects : matches : ANY # And the data subject is an employee values : - employee # And the data is identifiable, trigger a violation data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified Respecting Biometric PII biometric_data_policy.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 policy : - fides_key : biometric_data_policy name : Biometric Data description : Policy that describes valid uses of biometric and health data. rules : - name : Disallow Biometrics for Profit. description : Disallow the use of biometric data for profit-related purposes. data_categories : matches : ANY # If any of these data categories are being used values : - user.derived - user.provided.identifiable.credentials.biometric_credentials - user.provided.identifiable.biometric data_uses : matches : ANY # And the use of the data is for any of the following... values : - advertising - train_ai_system - improve - third_party_sharing data_subjects : matches : ANY # And the data subject is a customer values : - customer # And the data is identifiable, trigger a violation data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified Anonymous Derived User Contact Data derived_user_data_policy.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 policy : - fides_key : protect_derived_user_data name : Protect Derived User Data description : Policy that describes the proper use of derived user data. rules : - name : Disallow Non-Anonymized Derived User Data. description : Require that any use of derived user data must be de-identified to the anonymous level, as opposed to the pseudonymous. data_categories : matches : ANY # If any of these data categories are being used values : - user - account data_uses : matches : NONE # And for any data use values : [] data_subjects : matches : ANY # And the data subject is a customer values : - customer # And the data is either pseudonymized or more identifiable, trigger a violation data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized Phone Numbers for Transactional Messaging transactional_messaging_policy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 policy : - fides_key : transactional_messaging_policy rules : - name : Transactional Messaging only for phone numbers. description : Allows use of phone numbers for transactional messaging only. data_categories : matches : ANY # If any of these data categories are being used values : - user.provided.identifiable.contact.phone_number data_uses : matches : OTHER # And a data use other than these have been declared values : - provide.system.operations - provide.system.operations.support data_subjects : matches : ANY # And the data subject is a customer values : - customer # And the data is identifiable, trigger a violation data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified","title":"Example Policies"},{"location":"guides/policies/#example-policies","text":"This page gives a few examples of different policies one might want to use in their own organizations.","title":"Example Policies"},{"location":"guides/policies/#no-third-party-data-sharing","text":"data_sharing_policy.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 policy : - fides_key : data_sharing_policy name : Data Sharing description : The privacy policy that governs sharing of data with third parties. rules : - name : Disallow Third-Party Marketing description : Disallow collecting any user contact info to use for marketing. data_categories : matches : ANY # If any of these data categories are being used values : - account - user data_uses : matches : ANY # And the use of the data is for third-party sharing values : - third_party_sharing data_subjects : matches : ANY # And the data subject is a customer values : - customer # And the data is identifiable, trigger a violation data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified","title":"No Third-Party Data Sharing"},{"location":"guides/policies/#respecting-employee-data-privacy","text":"employee_data_processing_policy.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 policy : - fides_key : employee_data_processing name : Employee Data Processing description : Restrict employee data processing only to that which is required for systematic business functions. rules : - name : Disallow Non-System Use of Employee Data data_categories : matches : ANY # If any of these data categories are being used values : - account - user data_uses : matches : OTHER # And the use of the data is something other than... values : - provide.system.operations - improve.system - collect data_subjects : matches : ANY # And the data subject is an employee values : - employee # And the data is identifiable, trigger a violation data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified","title":"Respecting Employee Data Privacy"},{"location":"guides/policies/#respecting-biometric-pii","text":"biometric_data_policy.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 policy : - fides_key : biometric_data_policy name : Biometric Data description : Policy that describes valid uses of biometric and health data. rules : - name : Disallow Biometrics for Profit. description : Disallow the use of biometric data for profit-related purposes. data_categories : matches : ANY # If any of these data categories are being used values : - user.derived - user.provided.identifiable.credentials.biometric_credentials - user.provided.identifiable.biometric data_uses : matches : ANY # And the use of the data is for any of the following... values : - advertising - train_ai_system - improve - third_party_sharing data_subjects : matches : ANY # And the data subject is a customer values : - customer # And the data is identifiable, trigger a violation data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified","title":"Respecting Biometric PII"},{"location":"guides/policies/#anonymous-derived-user-contact-data","text":"derived_user_data_policy.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 policy : - fides_key : protect_derived_user_data name : Protect Derived User Data description : Policy that describes the proper use of derived user data. rules : - name : Disallow Non-Anonymized Derived User Data. description : Require that any use of derived user data must be de-identified to the anonymous level, as opposed to the pseudonymous. data_categories : matches : ANY # If any of these data categories are being used values : - user - account data_uses : matches : NONE # And for any data use values : [] data_subjects : matches : ANY # And the data subject is a customer values : - customer # And the data is either pseudonymized or more identifiable, trigger a violation data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized","title":"Anonymous Derived User Contact Data"},{"location":"guides/policies/#phone-numbers-for-transactional-messaging","text":"transactional_messaging_policy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 policy : - fides_key : transactional_messaging_policy rules : - name : Transactional Messaging only for phone numbers. description : Allows use of phone numbers for transactional messaging only. data_categories : matches : ANY # If any of these data categories are being used values : - user.provided.identifiable.contact.phone_number data_uses : matches : OTHER # And a data use other than these have been declared values : - provide.system.operations - provide.system.operations.support data_subjects : matches : ANY # And the data subject is a customer values : - customer # And the data is identifiable, trigger a violation data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified","title":"Phone Numbers for Transactional Messaging"},{"location":"installation/configuration/","text":"Configuration Fidesctl supports two methods of configuration. The first is via a toml file, and the second is via environment variables. They can also be used in tandem, with the environment variables overriding the toml configuration values. By default the fidesctl CLI doesn't require a config file and will instead leverage the default values. These are very likely to be wrong however so it is recommended to always configure your settings properly. Configuration file After initializing fidesctl, a default configuration file will be generated and placed within the .fides directory. Here's an example of a default fidesctl configuration file: fidesctl.toml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [api] database_user = \"postgres\" database_password = \"fidesctl\" database_host = \"fidesctl-db\" database_port = \"5432\" database_name = \"fidesctl\" # Logging log_destination = \"\" log_level = INFO log_serialization = \"\" [cli] local_mode = False server_host = \"localhost\" server_port = 8080 server_protocol = \"http analytics_id = \" test_analytics_id \" [user] encryption_key = \" test_encryption_key \" analytics_opt_out = false To better describe the various configuration options, the following tables describe each available option, grouped by section: API Section Name Type Default Description database_user String postgres The username of the Postgres account. database_password String fidesctl The password of the Postgres account. database_host String fidesctl-db The hostname of the Postgres database server. database_port String 5432 The port of the Postgres database server. database_name String fidesctl The name of the Postgres database. test_database_name String \"\" Used instead of the database_name when the FIDESCTL_TEST_MODE environment variable is set to True , to avoid overwriting production data. log_destination String \"\" The output location for log files. Accepts any valid file path. If left unset, log entries are printed to stdout and log files are not produced. log_level Enum (String) INFO The minimum log entry level to produce. Also accepts: TRACE , DEBUG , WARNING , ERROR , or CRITICAL (case insensitive). log_serialization Enum (String) \"\" The format with which to produce log entries. If left unset, produces log entries formatted using the internal custom formatter. Also accepts: \"JSON\" (case insensitive). CLI Section Name Type Default Description local_mode Boolean False When set to True , forbids the fidesctl CLI from making calls to the fidesctl webserver. server_host String localhost The hostname of the fidesctl webserver. server_protocol String http The protocol used by the fidesctl webserver. server_port Integer The optional port of the fidesctl webserver. analytics_id String \"\" A fully anonymized unique identifier for the fidesctl CLI instance. User Section Name Type Default Description encryption_key String \"\" An arbitrary string used to encrypt the user data stored in the database. Encryption is implemented using PGP. analytics_opt_out Boolean \"\" When set to true , prevents sending anonymous analytics data to Ethyca. By default fidesctl will look for a fidesctl.toml configuration file in the following places: At the path specified using the config file argument passed through the CLI At the path specified by the FIDESCTL_CONFIG_PATH environment variable In a .fides directory within the current working directory In a .fides directory within the user's home directory Environment Variables To configure environment variables for fidesctl, the following pattern is used: 1 FIDESCTL__<SECTION>__<VAR_NAME> For example, if we want to set the server_url on a Linux machine we could use: 1 2 3 export FIDESCTL__CLI__SERVER_HOST = \"localhost\" export FIDESCTL__CLI__SERVER_PORT = \"8080\" export FIDESCTL__CLI__SERVER_PROTOCOL = \"http\"","title":"Configuration"},{"location":"installation/configuration/#configuration","text":"Fidesctl supports two methods of configuration. The first is via a toml file, and the second is via environment variables. They can also be used in tandem, with the environment variables overriding the toml configuration values. By default the fidesctl CLI doesn't require a config file and will instead leverage the default values. These are very likely to be wrong however so it is recommended to always configure your settings properly.","title":"Configuration"},{"location":"installation/configuration/#configuration-file","text":"After initializing fidesctl, a default configuration file will be generated and placed within the .fides directory. Here's an example of a default fidesctl configuration file: fidesctl.toml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [api] database_user = \"postgres\" database_password = \"fidesctl\" database_host = \"fidesctl-db\" database_port = \"5432\" database_name = \"fidesctl\" # Logging log_destination = \"\" log_level = INFO log_serialization = \"\" [cli] local_mode = False server_host = \"localhost\" server_port = 8080 server_protocol = \"http analytics_id = \" test_analytics_id \" [user] encryption_key = \" test_encryption_key \" analytics_opt_out = false To better describe the various configuration options, the following tables describe each available option, grouped by section: API Section Name Type Default Description database_user String postgres The username of the Postgres account. database_password String fidesctl The password of the Postgres account. database_host String fidesctl-db The hostname of the Postgres database server. database_port String 5432 The port of the Postgres database server. database_name String fidesctl The name of the Postgres database. test_database_name String \"\" Used instead of the database_name when the FIDESCTL_TEST_MODE environment variable is set to True , to avoid overwriting production data. log_destination String \"\" The output location for log files. Accepts any valid file path. If left unset, log entries are printed to stdout and log files are not produced. log_level Enum (String) INFO The minimum log entry level to produce. Also accepts: TRACE , DEBUG , WARNING , ERROR , or CRITICAL (case insensitive). log_serialization Enum (String) \"\" The format with which to produce log entries. If left unset, produces log entries formatted using the internal custom formatter. Also accepts: \"JSON\" (case insensitive). CLI Section Name Type Default Description local_mode Boolean False When set to True , forbids the fidesctl CLI from making calls to the fidesctl webserver. server_host String localhost The hostname of the fidesctl webserver. server_protocol String http The protocol used by the fidesctl webserver. server_port Integer The optional port of the fidesctl webserver. analytics_id String \"\" A fully anonymized unique identifier for the fidesctl CLI instance. User Section Name Type Default Description encryption_key String \"\" An arbitrary string used to encrypt the user data stored in the database. Encryption is implemented using PGP. analytics_opt_out Boolean \"\" When set to true , prevents sending anonymous analytics data to Ethyca. By default fidesctl will look for a fidesctl.toml configuration file in the following places: At the path specified using the config file argument passed through the CLI At the path specified by the FIDESCTL_CONFIG_PATH environment variable In a .fides directory within the current working directory In a .fides directory within the user's home directory","title":"Configuration file"},{"location":"installation/configuration/#environment-variables","text":"To configure environment variables for fidesctl, the following pattern is used: 1 FIDESCTL__<SECTION>__<VAR_NAME> For example, if we want to set the server_url on a Linux machine we could use: 1 2 3 export FIDESCTL__CLI__SERVER_HOST = \"localhost\" export FIDESCTL__CLI__SERVER_PORT = \"8080\" export FIDESCTL__CLI__SERVER_PROTOCOL = \"http\"","title":"Environment Variables"},{"location":"installation/database/","text":"Setting up the database The fidesctl webserver is the only part of the application that touches the fidesctl database directly. It will automatically run migrations and seed the database with the default taxonomy on startup. If needed, you can also run fidesctl db init or fidesctl db reset via the CLI, which will tell the webserver to execute those actions, although these should not be needed under normal circumstances.","title":"Setting up the database"},{"location":"installation/database/#setting-up-the-database","text":"The fidesctl webserver is the only part of the application that touches the fidesctl database directly. It will automatically run migrations and seed the database with the default taxonomy on startup. If needed, you can also run fidesctl db init or fidesctl db reset via the CLI, which will tell the webserver to execute those actions, although these should not be needed under normal circumstances.","title":"Setting up the database"},{"location":"installation/docker/","text":"Installation from Docker For the ease of deployment in production, the community releases a production-ready reference container image. The fidesctl community releases Docker Images which are reference images for fidesctl. Every time a new version of fidesctl is released, the images are available in the ethyca/fidesctl DockerHub . There are also mid-release versions (dirty versions) that get uploaded to DockerHub on every commit to the main branch. These reference images contain all of the extras and dependencies for running the Python application. However they do not contain the required Postgres database. Fidesctl requires multiple components to function as it is a multi-part application. You may therefore also be interested in launching fidesctl in the Docker Compose environment, see: Running Fidesctl in Docker .","title":"Installation from Docker"},{"location":"installation/docker/#installation-from-docker","text":"For the ease of deployment in production, the community releases a production-ready reference container image. The fidesctl community releases Docker Images which are reference images for fidesctl. Every time a new version of fidesctl is released, the images are available in the ethyca/fidesctl DockerHub . There are also mid-release versions (dirty versions) that get uploaded to DockerHub on every commit to the main branch. These reference images contain all of the extras and dependencies for running the Python application. However they do not contain the required Postgres database. Fidesctl requires multiple components to function as it is a multi-part application. You may therefore also be interested in launching fidesctl in the Docker Compose environment, see: Running Fidesctl in Docker .","title":"Installation from Docker"},{"location":"installation/installation/","text":"Installation This page describes installation options that you might use when considering how to install fidesctl. Fidesctl consists of multiple components, possibly distributed among various physical or virtual machines. Fidesctl can be deployed flexibly, meeting the needs of various environments with different levels of complexity. You should also check-out the prerequisites that must be fulfilled when installing fidesctl. Fidesctl requires additional dependencies to be installed - which can be done via extras . When you install fidesctl, you need to setup the database which must also be kept updated when fidesctl is upgraded. Installation Tools Only pip installations are currently officially supported. For more details see Installation from PyPI In some cases a lightweight installation might be desired, for instance, if the webserver is not needed. If this is the case, our pip installation supports optional dependencies. While there are some successes with using other tools like poetry or pip-tools, they do not share the same workflow as the supported tools - especially when it comes to constraint vs. requirements management. Installing via Poetry or pip-tools is not currently supported. If you wish to install fidesctl using those tools you do so at your own discretion. When this option works best This installation method is useful when you are not familiar with containers and Docker and want to install fidesctl on physical or virtual machines and you are used to installing and running software using custom deployment mechanism. The only officially supported mechanisms of installation is pip. Intended users Users who are familiar with installing and configuring Python applications, managing Python environments, dependencies and running software with their custom deployment mechanisms. What are you expected to handle You are expected to install fidesctl - all components of it - on your own. You should develop and handle the deployment for all components of fidesctl. You are responsible for setting up the database, automated startup and recovery, maintenance, cleanup and upgrades of fidesctl. What the Fidesctl community provides for this method You have Installation from PyPI on how to install the software but due to various environments and tools you might want to use, you might expect that there will be problems which are specific to your deployment and environment that you will have to diagnose and solve. You have the Running fidesctl Locally guide where you can see an example of running fidesctl with minimal dependencies and setup. You can use this guide to start fidesctl quickly for local testing and development, however this is only intended to provide inspiration, not to represent a production-grade installation. Where to ask for help For quick and general troubleshooting questions, visit the #troubleshooting channel on the fidesctl Slack. For longer discussions or to share information, visit the GitHub discussions page. If you can provide description of a reproducible problem with the fidesctl software, you can open issue in GitHub issues . Using Production Docker Images More details: Installation from Docker When this option works best This installation method is useful if you are familiar with the container/Docker stack. It provides a capability of running fidesctl components in isolation from other software running on the same physical or virtual machines with easy maintenance of dependencies. The images are built by fidesctl CI/CD pipelines and offer versions for official releases as well as for every commit made to the main branch. For this reason, it is highly discouraged to use the latest tag, as any non-official release versions may contain some instability. Intended users Users who are familiar with containers and Docker stack and understand how to build and extend their own container images. Users who know how to create deployments using Docker by linking together multiple Docker containers and maintaining such deployments. What are you expected to handle You are expected to be able to customize or extend container/Docker images if you want to add extra dependencies. You are expected to put together a deployment built of several containers (for example using docker-compose) and to make sure that they are linked together. You are responsible for setting up the database, automated startup and recovery, maintenance, cleanup and upgrades of fidesctl. You should choose the right deployment mechanism. There a number of available options of deployments of containers. You can use your own custom mechanism, custom Kubernetes deployments, custom Docker Compose, custom Helm charts etc., and you should choose it based on your experience and expectations. What the Fidesctl community provides for this method You have Running Fidesctl in Docker where you can see an example of how to start fidesctl quickly for local testing and development. However this is just an inspiration. Do not expect to use this docker-compose.yml file for production installation, you need to get familiar with Docker Compose and its capabilities and build your own production-ready deployment with it if you choose Docker Compose for your deployment. The Docker Image is managed by the same people who build fidesctl, and they are committed to keeping it updated whenever new features and capabilities of fidesctl are released. Where to ask for help For quick and general troubleshooting questions, visit the #troubleshooting channel on the fidesctl Slack. For longer discussions or to share information, visit the GitHub discussions page. If you can provide description of a reproducible problem with the fidesctl software, you can open issue in GitHub issues .","title":"Installation Overview"},{"location":"installation/installation/#installation","text":"This page describes installation options that you might use when considering how to install fidesctl. Fidesctl consists of multiple components, possibly distributed among various physical or virtual machines. Fidesctl can be deployed flexibly, meeting the needs of various environments with different levels of complexity. You should also check-out the prerequisites that must be fulfilled when installing fidesctl. Fidesctl requires additional dependencies to be installed - which can be done via extras . When you install fidesctl, you need to setup the database which must also be kept updated when fidesctl is upgraded.","title":"Installation"},{"location":"installation/installation/#installation-tools","text":"Only pip installations are currently officially supported. For more details see Installation from PyPI In some cases a lightweight installation might be desired, for instance, if the webserver is not needed. If this is the case, our pip installation supports optional dependencies. While there are some successes with using other tools like poetry or pip-tools, they do not share the same workflow as the supported tools - especially when it comes to constraint vs. requirements management. Installing via Poetry or pip-tools is not currently supported. If you wish to install fidesctl using those tools you do so at your own discretion. When this option works best This installation method is useful when you are not familiar with containers and Docker and want to install fidesctl on physical or virtual machines and you are used to installing and running software using custom deployment mechanism. The only officially supported mechanisms of installation is pip. Intended users Users who are familiar with installing and configuring Python applications, managing Python environments, dependencies and running software with their custom deployment mechanisms. What are you expected to handle You are expected to install fidesctl - all components of it - on your own. You should develop and handle the deployment for all components of fidesctl. You are responsible for setting up the database, automated startup and recovery, maintenance, cleanup and upgrades of fidesctl. What the Fidesctl community provides for this method You have Installation from PyPI on how to install the software but due to various environments and tools you might want to use, you might expect that there will be problems which are specific to your deployment and environment that you will have to diagnose and solve. You have the Running fidesctl Locally guide where you can see an example of running fidesctl with minimal dependencies and setup. You can use this guide to start fidesctl quickly for local testing and development, however this is only intended to provide inspiration, not to represent a production-grade installation. Where to ask for help For quick and general troubleshooting questions, visit the #troubleshooting channel on the fidesctl Slack. For longer discussions or to share information, visit the GitHub discussions page. If you can provide description of a reproducible problem with the fidesctl software, you can open issue in GitHub issues .","title":"Installation Tools"},{"location":"installation/installation/#using-production-docker-images","text":"More details: Installation from Docker When this option works best This installation method is useful if you are familiar with the container/Docker stack. It provides a capability of running fidesctl components in isolation from other software running on the same physical or virtual machines with easy maintenance of dependencies. The images are built by fidesctl CI/CD pipelines and offer versions for official releases as well as for every commit made to the main branch. For this reason, it is highly discouraged to use the latest tag, as any non-official release versions may contain some instability. Intended users Users who are familiar with containers and Docker stack and understand how to build and extend their own container images. Users who know how to create deployments using Docker by linking together multiple Docker containers and maintaining such deployments. What are you expected to handle You are expected to be able to customize or extend container/Docker images if you want to add extra dependencies. You are expected to put together a deployment built of several containers (for example using docker-compose) and to make sure that they are linked together. You are responsible for setting up the database, automated startup and recovery, maintenance, cleanup and upgrades of fidesctl. You should choose the right deployment mechanism. There a number of available options of deployments of containers. You can use your own custom mechanism, custom Kubernetes deployments, custom Docker Compose, custom Helm charts etc., and you should choose it based on your experience and expectations. What the Fidesctl community provides for this method You have Running Fidesctl in Docker where you can see an example of how to start fidesctl quickly for local testing and development. However this is just an inspiration. Do not expect to use this docker-compose.yml file for production installation, you need to get familiar with Docker Compose and its capabilities and build your own production-ready deployment with it if you choose Docker Compose for your deployment. The Docker Image is managed by the same people who build fidesctl, and they are committed to keeping it updated whenever new features and capabilities of fidesctl are released. Where to ask for help For quick and general troubleshooting questions, visit the #troubleshooting channel on the fidesctl Slack. For longer discussions or to share information, visit the GitHub discussions page. If you can provide description of a reproducible problem with the fidesctl software, you can open issue in GitHub issues .","title":"Using Production Docker Images"},{"location":"installation/prerequisites_dependencies/","text":"Prerequisites & Dependencies Fidesctl supports the following Python versions: Python: 3.8 Fidesctl supports the following databases as the application database: PostgreSQL: 12 All of these must be installed, either locally or via Docker, before attempting to run fidesctl as an application. The CLI is capable of running with only the Python dependency.","title":"Prerequisites & Dependencies"},{"location":"installation/prerequisites_dependencies/#prerequisites-dependencies","text":"Fidesctl supports the following Python versions: Python: 3.8 Fidesctl supports the following databases as the application database: PostgreSQL: 12 All of these must be installed, either locally or via Docker, before attempting to run fidesctl as an application. The CLI is capable of running with only the Python dependency.","title":"Prerequisites &amp; Dependencies"},{"location":"installation/pypi/","text":"Installation from PyPI This page describes installations using the fidesctl package published on PyPI . Basic Installation To install Fidesctl, run: pip install fidesctl With the default installation fidesctl is designed to be as lightweight as possible. It ships with the ability to do most things you would do via the standalone CLI, such as evaluate and parse without the need to run a webserver. For interacting directly with databases and running the webserver, see the optional dependencies below. Installing Optional Dependencies Fidesctl ships with a number of optional dependencies that extend its functionality. To install these, use the following syntax: pip install \"fidesctl[extra_1, extra_2]\" The optional dependencies are as follows: all : includes all of the optional dependencies except for mssql due to platform-specific issues. aws : includes the boto3 package to connect to AWS. mssql : includes the MSSQL database connector. mysql : includes the MySQL database connector. postgres : includes the Postgres database connector. redshift : includes the Redshift database connector. snowflake : includes the Snowflake database connector. webserver : includes FastAPI and the Postgres database connector. Enables fidesctl webserver . NOTE: When installing database adapters there may be other dependencies, such as the pg_hba.conf file that usually requires a Postgres installation or the Microsoft ODBC Driver for SQL Server Apple M1 users of MSSQL: Known issues around connecting to MSSQL exist today, please reference the following issue for potential solutions: https://github.com/mkleehammer/pyodbc/issues/846","title":"Installation from PyPI"},{"location":"installation/pypi/#installation-from-pypi","text":"This page describes installations using the fidesctl package published on PyPI .","title":"Installation from PyPI"},{"location":"installation/pypi/#basic-installation","text":"To install Fidesctl, run: pip install fidesctl With the default installation fidesctl is designed to be as lightweight as possible. It ships with the ability to do most things you would do via the standalone CLI, such as evaluate and parse without the need to run a webserver. For interacting directly with databases and running the webserver, see the optional dependencies below.","title":"Basic Installation"},{"location":"installation/pypi/#installing-optional-dependencies","text":"Fidesctl ships with a number of optional dependencies that extend its functionality. To install these, use the following syntax: pip install \"fidesctl[extra_1, extra_2]\" The optional dependencies are as follows: all : includes all of the optional dependencies except for mssql due to platform-specific issues. aws : includes the boto3 package to connect to AWS. mssql : includes the MSSQL database connector. mysql : includes the MySQL database connector. postgres : includes the Postgres database connector. redshift : includes the Redshift database connector. snowflake : includes the Snowflake database connector. webserver : includes FastAPI and the Postgres database connector. Enables fidesctl webserver . NOTE: When installing database adapters there may be other dependencies, such as the pg_hba.conf file that usually requires a Postgres installation or the Microsoft ODBC Driver for SQL Server Apple M1 users of MSSQL: Known issues around connecting to MSSQL exist today, please reference the following issue for potential solutions: https://github.com/mkleehammer/pyodbc/issues/846","title":"Installing Optional Dependencies"},{"location":"language/overview/","text":"Fides Language Documentation This is the documentation for Fides' configuration language. It is relevant to users of Fides Control ( fidesctl ), Fides Ops ( fidesops , and other privacy tools that are in the roadmap. Hands-on : Try the fidesctl: Getting Started . The Fides language is Fides' primary user interface. In every use of Fides, configuration files written in the Fides language is always at the heart of the workflow. About the Fides Language The Fides language is based on YAML configuration files. YAML provides a well-understood structure, upon which the Fides language adds helpful primitives which represent types of data, processes or policies. By declaring these primitives with Fides you can describe: what types of data your application process (using Fides data_category annotations) how your system uses that data (using Fides data_use annotations) what policies you want your system to adhere to (using Fides Policy resources) etc. All other language features exist only to make the definition of privacy primitives more flexible and convenient. When fully utilized, these configuration files written using the Fides language tell other Fides tools what your software is doing with data and how to manage the privacy risks of that data process. Software systems are complicated though, so a full Fides configuration will consist of multiple files describing different resources, including: Dataset YAML A Dataset declaration in Fides language represents any location where data is stored: databases, data warehouses, caches and other data storage systems. Within a Fides Dataset, you declare the individual fields (e.g. database columns) where data is located and annotate them to describe the categories of data that are stored. System YAML A System declaration in Fides language represents the privacy properties of a single software project, service, codebase, or application. So the Fides System declaration describes both the categories of data being processed, but also the purposes for which that data is processed. Policy YAML A Policy declaration in Fides language represents a set of rules for privacy or compliance that the system must adhere to. The fidesctl tool evaluates these policies against the system & dataset declarations to ensure automated compliance.","title":"Overview"},{"location":"language/overview/#fides-language-documentation","text":"This is the documentation for Fides' configuration language. It is relevant to users of Fides Control ( fidesctl ), Fides Ops ( fidesops , and other privacy tools that are in the roadmap. Hands-on : Try the fidesctl: Getting Started . The Fides language is Fides' primary user interface. In every use of Fides, configuration files written in the Fides language is always at the heart of the workflow.","title":"Fides Language Documentation"},{"location":"language/overview/#about-the-fides-language","text":"The Fides language is based on YAML configuration files. YAML provides a well-understood structure, upon which the Fides language adds helpful primitives which represent types of data, processes or policies. By declaring these primitives with Fides you can describe: what types of data your application process (using Fides data_category annotations) how your system uses that data (using Fides data_use annotations) what policies you want your system to adhere to (using Fides Policy resources) etc. All other language features exist only to make the definition of privacy primitives more flexible and convenient. When fully utilized, these configuration files written using the Fides language tell other Fides tools what your software is doing with data and how to manage the privacy risks of that data process. Software systems are complicated though, so a full Fides configuration will consist of multiple files describing different resources, including:","title":"About the Fides Language"},{"location":"language/overview/#dataset-yaml","text":"A Dataset declaration in Fides language represents any location where data is stored: databases, data warehouses, caches and other data storage systems. Within a Fides Dataset, you declare the individual fields (e.g. database columns) where data is located and annotate them to describe the categories of data that are stored.","title":"Dataset YAML"},{"location":"language/overview/#system-yaml","text":"A System declaration in Fides language represents the privacy properties of a single software project, service, codebase, or application. So the Fides System declaration describes both the categories of data being processed, but also the purposes for which that data is processed.","title":"System YAML"},{"location":"language/overview/#policy-yaml","text":"A Policy declaration in Fides language represents a set of rules for privacy or compliance that the system must adhere to. The fidesctl tool evaluates these policies against the system & dataset declarations to ensure automated compliance.","title":"Policy YAML"},{"location":"language/syntax/","text":"Fides Configuration Syntax Other pages in this language section describe various concepts and resources that appear in the Fides language. This page describes the syntax of the language in more detail to help better interpret Fides whether you're authoring or reading. The Fides language is an intentionally simple language designed to be relatively easy for anyone to read and write. The primary objective is to translate complex privacy compliance concepts into a simple syntax, it's for this reason Fides is entirely written as YAML configurations. YAML - Building Block of Fides Fides Taxonomy The Fides language is intentionally simple. To assure this, Fides declarations use predefined primitives (e.g. data categories) that are used when describing your datasets, systems, policies, etc. These predefined primitives exist as part of the Fides taxonomy which is maintained in your fidesctl server so they can be consistently used across your organization's development team. You can learn more about the taxonomy structure and how to extend it in the taxonomy guide . Dot Notation and Snake_Case To make writing and reading Fides language as easy for humans as possible, declarations from the privacy taxonomy use dot notation for the keys and use snake_case compound labels. For example, to describe a field in a database as information provided by a user that is personally identifiable, you can write it's data category as: 1 2 # This declares that the data is provided by the user and identifies them directly user.provided.identifiable If we require greater specificity that just \"identifiable\", we could declare the contact type as a phone number by using a more specific sub-category: 1 2 3 # This declares that the is data provided by the user, # identifies them directly and is from the contact category and of type phone number. user.provided.identifiable.contact.phone_number The diagram below shows you the structure of that statement: notation-conventions Key-Value The key-value is YAML, and Fides', basic building block. Every item in a Fides YAML document is a member of at least one dictionary. The key is always a string . The value is a scalar so that it can be any datatype. So the value can be a string , a number , or another dictionary - most commonly in Fides, this will be a string that may provide a description or a pointer to a reference object in the taxonomy. If we use the example of a user's contact email, to correctly declare this in valid Fides YAML as part of a Dataset, it would be: 1 2 3 4 5 6 fields : # Group of fields in the dataset. - name : email description : User's Email data_categories : # Data category label(s) to assign field. - user.provided.identifiable.contact.email - account.contact.email The key for each key-value pair determines what value types are valid (for example, a resource type such as data_categories must use values from the Data Categories taxonomy), but many keys accept arbitrary strings as descriptive labels. Finally, as you see in the example above, keys such as data_categories accept a list of values for multi-labeling. In this case, the field email has been assigned the value user provided identifiable contact email as well as account related contact email , indicating that it may be either of those categories when used. Character Encoding Fides configuration files must always be UTF-8 encoded. While the delimiters of the language are all ASCII characters, Fides accepts non-ASCII characters in key-values, comments, and string values.","title":"Syntax"},{"location":"language/syntax/#fides-configuration-syntax","text":"Other pages in this language section describe various concepts and resources that appear in the Fides language. This page describes the syntax of the language in more detail to help better interpret Fides whether you're authoring or reading. The Fides language is an intentionally simple language designed to be relatively easy for anyone to read and write. The primary objective is to translate complex privacy compliance concepts into a simple syntax, it's for this reason Fides is entirely written as YAML configurations.","title":"Fides Configuration Syntax"},{"location":"language/syntax/#yaml-building-block-of-fides","text":"","title":"YAML - Building Block of Fides"},{"location":"language/syntax/#fides-taxonomy","text":"The Fides language is intentionally simple. To assure this, Fides declarations use predefined primitives (e.g. data categories) that are used when describing your datasets, systems, policies, etc. These predefined primitives exist as part of the Fides taxonomy which is maintained in your fidesctl server so they can be consistently used across your organization's development team. You can learn more about the taxonomy structure and how to extend it in the taxonomy guide .","title":"Fides Taxonomy"},{"location":"language/syntax/#dot-notation-and-snake_case","text":"To make writing and reading Fides language as easy for humans as possible, declarations from the privacy taxonomy use dot notation for the keys and use snake_case compound labels. For example, to describe a field in a database as information provided by a user that is personally identifiable, you can write it's data category as: 1 2 # This declares that the data is provided by the user and identifies them directly user.provided.identifiable If we require greater specificity that just \"identifiable\", we could declare the contact type as a phone number by using a more specific sub-category: 1 2 3 # This declares that the is data provided by the user, # identifies them directly and is from the contact category and of type phone number. user.provided.identifiable.contact.phone_number The diagram below shows you the structure of that statement: notation-conventions","title":"Dot Notation and Snake_Case"},{"location":"language/syntax/#key-value","text":"The key-value is YAML, and Fides', basic building block. Every item in a Fides YAML document is a member of at least one dictionary. The key is always a string . The value is a scalar so that it can be any datatype. So the value can be a string , a number , or another dictionary - most commonly in Fides, this will be a string that may provide a description or a pointer to a reference object in the taxonomy. If we use the example of a user's contact email, to correctly declare this in valid Fides YAML as part of a Dataset, it would be: 1 2 3 4 5 6 fields : # Group of fields in the dataset. - name : email description : User's Email data_categories : # Data category label(s) to assign field. - user.provided.identifiable.contact.email - account.contact.email The key for each key-value pair determines what value types are valid (for example, a resource type such as data_categories must use values from the Data Categories taxonomy), but many keys accept arbitrary strings as descriptive labels. Finally, as you see in the example above, keys such as data_categories accept a list of values for multi-labeling. In this case, the field email has been assigned the value user provided identifiable contact email as well as account related contact email , indicating that it may be either of those categories when used.","title":"Key-Value"},{"location":"language/syntax/#character-encoding","text":"Fides configuration files must always be UTF-8 encoded. While the delimiters of the language are all ASCII characters, Fides accepts non-ASCII characters in key-values, comments, and string values.","title":"Character Encoding"},{"location":"language/resources/dataset/","text":"Dataset A Dataset takes a database schema (tables and columns) and adds Fides privacy categorizations. This is a database-agnostic way to annotate privacy declarations. 1 2 3 4 5 6 organization |-> registry (optional) |-> system |-> ** dataset ** |-> collections |-> fields The schema is represented as a set of \"collections\" (tables) that contain \"fields\" (columns). At each level -- Dataset, collection, and field, you can assign one or more Data Categories and Data Qualifiers. The Categories and Qualifiers declared at each child level is additive, for example, if you declare a collection with category user.derived , and a field with category user.provided.identifiable.name , your dataset will contain both user-derived and user-provided name data. While you can create Dataset objects by hand, you typically use the generate command to create rudimentary Dataset manifest files that are based on your real-world databases. After you run the command, which creates the schema components, you add your Data Categories and Data Qualifiers to the manifest. You use your Datasets by adding them to Systems. A System can contain any number of Datasets, and a Dataset can be added to any number of Systems. When a dataset is referenced by a system, all applicable data categories set on the dataset are treated as part of the system. Datasets cannot contain other Datasets. Object Structure fides_key constrained string A string token of your own invention that uniquely identifies this Dataset. It's your responsibility to ensure that the value is unique across all of your Dataset objects. The value may only contain alphanumeric characters, underscores, and hyphens. ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the Dataset. description string A human-readable description of the Dataset. organization_fides_key string default: default_organization The fides key of the Organization to which this Dataset belongs. meta object An optional object that provides additional information about the Dataset. You can structure the object however you like. It can be a simple set of key: value properties or a deeply nested hierarchy of objects. How you use the object is up to you: Fides ignores it. third_country_transfers constrained string An optional array to identify any third countries where data is transited to. For consistency purposes, these fields are required to follow the Alpha-3 code set in ISO 3166-1 joint_controller [array] An optional array of contact information if a Joint Controller exists. This information can also be stored at the system level ( name , address , email , phone ). retention string An optional string to describe the retention policy for a dataset. This field can also be applied more granularly at either the Collection or field level of a Dataset data_categories [ string ] data_qualifiers [ string ] Arrays of Data Category and Data Qualifier resources, identified by fides_key , that apply to all collections in the Dataset. collections [ object ] An array of objects that describe the Dataset's collections. collections.name string A UI-friendly label for the collection. collections.description string A human-readable description of the collection. collections.data_categories [ string ] collections.data_qualifiers [ string ] Arrays of Data Category and Data Qualifier resources, identified by fides_key , that apply to all fields in the collection. collections.retention string An optional string to describe the retention policy for a Dataset collection. This field can also be applied more granularly at the field level of a Dataset. collections.fields [ object ] An array of objects that describe the collection's fields. collections.fields.name string A UI-friendly label for the field. collections.fields.description string A human-readable description of the field. collections.fields.data_categories [ string ] Arrays of Data Categories, identified by fides_key , that applies to this field. collections.fields.data_qualifier string A Data Qualifier that applies to this field. Note that this field holds a single value, therefore, the property name is singular. collections.fields.retention string An optional string to describe the retention policy for a field within a Dataset collection. collections.fields.fields [ object ] An optional array of objects that describe hierarchical/nested fields (typically found in NoSQL databases) Examples Manifest File 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 dataset : - fides_key : demo_users_dataset name : Demo Users Dataset description : Data collected about users for our analytics system. third_country_transfers : - USA - CAN joint_controller : name : Dave L. Epper address : 1 Acme Pl. New York, NY email : controller@acmeinc.com phone : +1 555 555 5555 retention : 1 year post account deletion collections : - name : users description : User information data_categories : - user.derived retention : 30 days post account deletion fields : - name : first_name description : User's first name data_categories : - user.provided.identifiable.name - name : email description : User's Email data_categories : - user.provided.identifiable.contact.email - name : phone description : User's phone numbers data_categories : - user.provided.identifiable.contact.phone_number retention : end of user relationship fields : - name : mobile description : User's mobile phone number data_categories : - user.provided.identifiable.contact.phone_number - name : home description : User's home phone number data_categories : - user.provided.identifiable.contact.phone_number API Payload 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 { \"fides_key\" : \"demo_users_dataset\" , \"name\" : \"Demo Users Dataset\" , \"description\" : \"Data collected about users for our analytics system.\" , \"third_country_transfers\" : [ \"USA\" , \"CAN\" ], \"joint_controller\" : { \"name\" : \"Dave L. Epper\" , \"address\" : \"1 Acme Pl. New York, NY\" , \"email\" : \"controller@acmeinc.com\" , \"phone\" : \"+1 555 555 5555\" }, \"retention\" : \"1 year post account deletion\" , \"collections\" : [ { \"name\" : \"users\" , \"description\" : \"User information\" , \"retention\" : \"30 days post account deletion\" , \"fields\" : [ { \"name\" : \"first_name\" , \"description\" : \"User's first name\" , \"data_categories\" : [ \"user.provided.identifiable.name\" ] }, { \"name\" : \"email\" , \"description\" : \"User's Email\" , \"data_categories\" : [ \"user.provided.identifiable.contact.email\" ] }, { \"name\" : \"phone\" , \"description\" : \"User's phone numbers\" , \"data_categories\" : [ \"user.provided.identifiable.contact.phone_number\" ], \"retention\" : \"end of user relationship\" , \"fields\" : [ { \"name\" : \"mobile\" , \"description\" : \"User's mobile phone number\" , \"data_categories\" : [ \"user.provided.identifiable.contact.phone_number\" ], }, { \"name\" : \"home\" , \"description\" : \"User's home phone number\" , \"data_categories\" : [ \"user.provided.identifiable.contact.phone_number\" ] } ] } ] } ] }","title":"Dataset"},{"location":"language/resources/dataset/#dataset","text":"A Dataset takes a database schema (tables and columns) and adds Fides privacy categorizations. This is a database-agnostic way to annotate privacy declarations. 1 2 3 4 5 6 organization |-> registry (optional) |-> system |-> ** dataset ** |-> collections |-> fields The schema is represented as a set of \"collections\" (tables) that contain \"fields\" (columns). At each level -- Dataset, collection, and field, you can assign one or more Data Categories and Data Qualifiers. The Categories and Qualifiers declared at each child level is additive, for example, if you declare a collection with category user.derived , and a field with category user.provided.identifiable.name , your dataset will contain both user-derived and user-provided name data. While you can create Dataset objects by hand, you typically use the generate command to create rudimentary Dataset manifest files that are based on your real-world databases. After you run the command, which creates the schema components, you add your Data Categories and Data Qualifiers to the manifest. You use your Datasets by adding them to Systems. A System can contain any number of Datasets, and a Dataset can be added to any number of Systems. When a dataset is referenced by a system, all applicable data categories set on the dataset are treated as part of the system. Datasets cannot contain other Datasets.","title":"Dataset"},{"location":"language/resources/dataset/#object-structure","text":"fides_key constrained string A string token of your own invention that uniquely identifies this Dataset. It's your responsibility to ensure that the value is unique across all of your Dataset objects. The value may only contain alphanumeric characters, underscores, and hyphens. ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the Dataset. description string A human-readable description of the Dataset. organization_fides_key string default: default_organization The fides key of the Organization to which this Dataset belongs. meta object An optional object that provides additional information about the Dataset. You can structure the object however you like. It can be a simple set of key: value properties or a deeply nested hierarchy of objects. How you use the object is up to you: Fides ignores it. third_country_transfers constrained string An optional array to identify any third countries where data is transited to. For consistency purposes, these fields are required to follow the Alpha-3 code set in ISO 3166-1 joint_controller [array] An optional array of contact information if a Joint Controller exists. This information can also be stored at the system level ( name , address , email , phone ). retention string An optional string to describe the retention policy for a dataset. This field can also be applied more granularly at either the Collection or field level of a Dataset data_categories [ string ] data_qualifiers [ string ] Arrays of Data Category and Data Qualifier resources, identified by fides_key , that apply to all collections in the Dataset. collections [ object ] An array of objects that describe the Dataset's collections. collections.name string A UI-friendly label for the collection. collections.description string A human-readable description of the collection. collections.data_categories [ string ] collections.data_qualifiers [ string ] Arrays of Data Category and Data Qualifier resources, identified by fides_key , that apply to all fields in the collection. collections.retention string An optional string to describe the retention policy for a Dataset collection. This field can also be applied more granularly at the field level of a Dataset. collections.fields [ object ] An array of objects that describe the collection's fields. collections.fields.name string A UI-friendly label for the field. collections.fields.description string A human-readable description of the field. collections.fields.data_categories [ string ] Arrays of Data Categories, identified by fides_key , that applies to this field. collections.fields.data_qualifier string A Data Qualifier that applies to this field. Note that this field holds a single value, therefore, the property name is singular. collections.fields.retention string An optional string to describe the retention policy for a field within a Dataset collection. collections.fields.fields [ object ] An optional array of objects that describe hierarchical/nested fields (typically found in NoSQL databases)","title":"Object Structure"},{"location":"language/resources/dataset/#examples","text":"","title":"Examples"},{"location":"language/resources/dataset/#manifest-file","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 dataset : - fides_key : demo_users_dataset name : Demo Users Dataset description : Data collected about users for our analytics system. third_country_transfers : - USA - CAN joint_controller : name : Dave L. Epper address : 1 Acme Pl. New York, NY email : controller@acmeinc.com phone : +1 555 555 5555 retention : 1 year post account deletion collections : - name : users description : User information data_categories : - user.derived retention : 30 days post account deletion fields : - name : first_name description : User's first name data_categories : - user.provided.identifiable.name - name : email description : User's Email data_categories : - user.provided.identifiable.contact.email - name : phone description : User's phone numbers data_categories : - user.provided.identifiable.contact.phone_number retention : end of user relationship fields : - name : mobile description : User's mobile phone number data_categories : - user.provided.identifiable.contact.phone_number - name : home description : User's home phone number data_categories : - user.provided.identifiable.contact.phone_number","title":"Manifest File"},{"location":"language/resources/dataset/#api-payload","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 { \"fides_key\" : \"demo_users_dataset\" , \"name\" : \"Demo Users Dataset\" , \"description\" : \"Data collected about users for our analytics system.\" , \"third_country_transfers\" : [ \"USA\" , \"CAN\" ], \"joint_controller\" : { \"name\" : \"Dave L. Epper\" , \"address\" : \"1 Acme Pl. New York, NY\" , \"email\" : \"controller@acmeinc.com\" , \"phone\" : \"+1 555 555 5555\" }, \"retention\" : \"1 year post account deletion\" , \"collections\" : [ { \"name\" : \"users\" , \"description\" : \"User information\" , \"retention\" : \"30 days post account deletion\" , \"fields\" : [ { \"name\" : \"first_name\" , \"description\" : \"User's first name\" , \"data_categories\" : [ \"user.provided.identifiable.name\" ] }, { \"name\" : \"email\" , \"description\" : \"User's Email\" , \"data_categories\" : [ \"user.provided.identifiable.contact.email\" ] }, { \"name\" : \"phone\" , \"description\" : \"User's phone numbers\" , \"data_categories\" : [ \"user.provided.identifiable.contact.phone_number\" ], \"retention\" : \"end of user relationship\" , \"fields\" : [ { \"name\" : \"mobile\" , \"description\" : \"User's mobile phone number\" , \"data_categories\" : [ \"user.provided.identifiable.contact.phone_number\" ], }, { \"name\" : \"home\" , \"description\" : \"User's home phone number\" , \"data_categories\" : [ \"user.provided.identifiable.contact.phone_number\" ] } ] } ] } ] }","title":"API Payload"},{"location":"language/resources/organization/","text":"Organization An Organization represents all or part of an enterprise or company, and establishes the root of your resource hierarchy. This means that while you can have more than one Organization resource, they can't refer to each other's sub-resources. For example, your \"American Stores\" Organization can't refer to the Policy objects that are defined by your \"European Stores\" Organization. The Organization resource will also contain vital information with regards to compliance reporting in the case of a data map or RoPA (Record of Processing Activities). All other resource types must refer to an Organization (through their organization_fides_key properties). Fides creates a default Organization that it uses for all resources that don't otherwise specify an Organization. Unless you're creating multiple Organizations (which should be rare), it is suggested to use the default Organization resource. The fides key for the default Organization is default_organization . Object Structure fides_key string A string token of your own invention that uniquely identifies this Organization. It's your responsibility to ensure that the value is unique across all of your Organization objects. The value can only contain alphanumeric characters, hyphens, periods and underscores ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the Organization. description string A human-readable description of the Organization. controller [array] An array of contact information for the controller over personal data usage within the organization ( name , address , email , phone ). data_protection_officer [array] An array of contact information for the Data Protection Officer (DPO) within the organization ( name , address , email , phone ). representative [array] An array of contact information for an optional representative for the organization on behalf of the controller and/or DPO ( name , address , email , phone ). security_policy string A url to the organization security policy, (i.e. https://ethyca.com/privacy-policy/) Examples Manifest File 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 organization : fides_key : default_organization name : Acme Incorporated description : An Organization that represents all of Acme Inc. security_policy : https://example.org/privacy controller : name : Dave L. Epper address : 1 Acme Pl. New York, NY email : controller@acmeinc.com phone : +1 555 555 5555 data_protection_officer : name : Preet Ector address : 1 Acme Pl. New York, NY email : dpo@acmeinc.com phone : +1 555 555 5555 representative : name : Ann Othername address : 1 Acme Pl. New York, NY email : representative@acmeinc.com phone : +1 555 555 5555 API Payload 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"fides_key\" : \"default_organization\" , \"name\" : \"Acme Incorporated\" , \"description\" : \"An Organization that represents all of Acme Inc.\" , \"security_policy\" : \"https://example.org/privacy\" , \"controller\" : { \"name\" : \"Dave L. Epper\" , \"address\" : \"1 Acme Pl. New York, NY\" , \"email\" : \"controller@acmeinc.com\" , \"phone\" : \"+1 555 555 5555\" }, \"data_protection_officer\" : { \"name\" : \"Preet Ector\" , \"address\" : \"1 Acme Pl. New York, NY\" , \"email\" : \"dpo@acmeinc.com\" , \"phone\" : \"+1 555 555 5555\" }, \"representative\" : { \"name\" : \"Ann Othername\" , \"address\" : \"1 Acme Pl. New York, NY\" , \"email\" : \"representative@acmeinc.com\" , \"phone\" : \"+1 555 555 5555\" } }","title":"Organization"},{"location":"language/resources/organization/#organization","text":"An Organization represents all or part of an enterprise or company, and establishes the root of your resource hierarchy. This means that while you can have more than one Organization resource, they can't refer to each other's sub-resources. For example, your \"American Stores\" Organization can't refer to the Policy objects that are defined by your \"European Stores\" Organization. The Organization resource will also contain vital information with regards to compliance reporting in the case of a data map or RoPA (Record of Processing Activities). All other resource types must refer to an Organization (through their organization_fides_key properties). Fides creates a default Organization that it uses for all resources that don't otherwise specify an Organization. Unless you're creating multiple Organizations (which should be rare), it is suggested to use the default Organization resource. The fides key for the default Organization is default_organization .","title":"Organization"},{"location":"language/resources/organization/#object-structure","text":"fides_key string A string token of your own invention that uniquely identifies this Organization. It's your responsibility to ensure that the value is unique across all of your Organization objects. The value can only contain alphanumeric characters, hyphens, periods and underscores ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the Organization. description string A human-readable description of the Organization. controller [array] An array of contact information for the controller over personal data usage within the organization ( name , address , email , phone ). data_protection_officer [array] An array of contact information for the Data Protection Officer (DPO) within the organization ( name , address , email , phone ). representative [array] An array of contact information for an optional representative for the organization on behalf of the controller and/or DPO ( name , address , email , phone ). security_policy string A url to the organization security policy, (i.e. https://ethyca.com/privacy-policy/)","title":"Object Structure"},{"location":"language/resources/organization/#examples","text":"","title":"Examples"},{"location":"language/resources/organization/#manifest-file","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 organization : fides_key : default_organization name : Acme Incorporated description : An Organization that represents all of Acme Inc. security_policy : https://example.org/privacy controller : name : Dave L. Epper address : 1 Acme Pl. New York, NY email : controller@acmeinc.com phone : +1 555 555 5555 data_protection_officer : name : Preet Ector address : 1 Acme Pl. New York, NY email : dpo@acmeinc.com phone : +1 555 555 5555 representative : name : Ann Othername address : 1 Acme Pl. New York, NY email : representative@acmeinc.com phone : +1 555 555 5555","title":"Manifest File"},{"location":"language/resources/organization/#api-payload","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"fides_key\" : \"default_organization\" , \"name\" : \"Acme Incorporated\" , \"description\" : \"An Organization that represents all of Acme Inc.\" , \"security_policy\" : \"https://example.org/privacy\" , \"controller\" : { \"name\" : \"Dave L. Epper\" , \"address\" : \"1 Acme Pl. New York, NY\" , \"email\" : \"controller@acmeinc.com\" , \"phone\" : \"+1 555 555 5555\" }, \"data_protection_officer\" : { \"name\" : \"Preet Ector\" , \"address\" : \"1 Acme Pl. New York, NY\" , \"email\" : \"dpo@acmeinc.com\" , \"phone\" : \"+1 555 555 5555\" }, \"representative\" : { \"name\" : \"Ann Othername\" , \"address\" : \"1 Acme Pl. New York, NY\" , \"email\" : \"representative@acmeinc.com\" , \"phone\" : \"+1 555 555 5555\" } }","title":"API Payload"},{"location":"language/resources/policy/","text":"Policy A Policy is your privacy policy as code, it lists a set of acceptable and non-acceptable rules and uses all 4 privacy attributes ( data_category , data_use , data_subject , and data_qualifier ). The purpose of the policy is to state what types of data are allowed for certain usages. 1 2 3 organization |-> ** policy ** |-> rules Object Structure fides_key constrained string A string token of your own invention that uniquely identifies this Policy. It's your responsibility to ensure that the value is unique across all of your Policy objects. The value may only contain alphanumeric characters, underscores, and hyphens. ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the Policy. description string A human-readable description of the Policy. data_categories string The Data Categories privacy attribute describes types of sensitive data as defined in the taxonomy. data_uses string The Data Use privacy attribute describes the various categories of data processing and operations at your organization. data_subject string The Data Subjects privacy attribute describes the individual persons whose data your rule pertains to. data_qualifier string The Data Qualifier privacy attribute describes the acceptable or non-acceptable level of deidentification for this data. matches enum ANY ALL NONE OTHER The matches criteria describes how you would like this rule to be evaluated. These basic logic gates determine whether the array of privacy attributes will be fully included ( ALL ), not included at all ( NONE ), only included if at least 1 item in the array matches ( ANY ), or excluded with any additional attributes included ( OTHER ). organization_fides_key string default: default_organization The fides key of the Organization to which this Policy belongs. Examples Manifest File 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 policy : - fides_key : demo_privacy_policy name : Demo Privacy Policy description : The main privacy policy for the organization. rules : - fides_key : reject_direct_marketing name : Reject Direct Marketing description : Disallow collecting any user contact info to use for marketing. data_categories : matches : ANY values : - user.provided.identifiable.contact data_uses : matches : ANY values : - advertising data_subjects : matches : ANY values : - customer data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified Demo manifest file: /fides/fidesctl/demo_resources/demo_policy.yml API Payload 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 POST /policy { \"fides_key\" : \"demo_privacy_policy\" , \"organization_fides_key\" : \"default_organization\" , \"name\" : \"string\" , \"description\" : \"The main privacy policy for the organization.\" , \"rules\" : [ { \"fides_key\" : \"reject_direct_marketing\" , \"organization_fides_key\" : \"default_organization\" , \"name\" : \"Reject Direct Marketing\" , \"description\" : \"Disallow collecting any user contact info to use for marketing.\" , \"data_categories\" : { \"matches\" : \"ANY\" , \"values\" : [ \"user.provided.identifiable.contact\" ] }, \"data_uses\" : { \"matches\" : \"ANY\" , \"values\" : [ \"advertising\" ] }, \"data_subjects\" : { \"matches\" : \"ANY\" , \"values\" : [ \"customer\" ] }, \"data_qualifier\" : \"aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified\" } ] }","title":"Policy"},{"location":"language/resources/policy/#policy","text":"A Policy is your privacy policy as code, it lists a set of acceptable and non-acceptable rules and uses all 4 privacy attributes ( data_category , data_use , data_subject , and data_qualifier ). The purpose of the policy is to state what types of data are allowed for certain usages. 1 2 3 organization |-> ** policy ** |-> rules","title":"Policy"},{"location":"language/resources/policy/#object-structure","text":"fides_key constrained string A string token of your own invention that uniquely identifies this Policy. It's your responsibility to ensure that the value is unique across all of your Policy objects. The value may only contain alphanumeric characters, underscores, and hyphens. ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the Policy. description string A human-readable description of the Policy. data_categories string The Data Categories privacy attribute describes types of sensitive data as defined in the taxonomy. data_uses string The Data Use privacy attribute describes the various categories of data processing and operations at your organization. data_subject string The Data Subjects privacy attribute describes the individual persons whose data your rule pertains to. data_qualifier string The Data Qualifier privacy attribute describes the acceptable or non-acceptable level of deidentification for this data. matches enum ANY ALL NONE OTHER The matches criteria describes how you would like this rule to be evaluated. These basic logic gates determine whether the array of privacy attributes will be fully included ( ALL ), not included at all ( NONE ), only included if at least 1 item in the array matches ( ANY ), or excluded with any additional attributes included ( OTHER ). organization_fides_key string default: default_organization The fides key of the Organization to which this Policy belongs.","title":"Object Structure"},{"location":"language/resources/policy/#examples","text":"","title":"Examples"},{"location":"language/resources/policy/#manifest-file","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 policy : - fides_key : demo_privacy_policy name : Demo Privacy Policy description : The main privacy policy for the organization. rules : - fides_key : reject_direct_marketing name : Reject Direct Marketing description : Disallow collecting any user contact info to use for marketing. data_categories : matches : ANY values : - user.provided.identifiable.contact data_uses : matches : ANY values : - advertising data_subjects : matches : ANY values : - customer data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified Demo manifest file: /fides/fidesctl/demo_resources/demo_policy.yml","title":"Manifest File"},{"location":"language/resources/policy/#api-payload","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 POST /policy { \"fides_key\" : \"demo_privacy_policy\" , \"organization_fides_key\" : \"default_organization\" , \"name\" : \"string\" , \"description\" : \"The main privacy policy for the organization.\" , \"rules\" : [ { \"fides_key\" : \"reject_direct_marketing\" , \"organization_fides_key\" : \"default_organization\" , \"name\" : \"Reject Direct Marketing\" , \"description\" : \"Disallow collecting any user contact info to use for marketing.\" , \"data_categories\" : { \"matches\" : \"ANY\" , \"values\" : [ \"user.provided.identifiable.contact\" ] }, \"data_uses\" : { \"matches\" : \"ANY\" , \"values\" : [ \"advertising\" ] }, \"data_subjects\" : { \"matches\" : \"ANY\" , \"values\" : [ \"customer\" ] }, \"data_qualifier\" : \"aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified\" } ] }","title":"API Payload"},{"location":"language/resources/registry/","text":"Registry A Registry is a collection of System resources. You may add a System to a Registry by setting the System's registry_id field. 1 2 3 organization |-> ** registry ** (optional) |-> system A System may belong to only one Registry. All Registries are siblings: You cannot create a hierarchy of Registries. Collecting your systems into Registries is optional. Object Structure fides_key constrained string A string token of your own invention that uniquely identifies this Registry. It's your responsibility to ensure that the value is unique across all of your Registry objects. The value may only contain alphanumeric characters, underscores, and hyphens. ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the Registry. description string A human-readable description of the Registry. organization_fides_key string default: default_organization The fides key of the Organization to which this Registry belongs. Examples Manifest File 1 2 3 4 registry : - fides_key : user_systems_registry name : User Systems Registry description : A Registry for all of the user-related systems. API Payload 1 2 3 4 5 { \"fides_key\" : \"user_systems_registry\" , \"name\" : \"User Systems Registry\" , \"description\" : \"A Registry for all of the user-related systems.\" }","title":"Registry"},{"location":"language/resources/registry/#registry","text":"A Registry is a collection of System resources. You may add a System to a Registry by setting the System's registry_id field. 1 2 3 organization |-> ** registry ** (optional) |-> system A System may belong to only one Registry. All Registries are siblings: You cannot create a hierarchy of Registries. Collecting your systems into Registries is optional.","title":"Registry"},{"location":"language/resources/registry/#object-structure","text":"fides_key constrained string A string token of your own invention that uniquely identifies this Registry. It's your responsibility to ensure that the value is unique across all of your Registry objects. The value may only contain alphanumeric characters, underscores, and hyphens. ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the Registry. description string A human-readable description of the Registry. organization_fides_key string default: default_organization The fides key of the Organization to which this Registry belongs.","title":"Object Structure"},{"location":"language/resources/registry/#examples","text":"","title":"Examples"},{"location":"language/resources/registry/#manifest-file","text":"1 2 3 4 registry : - fides_key : user_systems_registry name : User Systems Registry description : A Registry for all of the user-related systems.","title":"Manifest File"},{"location":"language/resources/registry/#api-payload","text":"1 2 3 4 5 { \"fides_key\" : \"user_systems_registry\" , \"name\" : \"User Systems Registry\" , \"description\" : \"A Registry for all of the user-related systems.\" }","title":"API Payload"},{"location":"language/resources/system/","text":"System A System is a model for describing anything that processes data for your organization (applications, services, 3rd party APIs, etc.) and describes how these datasets are used for business functions of instances of your data resources. It contains all 4 privacy attributes ( data_category , data_use , data_subject , and data_qualifier ). 1 2 3 4 organization |-> registry (optional) |-> ** system ** |-> privacy declarations Object Structure fides_key constrained string A string token of your own invention that uniquely identifies this System. It's your responsibility to ensure that the value is unique across all of your System objects. The value may only contain alphanumeric characters, underscores, and hyphens. ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the System. description string A human-readable description of the System. system_type string A required value to describe the type of system being modeled, examples include: Service, Application, Third Party, etc. data_responsibility_title enum An attribute to describe the role of responsibility over the personal data, used when exporting to a data map. Defaults to Controller if not set explicitly. Controller Processor Sub-Processor administrating_department string An optional value to identify the owning department or group of the system within your organization third_country_transfers constrained string An optional array to identify any third countries where data is transited to. For consistency purposes, these fields are required to follow the Alpha-3 code set in ISO 3166-1 joint_controller [array] An optional array of contact information if a Joint Controller exists. This information can also be more granularly stored at the dataset level ( name , address , email , phone ). data_protection_impact_assessment [array] The array of properties that declare the requirement for and information surrounding a Data Protection Impact Assessment ( is_required , progress , link ). Information will be exported as part of the data map or Record of Processing Activites (RoPA) privacy_declarations [array] The array of declarations describing the types of data in your system. This is a list of the privcy attributes ( data_category , data_use , data_subject , and data_qualifier ) for each of your systems. If a dataset is referenced as part of the system, all applicable data categories set on the dataset are treated as part of the system. organization_fides_key string default: default_organization The fides key of the Organization to which this System belongs. Examples Manifest File 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 system : - fides_key : demo_analytics_system name : Demo Analytics System description : A system used for analyzing customer behaviour. system_type : Service data_responsibility_title : Controller administrating_department : Engineering third_country_transfers : - USA - CAN joint_controller : name : Dave L. Epper address : 1 Acme Pl. New York, NY email : controller@acmeinc.com phone : +1 555 555 5555 data_protection_impact_assessment : is_required : True progress : Complete link : https://example.org/analytics_system_data_protection_impact_assessment privacy_declarations : - name : Analyze customer behaviour for improvements. data_categories : - user.provided.identifiable.contact - user.derived.identifiable.device.cookie_id data_use : improve.system data_subjects : - customer data_qualifier : identified_data dataset_references : - demo_users_dataset Demo manifest file: /fides/fidesctl/demo_resources/demo_system.yml API 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 POST /sys te m { \"fides_key\" : \"demo_analytics_system\" , \"name\" : \"Demo Analytics System\" , \"description\" : \"A system used for analyzing customer behaviour.\" , \"system_type\" : \"Service\" , \"data_responsibility_title\" : \"Controller\" , \"administrating_department\" : \"Engineering\" , \"third_country_transfers\" : [ \"USA\" , \"CAN\" ], \"joint_controller\" : { \"name\" : \"Dave L. Epper\" , \"address\" : \"1 Acme Pl. New York, NY\" , \"email\" : \"controller@acmeinc.com\" , \"phone\" : \"+1 555 555 5555\" }, \"privacy_declarations\" : [ { \"name\" : \"Analyze customer behaviour for improvements.\" , \"data_categories\" : [ \"user.provided.identifiable.contact\" , \"user.derived.identifiable.device.cookie_id\" ], \"data_use\" : \"improve.system\" , \"data_subjects\" : [ \"customer\" ], \"data_qualifier\" : \"identified_data\" , \"dataset_references\" : [ \"demo_users_dataset\" ] } ] }","title":"System"},{"location":"language/resources/system/#system","text":"A System is a model for describing anything that processes data for your organization (applications, services, 3rd party APIs, etc.) and describes how these datasets are used for business functions of instances of your data resources. It contains all 4 privacy attributes ( data_category , data_use , data_subject , and data_qualifier ). 1 2 3 4 organization |-> registry (optional) |-> ** system ** |-> privacy declarations","title":"System"},{"location":"language/resources/system/#object-structure","text":"fides_key constrained string A string token of your own invention that uniquely identifies this System. It's your responsibility to ensure that the value is unique across all of your System objects. The value may only contain alphanumeric characters, underscores, and hyphens. ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the System. description string A human-readable description of the System. system_type string A required value to describe the type of system being modeled, examples include: Service, Application, Third Party, etc. data_responsibility_title enum An attribute to describe the role of responsibility over the personal data, used when exporting to a data map. Defaults to Controller if not set explicitly. Controller Processor Sub-Processor administrating_department string An optional value to identify the owning department or group of the system within your organization third_country_transfers constrained string An optional array to identify any third countries where data is transited to. For consistency purposes, these fields are required to follow the Alpha-3 code set in ISO 3166-1 joint_controller [array] An optional array of contact information if a Joint Controller exists. This information can also be more granularly stored at the dataset level ( name , address , email , phone ). data_protection_impact_assessment [array] The array of properties that declare the requirement for and information surrounding a Data Protection Impact Assessment ( is_required , progress , link ). Information will be exported as part of the data map or Record of Processing Activites (RoPA) privacy_declarations [array] The array of declarations describing the types of data in your system. This is a list of the privcy attributes ( data_category , data_use , data_subject , and data_qualifier ) for each of your systems. If a dataset is referenced as part of the system, all applicable data categories set on the dataset are treated as part of the system. organization_fides_key string default: default_organization The fides key of the Organization to which this System belongs.","title":"Object Structure"},{"location":"language/resources/system/#examples","text":"","title":"Examples"},{"location":"language/resources/system/#manifest-file","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 system : - fides_key : demo_analytics_system name : Demo Analytics System description : A system used for analyzing customer behaviour. system_type : Service data_responsibility_title : Controller administrating_department : Engineering third_country_transfers : - USA - CAN joint_controller : name : Dave L. Epper address : 1 Acme Pl. New York, NY email : controller@acmeinc.com phone : +1 555 555 5555 data_protection_impact_assessment : is_required : True progress : Complete link : https://example.org/analytics_system_data_protection_impact_assessment privacy_declarations : - name : Analyze customer behaviour for improvements. data_categories : - user.provided.identifiable.contact - user.derived.identifiable.device.cookie_id data_use : improve.system data_subjects : - customer data_qualifier : identified_data dataset_references : - demo_users_dataset Demo manifest file: /fides/fidesctl/demo_resources/demo_system.yml","title":"Manifest File"},{"location":"language/resources/system/#api","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 POST /sys te m { \"fides_key\" : \"demo_analytics_system\" , \"name\" : \"Demo Analytics System\" , \"description\" : \"A system used for analyzing customer behaviour.\" , \"system_type\" : \"Service\" , \"data_responsibility_title\" : \"Controller\" , \"administrating_department\" : \"Engineering\" , \"third_country_transfers\" : [ \"USA\" , \"CAN\" ], \"joint_controller\" : { \"name\" : \"Dave L. Epper\" , \"address\" : \"1 Acme Pl. New York, NY\" , \"email\" : \"controller@acmeinc.com\" , \"phone\" : \"+1 555 555 5555\" }, \"privacy_declarations\" : [ { \"name\" : \"Analyze customer behaviour for improvements.\" , \"data_categories\" : [ \"user.provided.identifiable.contact\" , \"user.derived.identifiable.device.cookie_id\" ], \"data_use\" : \"improve.system\" , \"data_subjects\" : [ \"customer\" ], \"data_qualifier\" : \"identified_data\" , \"dataset_references\" : [ \"demo_users_dataset\" ] } ] }","title":"API"},{"location":"language/taxonomy/data_categories/","text":"Data Categories Reference Data Categories are hierarchical labels used to describe the type of data processed by your software. Data Category objects form a hierarchy: A Data Category can contain any number of children, but a given Category may only have one parent. You assign a child Category to a parent by setting the child's parent_key property. For example, the user.provided.identifiable.job_title Category is used for personally-identifiable job title information that was provided by the user. These are most heavily used by the System and Dataset resources, where you can assign one or more data categories to each field. Object Structure fides_key constrained string A string token that uniquely identifies this Data Category. The value is a dot-separated concatenation of the fides_key values of the resource's ancestors plus a final element for this resource: grandparent.parent.this_data_category The final element ( this_data_category ) may only contain alphanumeric characters and underscores ( [A-Za-z0-9_.-] ). The dot character is reserved as a separator. name string A UI-friendly label for the Data Category. description string A human-readable description of the Data Category. parent_key string The fides key of the Data Category's parent. organization_fides_key string default: default_organization The fides key of the organization to which this Data Category belongs. Extensibility and Interopability Data Categories in Fides are designed to support common privacy regulations and standards out of the box, these include GDPR, CCPA, LGPD and ISO 19944. You can extend the taxonomy to support your system needs. If you do this, we recommend extending from the existing categories to ensure interopability inside and outside your organization. If you have suggestions for core categories that should ship with the taxonomy, please submit your requests here Top Level Data Categories There are three top-level categories: Label Parent Key Description account - Data related to an account on the system. system - Data unique to, and under control of the system. user - Data related to the user of the system. For each top level category there are multiple subcategories that provide richer context. Below is a reference for all subcategories of account , system and user to assist with describing all data across systems. Account Data Categories Account Contact Data Label Parent Key Description contact account Contact data related to a system account. city account.contact Account's city level address data. country account.contact Account's country level address data. email account.contact Account's email address. phone_number account.contact Account's phone number. postal_code account.contact Account's postal code. state account.contact Account's state level address data. street account.contact Account's street level address. Account Payment Data Label Parent Key Description payment account Payment data related to system account. financial_account_number account.payment Payment data related to system account. System Data Categories Label Parent Key Description authentication system Data used to manage access to the system. operations system Data used for system operations. User Data Categories The \"User\" data category has two important subcategories for derived and provided data. In turn, derived and provided both have subcategories for identifiable and nonidentifiable data, to make it clear what data is considered identifiable in your systems. User Derived Data Data derived from user provided data or as a result of user actions in the system. Label Parent Key Description identifiable user.derived Derived data that is linked to, or identifies a user. biometric_health user.derived.identifiable Encoded characteristic collected about a user. browsing_history user.derived.identifiable Content browsing history of a user. contact user.derived.identifiable Contact data collected about a user. demographic user.derived.identifiable Demographic data about a user. gender user.derived.identifiable Gender of an individual. location user.derived.identifiable Records of the location of a user. media_consumption user.derived.identifiable Media type consumption data of a user. non_specific_age user.derived.identifiable Age range data. observed user.derived.identifiable Data collected through observation of use of the system. organization user.derived.identifiable Derived data that is linked to, or identifies an organization. profiling user.derived.identifiable Preference and interest data about a user. race user.derived.identifiable Racial or ethnic origin data. religious_belief user.derived.identifiable Religion or religious belief. search_history user.derived.identifiable Records of search history and queries of a user. sexual_orientation user.derived.identifiable Personal sex life or sexual data. social user.derived.identifiable Social activity and interaction data. telemetry user.derived.identifiable User identifiable measurement data from system sensors and monitoring. unique_id user.derived.identifiable Unique identifier for a user assigned through system use. user_sensor user.derived.identifiable Measurement data derived about a user's environment through system use. workplace user.derived.identifiable Organization of employment. device user.derived.identifiable Data related to a user's device, configuration and setting. cookie_id user.derived.identifiable.device Cookie unique identification number. device_id user.derived.identifiable.device Device unique identification number. ip_address user.derived.identifiable.device Unique identifier related to device connection. nonidentifiable user.derived Non-user identifiable data derived related to a user as a result of user actions in the system. nonsensor user.derived.nonidentifiable Non-user identifiable measurement data derived from sensors and monitoring systems. User Provided Data Data provided or created directly by a user of the system. Label Parent Key Description identifiable user.provided Data provided or created directly by a user that is linked to or identifies a user. biometric user.provided.identifiable Encoded characteristics provided by a user. childrens user.provided.identifiable Data relating to children. health_and_medical user.provided.identifiable Health records or individual's personal medical information. job_title user.provided.identifiable Professional data. name user.provided.identifiable User's real name. non_specific_age user.provided.identifiable Age range data. political_opinion user.provided.identifiable Data related to the individual's political opinions. race user.provided.identifiable Racial or ethnic origin data. religious_belief user.provided.identifiable Religion or religious belief. sexual_orientation user.provided.identifiable Personal sex life or sexual data. workplace user.provided.identifiable Organization of employment. date_of_birth user.provided.identifiable User's date of birth. gender user.provided.identifiable Gender of an individual. genetic user.provided.identifiable Data about the genetic makeup provided by a user. contact user.provided.identifiable User provided contact data for purposes other than account management. city user.provided.identifiable.contact User's city level address data. country user.provided.identifiable.contact User's country level address data. email user.provided.identifiable.contact User's provided email address. phone_number user.provided.identifiable.contact User's phone number. postal_code user.provided.identifiable.contact User's postal code. state user.provided.identifiable.contact User's state level address data. street user.provided.identifiable.contact User's street level address data. credentials user.provided.identifiable User provided authentication data. biometric_credentials user.provided.identifiable.credentials Credentials for system authentication. password user.provided.identifiable.credentials Password for system authentication. financial user.provided.identifiable Payment data and financial history. account_number user.provided.identifiable.financial User's account number for a payment card, bank account, or other financial system. government_id user.provided.identifiable State provided identification data. drivers_license_number user.provided.identifiable.government_id State issued driving identification number. national_identification_number user.provided.identifiable.government_id State issued personal identification number. passport_number user.provided.identifiable.government_id State issued passport data. nonidentifiable user.provided Data provided or created directly by a user that is not identifiable.","title":"Data Categories"},{"location":"language/taxonomy/data_categories/#data-categories-reference","text":"Data Categories are hierarchical labels used to describe the type of data processed by your software. Data Category objects form a hierarchy: A Data Category can contain any number of children, but a given Category may only have one parent. You assign a child Category to a parent by setting the child's parent_key property. For example, the user.provided.identifiable.job_title Category is used for personally-identifiable job title information that was provided by the user. These are most heavily used by the System and Dataset resources, where you can assign one or more data categories to each field.","title":"Data Categories Reference"},{"location":"language/taxonomy/data_categories/#object-structure","text":"fides_key constrained string A string token that uniquely identifies this Data Category. The value is a dot-separated concatenation of the fides_key values of the resource's ancestors plus a final element for this resource: grandparent.parent.this_data_category The final element ( this_data_category ) may only contain alphanumeric characters and underscores ( [A-Za-z0-9_.-] ). The dot character is reserved as a separator. name string A UI-friendly label for the Data Category. description string A human-readable description of the Data Category. parent_key string The fides key of the Data Category's parent. organization_fides_key string default: default_organization The fides key of the organization to which this Data Category belongs. Extensibility and Interopability Data Categories in Fides are designed to support common privacy regulations and standards out of the box, these include GDPR, CCPA, LGPD and ISO 19944. You can extend the taxonomy to support your system needs. If you do this, we recommend extending from the existing categories to ensure interopability inside and outside your organization. If you have suggestions for core categories that should ship with the taxonomy, please submit your requests here","title":"Object Structure"},{"location":"language/taxonomy/data_categories/#top-level-data-categories","text":"There are three top-level categories: Label Parent Key Description account - Data related to an account on the system. system - Data unique to, and under control of the system. user - Data related to the user of the system. For each top level category there are multiple subcategories that provide richer context. Below is a reference for all subcategories of account , system and user to assist with describing all data across systems.","title":"Top Level Data Categories"},{"location":"language/taxonomy/data_categories/#account-data-categories","text":"","title":"Account Data Categories"},{"location":"language/taxonomy/data_categories/#account-contact-data","text":"Label Parent Key Description contact account Contact data related to a system account. city account.contact Account's city level address data. country account.contact Account's country level address data. email account.contact Account's email address. phone_number account.contact Account's phone number. postal_code account.contact Account's postal code. state account.contact Account's state level address data. street account.contact Account's street level address.","title":"Account Contact Data"},{"location":"language/taxonomy/data_categories/#account-payment-data","text":"Label Parent Key Description payment account Payment data related to system account. financial_account_number account.payment Payment data related to system account.","title":"Account Payment Data"},{"location":"language/taxonomy/data_categories/#system-data-categories","text":"Label Parent Key Description authentication system Data used to manage access to the system. operations system Data used for system operations.","title":"System Data Categories"},{"location":"language/taxonomy/data_categories/#user-data-categories","text":"The \"User\" data category has two important subcategories for derived and provided data. In turn, derived and provided both have subcategories for identifiable and nonidentifiable data, to make it clear what data is considered identifiable in your systems.","title":"User Data Categories"},{"location":"language/taxonomy/data_categories/#user-derived-data","text":"Data derived from user provided data or as a result of user actions in the system. Label Parent Key Description identifiable user.derived Derived data that is linked to, or identifies a user. biometric_health user.derived.identifiable Encoded characteristic collected about a user. browsing_history user.derived.identifiable Content browsing history of a user. contact user.derived.identifiable Contact data collected about a user. demographic user.derived.identifiable Demographic data about a user. gender user.derived.identifiable Gender of an individual. location user.derived.identifiable Records of the location of a user. media_consumption user.derived.identifiable Media type consumption data of a user. non_specific_age user.derived.identifiable Age range data. observed user.derived.identifiable Data collected through observation of use of the system. organization user.derived.identifiable Derived data that is linked to, or identifies an organization. profiling user.derived.identifiable Preference and interest data about a user. race user.derived.identifiable Racial or ethnic origin data. religious_belief user.derived.identifiable Religion or religious belief. search_history user.derived.identifiable Records of search history and queries of a user. sexual_orientation user.derived.identifiable Personal sex life or sexual data. social user.derived.identifiable Social activity and interaction data. telemetry user.derived.identifiable User identifiable measurement data from system sensors and monitoring. unique_id user.derived.identifiable Unique identifier for a user assigned through system use. user_sensor user.derived.identifiable Measurement data derived about a user's environment through system use. workplace user.derived.identifiable Organization of employment. device user.derived.identifiable Data related to a user's device, configuration and setting. cookie_id user.derived.identifiable.device Cookie unique identification number. device_id user.derived.identifiable.device Device unique identification number. ip_address user.derived.identifiable.device Unique identifier related to device connection. nonidentifiable user.derived Non-user identifiable data derived related to a user as a result of user actions in the system. nonsensor user.derived.nonidentifiable Non-user identifiable measurement data derived from sensors and monitoring systems.","title":"User Derived Data"},{"location":"language/taxonomy/data_categories/#user-provided-data","text":"Data provided or created directly by a user of the system. Label Parent Key Description identifiable user.provided Data provided or created directly by a user that is linked to or identifies a user. biometric user.provided.identifiable Encoded characteristics provided by a user. childrens user.provided.identifiable Data relating to children. health_and_medical user.provided.identifiable Health records or individual's personal medical information. job_title user.provided.identifiable Professional data. name user.provided.identifiable User's real name. non_specific_age user.provided.identifiable Age range data. political_opinion user.provided.identifiable Data related to the individual's political opinions. race user.provided.identifiable Racial or ethnic origin data. religious_belief user.provided.identifiable Religion or religious belief. sexual_orientation user.provided.identifiable Personal sex life or sexual data. workplace user.provided.identifiable Organization of employment. date_of_birth user.provided.identifiable User's date of birth. gender user.provided.identifiable Gender of an individual. genetic user.provided.identifiable Data about the genetic makeup provided by a user. contact user.provided.identifiable User provided contact data for purposes other than account management. city user.provided.identifiable.contact User's city level address data. country user.provided.identifiable.contact User's country level address data. email user.provided.identifiable.contact User's provided email address. phone_number user.provided.identifiable.contact User's phone number. postal_code user.provided.identifiable.contact User's postal code. state user.provided.identifiable.contact User's state level address data. street user.provided.identifiable.contact User's street level address data. credentials user.provided.identifiable User provided authentication data. biometric_credentials user.provided.identifiable.credentials Credentials for system authentication. password user.provided.identifiable.credentials Password for system authentication. financial user.provided.identifiable Payment data and financial history. account_number user.provided.identifiable.financial User's account number for a payment card, bank account, or other financial system. government_id user.provided.identifiable State provided identification data. drivers_license_number user.provided.identifiable.government_id State issued driving identification number. national_identification_number user.provided.identifiable.government_id State issued personal identification number. passport_number user.provided.identifiable.government_id State issued passport data. nonidentifiable user.provided Data provided or created directly by a user that is not identifiable.","title":"User Provided Data"},{"location":"language/taxonomy/data_qualifiers/","text":"Data Qualifiers Reference Data Qualifiers describe the degree of identification of the given data. Think of this as a spectrum: on one end is completely anonymous data, i.e. it is impossible to identify an individual from it, and on the other end is data that specifically identifies an individual. Extensibility and Interopability Data Qualifiers in Fides are designed to support common privacy regulations and standards out of the box, these include GDPR, CCPA, LGPD and ISO 19944. You can extend the taxonomy to support your system needs. If you do this, we recommend extending from the existing categories to ensure interopability inside and outside your organization. If you have suggestions for core categories that should ship with the taxonomy, please submit your requests here Data Qualifiers Data Qualifiers are arranged as a series of nested subcategories, going from least identifiable (aggregated) to most identifiable (identified). Label Parent Key Description aggregated - Statistical data that does not contain individually identifying information but includes information about groups of individuals that renders individual identification impossible. anonymized anonymized Data where all attributes have been sufficiently altered that the individaul cannot be reidentified by this data or in combination with other datasets. unlinked_pseudonymized aggregated.anonymized Data for which all identifiers have been substituted with unrelated values and linkages broken such that it may not be reversed, even by the party that performed the pseudonymization. pseudonymized aggregated.anonymized.unlinked_pseudonymized Data for which all identifiers have been substituted with unrelated values, rendering the individual unidentifiable and cannot be reasonably reversed other than by the party that performed the pseudonymization. identified aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified Data that directly identifies an individual.","title":"Data Qualifiers"},{"location":"language/taxonomy/data_qualifiers/#data-qualifiers-reference","text":"Data Qualifiers describe the degree of identification of the given data. Think of this as a spectrum: on one end is completely anonymous data, i.e. it is impossible to identify an individual from it, and on the other end is data that specifically identifies an individual. Extensibility and Interopability Data Qualifiers in Fides are designed to support common privacy regulations and standards out of the box, these include GDPR, CCPA, LGPD and ISO 19944. You can extend the taxonomy to support your system needs. If you do this, we recommend extending from the existing categories to ensure interopability inside and outside your organization. If you have suggestions for core categories that should ship with the taxonomy, please submit your requests here","title":"Data Qualifiers Reference"},{"location":"language/taxonomy/data_qualifiers/#data-qualifiers","text":"Data Qualifiers are arranged as a series of nested subcategories, going from least identifiable (aggregated) to most identifiable (identified). Label Parent Key Description aggregated - Statistical data that does not contain individually identifying information but includes information about groups of individuals that renders individual identification impossible. anonymized anonymized Data where all attributes have been sufficiently altered that the individaul cannot be reidentified by this data or in combination with other datasets. unlinked_pseudonymized aggregated.anonymized Data for which all identifiers have been substituted with unrelated values and linkages broken such that it may not be reversed, even by the party that performed the pseudonymization. pseudonymized aggregated.anonymized.unlinked_pseudonymized Data for which all identifiers have been substituted with unrelated values, rendering the individual unidentifiable and cannot be reasonably reversed other than by the party that performed the pseudonymization. identified aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified Data that directly identifies an individual.","title":"Data Qualifiers"},{"location":"language/taxonomy/data_subjects/","text":"Data Subjects Reference A Data Subject is a label that describes a segment of individuals whose data you store. Data Subject labels are typically fairly broad -- \"Citizen\", \"Visitor\", \"Passenger\", and so on -- although you be as specific as your system needs: \"Fans in Section K\", for example. Object Structure fides_key constrained string A string token of your own invention that uniquely identifies this Data Subject. It's your responsibility to ensure that the value is unique across all of your Data Subject objects. The value can only contain alphanumeric characters, hyphens, periods and underscores ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the Data Subject. description string A human-readable description of the Data Subject. rights enum An array of rights available to the data subject, made of available values coupled with Chapter 3 of the GDPR. The output of a data map is based upon the strategy for applying rights ( rights.strategy ) and the selections made from the following valid options: Informed Access Rectification Erasure Portability Restrict Processing Withdraw Consent Object Object to Automated Processing strategy enum A strategy for selecting the rights available to the data subject. ALL EXCLUDE INCLUDE NONE automated_decisions_or_profiling boolean A boolean value of whether or not automated decision-making or profiling exists. Tied to article 22 of the GDPR. organization_fides_key string default: default_organization The fides key of the organization to which this Data Subject belongs. Extensibility and Interopability Data Subjects in Fides are designed to support common privacy regulations and standards out of the box, these include GDPR, CCPA, LGPD and ISO 19944. You can extend the taxonomy to support your organization's needs. If you do this, we recommend extending from the existing categories to ensure interopability inside and outside your organization. If you have suggestions for core categories that should ship with the taxonomy, please submit your requests here Default Data Subject Types Currently, your collection of Data Subjects is given as a flat list: A Data Subject can't contain other Data Subjects. Label Parent Key Description anonymous_user - An individual who is unidentifiable to the systems. Note - This should only be applied to truly anonymous users where there is no risk of re-identification citizen_voter - An individual registered to voter with a state or authority. commuter - An individual who is traveling or transiting in the context of location tracking. consultant - An individual employed in a consultative/temporary capacity by the organization. customer - An individual or other organization that purchases goods or services from the organization. employee - An individual employed by the organization. job_applicant - An individual applying for employment to the organization. next_of_kin - A relative of any other individual subject where such a relationship is known. passenger - An individual traveling on some means of provided transport. patient - An individual identified for the purposes of any medical care. prospect - An individual or organization to whom an organization is selling goods or services. shareholder - An individual or organization that holds equity in the organization. supplier_vendor - An individual or organization that provides services or goods to the organization. trainee - An individual undergoing training by the organization. visitor - An individual visiting a location.","title":"Data Subjects"},{"location":"language/taxonomy/data_subjects/#data-subjects-reference","text":"A Data Subject is a label that describes a segment of individuals whose data you store. Data Subject labels are typically fairly broad -- \"Citizen\", \"Visitor\", \"Passenger\", and so on -- although you be as specific as your system needs: \"Fans in Section K\", for example.","title":"Data Subjects Reference"},{"location":"language/taxonomy/data_subjects/#object-structure","text":"fides_key constrained string A string token of your own invention that uniquely identifies this Data Subject. It's your responsibility to ensure that the value is unique across all of your Data Subject objects. The value can only contain alphanumeric characters, hyphens, periods and underscores ( [A-Za-z0-9_.-] ). name string A UI-friendly label for the Data Subject. description string A human-readable description of the Data Subject. rights enum An array of rights available to the data subject, made of available values coupled with Chapter 3 of the GDPR. The output of a data map is based upon the strategy for applying rights ( rights.strategy ) and the selections made from the following valid options: Informed Access Rectification Erasure Portability Restrict Processing Withdraw Consent Object Object to Automated Processing strategy enum A strategy for selecting the rights available to the data subject. ALL EXCLUDE INCLUDE NONE automated_decisions_or_profiling boolean A boolean value of whether or not automated decision-making or profiling exists. Tied to article 22 of the GDPR. organization_fides_key string default: default_organization The fides key of the organization to which this Data Subject belongs. Extensibility and Interopability Data Subjects in Fides are designed to support common privacy regulations and standards out of the box, these include GDPR, CCPA, LGPD and ISO 19944. You can extend the taxonomy to support your organization's needs. If you do this, we recommend extending from the existing categories to ensure interopability inside and outside your organization. If you have suggestions for core categories that should ship with the taxonomy, please submit your requests here","title":"Object Structure"},{"location":"language/taxonomy/data_subjects/#default-data-subject-types","text":"Currently, your collection of Data Subjects is given as a flat list: A Data Subject can't contain other Data Subjects. Label Parent Key Description anonymous_user - An individual who is unidentifiable to the systems. Note - This should only be applied to truly anonymous users where there is no risk of re-identification citizen_voter - An individual registered to voter with a state or authority. commuter - An individual who is traveling or transiting in the context of location tracking. consultant - An individual employed in a consultative/temporary capacity by the organization. customer - An individual or other organization that purchases goods or services from the organization. employee - An individual employed by the organization. job_applicant - An individual applying for employment to the organization. next_of_kin - A relative of any other individual subject where such a relationship is known. passenger - An individual traveling on some means of provided transport. patient - An individual identified for the purposes of any medical care. prospect - An individual or organization to whom an organization is selling goods or services. shareholder - An individual or organization that holds equity in the organization. supplier_vendor - An individual or organization that provides services or goods to the organization. trainee - An individual undergoing training by the organization. visitor - An individual visiting a location.","title":"Default Data Subject Types"},{"location":"language/taxonomy/data_uses/","text":"Data Uses Reference A Data Use is a label that denotes the way data is used in your system: \"Advertising, Marketing or Promotion\", \"First Party Advertising\", and \"Sharing for Legal Obligation\", as examples. Data Use objects form a hierarchy: A Data Use can contain any number of children, but a given Data Use may only have one parent. You assign a child Data Use to a parent by setting the child's parent_key property. For example, the third_party_sharing.personalized_advertising Data Use type is data used for personalized advertising when shared with third parties. Object Structure fides_key constrained string A string token that uniquely identifies this Data Use. The value is a dot-separated concatenation of the fides_key values of the resource's ancestors plus a final element for this resource: grandparent.parent.this_data_use The final element ( this_data_use ) may only contain alphanumeric characters and underscores ( [A-Za-z0-9_.-] ). The dot character is reserved as a separator. name string A UI-friendly label for the Data Use. description string A human-readable description of the Data Use. parent_key string The fides key of the the Data Use's parent. legal_basis enum The legal basis category of which the data use falls under. This field is used as part of the creation of an exportable data map. Current valid options: Consent Contract Legal Obligation Vital Interest Public Interest Legitimate Interest special_category enum The special category for processing of which the data use falls under. This field is used as part of the creation of an exportable data map. Current valid options: Consent Employment Vital Interests Non-profit Bodies Public by Data Subject Legal Claims Substantial Public Interest Medical Public Health Interest recipent string An array of recipients is applied here when sharing personal data outside of your organization (e.g. Internal Revenue Service, HMRC, etc.) legitimate_interest boolean default: False A boolean value representing whether the legal basis is a Legitimate Interest . This is validated at run time and looks for a legitimate_interest_impact_assessment to exist if true. legitimate_interest_impact_assessment url A url to the legitimate interest impact assessment. Can be any valid url (e.g. http, file, etc.) organization_fides_key string default: default_organization The fides key of the organization to which this Data Use belongs. Extensibility and Interoperability Data Uses in Fides are designed to support common privacy regulations and standards out of the box, these include GDPR, CCPA, LGPD and ISO 19944. You can extend the taxonomy to support your organization's needs. If you do this, we recommend extending from the existing categories to ensure interoperability inside and outside your organization. If you have suggestions for core categories that should ship with the taxonomy, please submit your requests here . Top Level Data Uses There are seven top-level Data Use classes: Label Parent Key Description provide - Provide, give, or make available the product, service, application or system. improve - Improve the product, service, application or system. personalize - Personalize the product, service, application or system. advertising - The promotion of products or services targeted to users based on the the processing of user provided data in the system. third_party_sharing - The transfer of specified data categories to third parties outside of the system/application's scope. collect - Collecting and storing data in order to use it for another purpose such as data training for ML. train_ai_system - Training an AI system. Please note when this data use is specified, the method and degree to which a user may be directly identified in the resulting AI system should be appended. For each top level classification there are multiple subclasses that provide richer context. Below is a reference for all subclasses of account , system and user to assist with describing all data across systems. Provide Data Uses Label Parent Key Description system provide The source system, product, service or application being provided to the user. provide.system.operations provide.system Use of specified data categories to operate and protect the system in order to provide the service. provide.system.operations.support provide.system.operations Use of specified data categories to provide support for operation and protection of the system in order to provide the service. provide.system.operations.support.optimization provide.system.operations.support Use of specified data categories to optimize and improve support operations in order to provide the service. provide.system.upgrades provide.system Offer upgrades or upsales such as increased capacity for the service based on monitoring of service usage. Improve Data Uses Label Parent Key Description system improve The source system, product, service or application being improved. Personalize Data Uses Label Parent Key Description system personalize The source system, product, service or application being personalized. Advertising Data Uses Label Parent Key Description first_party advertising The promotion of products or services targeting users based on processing of derviced data from prior use of the system. contextual advertising.first_party The promotion of products or services targeted to users based on the processing of derived data from the users prior use of the services. personalized advertising.first_party The targeting and changing of promotional content based on processing of specific data categories from the user. third_party advertising The promotion of products or services targeting users based on processing of specific categories of data acquired from third party sources. personalized advertising.third_party The targeting and changing of promotional content based on processing of specific categories of user data acquired from third party sources. Third Party Sharing Data Uses Label Parent Key Description payment_processing third_party_sharing Sharing of specified data categories with a third party for payment processing. personalized_advertising third_party_sharing Sharing of specified data categories for the purpose of marketing/advertising/promotion. fraud_detection third_party_sharing Sharing of specified data categories with a third party fo fraud prevention/detection. legal_obligation third_party_sharing Sharing of data for legal obligations, including contracts, applicable laws or regulations. Collection & AI Training Data Uses In the case of collection and train_ai_system , you will see these have no subclasses at present however define very specific data use cases that should be captured in data processes if they occur. Label Parent Key Description collect - Collecting and storing data in order to use it for another purpose such as data training for ML. train_ai_system - Training an AI system. Please note when this data use is specified, the method and degree to which a user may be directly identified in the resulting AI system should be appended.","title":"Data Uses"},{"location":"language/taxonomy/data_uses/#data-uses-reference","text":"A Data Use is a label that denotes the way data is used in your system: \"Advertising, Marketing or Promotion\", \"First Party Advertising\", and \"Sharing for Legal Obligation\", as examples. Data Use objects form a hierarchy: A Data Use can contain any number of children, but a given Data Use may only have one parent. You assign a child Data Use to a parent by setting the child's parent_key property. For example, the third_party_sharing.personalized_advertising Data Use type is data used for personalized advertising when shared with third parties.","title":"Data Uses Reference"},{"location":"language/taxonomy/data_uses/#object-structure","text":"fides_key constrained string A string token that uniquely identifies this Data Use. The value is a dot-separated concatenation of the fides_key values of the resource's ancestors plus a final element for this resource: grandparent.parent.this_data_use The final element ( this_data_use ) may only contain alphanumeric characters and underscores ( [A-Za-z0-9_.-] ). The dot character is reserved as a separator. name string A UI-friendly label for the Data Use. description string A human-readable description of the Data Use. parent_key string The fides key of the the Data Use's parent. legal_basis enum The legal basis category of which the data use falls under. This field is used as part of the creation of an exportable data map. Current valid options: Consent Contract Legal Obligation Vital Interest Public Interest Legitimate Interest special_category enum The special category for processing of which the data use falls under. This field is used as part of the creation of an exportable data map. Current valid options: Consent Employment Vital Interests Non-profit Bodies Public by Data Subject Legal Claims Substantial Public Interest Medical Public Health Interest recipent string An array of recipients is applied here when sharing personal data outside of your organization (e.g. Internal Revenue Service, HMRC, etc.) legitimate_interest boolean default: False A boolean value representing whether the legal basis is a Legitimate Interest . This is validated at run time and looks for a legitimate_interest_impact_assessment to exist if true. legitimate_interest_impact_assessment url A url to the legitimate interest impact assessment. Can be any valid url (e.g. http, file, etc.) organization_fides_key string default: default_organization The fides key of the organization to which this Data Use belongs. Extensibility and Interoperability Data Uses in Fides are designed to support common privacy regulations and standards out of the box, these include GDPR, CCPA, LGPD and ISO 19944. You can extend the taxonomy to support your organization's needs. If you do this, we recommend extending from the existing categories to ensure interoperability inside and outside your organization. If you have suggestions for core categories that should ship with the taxonomy, please submit your requests here .","title":"Object Structure"},{"location":"language/taxonomy/data_uses/#top-level-data-uses","text":"There are seven top-level Data Use classes: Label Parent Key Description provide - Provide, give, or make available the product, service, application or system. improve - Improve the product, service, application or system. personalize - Personalize the product, service, application or system. advertising - The promotion of products or services targeted to users based on the the processing of user provided data in the system. third_party_sharing - The transfer of specified data categories to third parties outside of the system/application's scope. collect - Collecting and storing data in order to use it for another purpose such as data training for ML. train_ai_system - Training an AI system. Please note when this data use is specified, the method and degree to which a user may be directly identified in the resulting AI system should be appended. For each top level classification there are multiple subclasses that provide richer context. Below is a reference for all subclasses of account , system and user to assist with describing all data across systems.","title":"Top Level Data Uses"},{"location":"language/taxonomy/data_uses/#provide-data-uses","text":"Label Parent Key Description system provide The source system, product, service or application being provided to the user. provide.system.operations provide.system Use of specified data categories to operate and protect the system in order to provide the service. provide.system.operations.support provide.system.operations Use of specified data categories to provide support for operation and protection of the system in order to provide the service. provide.system.operations.support.optimization provide.system.operations.support Use of specified data categories to optimize and improve support operations in order to provide the service. provide.system.upgrades provide.system Offer upgrades or upsales such as increased capacity for the service based on monitoring of service usage.","title":"Provide Data Uses"},{"location":"language/taxonomy/data_uses/#improve-data-uses","text":"Label Parent Key Description system improve The source system, product, service or application being improved.","title":"Improve Data Uses"},{"location":"language/taxonomy/data_uses/#personalize-data-uses","text":"Label Parent Key Description system personalize The source system, product, service or application being personalized.","title":"Personalize Data Uses"},{"location":"language/taxonomy/data_uses/#advertising-data-uses","text":"Label Parent Key Description first_party advertising The promotion of products or services targeting users based on processing of derviced data from prior use of the system. contextual advertising.first_party The promotion of products or services targeted to users based on the processing of derived data from the users prior use of the services. personalized advertising.first_party The targeting and changing of promotional content based on processing of specific data categories from the user. third_party advertising The promotion of products or services targeting users based on processing of specific categories of data acquired from third party sources. personalized advertising.third_party The targeting and changing of promotional content based on processing of specific categories of user data acquired from third party sources.","title":"Advertising Data Uses"},{"location":"language/taxonomy/data_uses/#third-party-sharing-data-uses","text":"Label Parent Key Description payment_processing third_party_sharing Sharing of specified data categories with a third party for payment processing. personalized_advertising third_party_sharing Sharing of specified data categories for the purpose of marketing/advertising/promotion. fraud_detection third_party_sharing Sharing of specified data categories with a third party fo fraud prevention/detection. legal_obligation third_party_sharing Sharing of data for legal obligations, including contracts, applicable laws or regulations.","title":"Third Party Sharing Data Uses"},{"location":"language/taxonomy/data_uses/#collection-ai-training-data-uses","text":"In the case of collection and train_ai_system , you will see these have no subclasses at present however define very specific data use cases that should be captured in data processes if they occur. Label Parent Key Description collect - Collecting and storing data in order to use it for another purpose such as data training for ML. train_ai_system - Training an AI system. Please note when this data use is specified, the method and degree to which a user may be directly identified in the resulting AI system should be appended.","title":"Collection &amp; AI Training Data Uses"},{"location":"language/taxonomy/explorer/","text":"Fides Taxonomy Explorer The taxonomy explorer is a useful way to visualize and review the taxonomy for those looking to explore in greater depth. Data Categories Data Uses Data Subjects Data Qualifiers","title":"Taxonomy Explorer"},{"location":"language/taxonomy/explorer/#fides-taxonomy-explorer","text":"The taxonomy explorer is a useful way to visualize and review the taxonomy for those looking to explore in greater depth. Data Categories Data Uses Data Subjects Data Qualifiers","title":"Fides Taxonomy Explorer"},{"location":"language/taxonomy/overview/","text":"Fides Taxonomy The Fides taxonomy contains four classification groups that are used together to easily describe all of the data types and associated processing behaviors of an entire tech stack; both the application and it's data storage. Summary of Taxonomy Classification Groups 1. Data Categories Data Categories are labels to describe the type of data processed by your software. These are most heavily used by the System and Dataset resources, where you can assign one or more data categories to each field. Data Categories are hierarchical with natural inheritance, meaning you can classify data coarsely with a high-level category (e.g. user.provided data), or you can classify it with greater precision using subcategories (e.g. user.provided.identifiable.contact.email data). Learn more about Data Categories in the taxonomy reference now . 2. Data Uses Data Uses are labels that describe how, or for what purpose(s) a component of your system is using data. Data Uses are also hierarchical with natural inheritance, meaning you can easily describe what you're using data for either coarsely (e.g. provide.system.operations ) or with more precision using subcategories (e.g. provide.system.operations.support.optimization ). Learn more about Data Uses in the taxonomy reference now . 3. Data Subjects Data Subject is a label commonly used in the regulatory world to describe the users of a system who's data is being processed. In many systems a generic user label may be sufficient, however Fides language is intended to provide greater control through specificity where needed. Examples of this are: anonymous_user employee customer patient next_of_kin Learn more about Data Subjects in the taxonomy reference now . 4. Data Qualifiers Data Qualifiers describe the degree of identification of the given data. Think of this as a spectrum: on one end is completely anonymous data, i.e. it is impossible to identify an individual from it, and on the other end is data that specifically identifies an individual. Along this spectrum are labels that describe the degree of identification that a given data might provide, such as: identified anonymized aggregated Learn more about Data Qualifiers in the taxonomy reference now . Extensibility & Interopability The Fides language is designed to support common privacy compliance regulations and standards out of the box, these include GDPR, CCPA, LGPD and ISO 19944. You can extend the taxonomy to support your organization's needs. If you do this, we recommend extending from the existing categories to ensure interopability inside and outside your organization.","title":"Overview"},{"location":"language/taxonomy/overview/#fides-taxonomy","text":"The Fides taxonomy contains four classification groups that are used together to easily describe all of the data types and associated processing behaviors of an entire tech stack; both the application and it's data storage.","title":"Fides Taxonomy"},{"location":"language/taxonomy/overview/#summary-of-taxonomy-classification-groups","text":"","title":"Summary of Taxonomy Classification Groups"},{"location":"language/taxonomy/overview/#1-data-categories","text":"Data Categories are labels to describe the type of data processed by your software. These are most heavily used by the System and Dataset resources, where you can assign one or more data categories to each field. Data Categories are hierarchical with natural inheritance, meaning you can classify data coarsely with a high-level category (e.g. user.provided data), or you can classify it with greater precision using subcategories (e.g. user.provided.identifiable.contact.email data). Learn more about Data Categories in the taxonomy reference now .","title":"1. Data Categories"},{"location":"language/taxonomy/overview/#2-data-uses","text":"Data Uses are labels that describe how, or for what purpose(s) a component of your system is using data. Data Uses are also hierarchical with natural inheritance, meaning you can easily describe what you're using data for either coarsely (e.g. provide.system.operations ) or with more precision using subcategories (e.g. provide.system.operations.support.optimization ). Learn more about Data Uses in the taxonomy reference now .","title":"2. Data Uses"},{"location":"language/taxonomy/overview/#3-data-subjects","text":"Data Subject is a label commonly used in the regulatory world to describe the users of a system who's data is being processed. In many systems a generic user label may be sufficient, however Fides language is intended to provide greater control through specificity where needed. Examples of this are: anonymous_user employee customer patient next_of_kin Learn more about Data Subjects in the taxonomy reference now .","title":"3. Data Subjects"},{"location":"language/taxonomy/overview/#4-data-qualifiers","text":"Data Qualifiers describe the degree of identification of the given data. Think of this as a spectrum: on one end is completely anonymous data, i.e. it is impossible to identify an individual from it, and on the other end is data that specifically identifies an individual. Along this spectrum are labels that describe the degree of identification that a given data might provide, such as: identified anonymized aggregated Learn more about Data Qualifiers in the taxonomy reference now .","title":"4. Data Qualifiers"},{"location":"language/taxonomy/overview/#extensibility-interopability","text":"The Fides language is designed to support common privacy compliance regulations and standards out of the box, these include GDPR, CCPA, LGPD and ISO 19944. You can extend the taxonomy to support your organization's needs. If you do this, we recommend extending from the existing categories to ensure interopability inside and outside your organization.","title":"Extensibility &amp; Interopability"},{"location":"quickstart/docker/","text":"Running Fidesctl in Docker The recommended way to get Fidesctl running is via Docker. The following guide will describe how to get things going, step-by-step. System Requirements Docker and Docker-Compose are the only requirements here. Install docker locally (see Docker Desktop or your preferred installation). The minimum verified Docker version is 20.10.8 If your docker installation did not include docker-compose , make sure to get at least version 1.29.0 . Installation instructions can be found here . Create a .fides folder in the root directory of your project (or in the same directory as your docker compose file). Docker Setup This is a reference file that you can copy/paste into a local docker-compose.yml file, which should be created in your project's root folder. It will create a database and spin up the fidesctl webserver. Make sure that you don't have anything else running on port 5432 or 8080 before using this file. docker-compose.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 services: fidesctl: image: ethyca/fidesctl command: fidesctl webserver healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://0.0.0.0:8000/health\"] interval: 5s timeout: 5s retries: 5 depends_on: fidesctl-db: condition: service_healthy expose: - 8080 ports: - \"8080:8080\" environment: FIDESCTL_TEST_MODE: \"True\" FIDESCTL__CLI__SERVER_HOST: \"fidesctl\" FIDESCTL__CLI__SERVER_PORT: \"8080\" FIDESCTL__API__DATABASE_HOST: \"fidesctl-db\" volumes: - type: bind source: . target: /fides read_only: False fidesctl-db: image: postgres:12 healthcheck: test: [\"CMD-SHELL\", \"pg_isready -U postgres\"] interval: 5s timeout: 5s retries: 5 volumes: - postgres-fidesctl:/var/lib/postgresql/data expose: - 5432 ports: - \"5432:5432\" environment: - POSTGRES_USER=postgres - POSTGRES_PASSWORD=fidesctl - POSTGRES_DB=fidesctl volumes: postgres-fidesctl: Now you can start interacting with your installation. Run the following commands to get going: docker-compose up -d -> This will spin up the docker-compose file in the background. docker-compose run --rm fidesctl /bin/bash -> This opens a shell within the fidesctl container. fidesctl init -> This will create a default configuration file at ./.fides/fidesctl.toml . 1 2 3 4 5 6 7 8 Created a fidesctl config file: ./.fides/fidesctl.toml To learn more about configuring fidesctl, see: https://ethyca.github.io/fides/installation/configuration/ ---------- For example policies and help getting started, see: https://ethyca.github.io/fides/guides/policies/ ---------- Fidesctl initialization complete. fidesctl status -> This confirms that your fidesctl CLI can reach the server and everything is ready to go! Once your installation is running, you can use fidesctl from the shell to get a list of all the possible CLI commands . Next Steps You're now ready to start enforcing privacy with Fidesctl! See the Tutorial page for a step-by-step guide on setting up a Fidesctl data privacy workflow.","title":"Running Fidesctl in Docker"},{"location":"quickstart/docker/#running-fidesctl-in-docker","text":"The recommended way to get Fidesctl running is via Docker. The following guide will describe how to get things going, step-by-step.","title":"Running Fidesctl in Docker"},{"location":"quickstart/docker/#system-requirements","text":"Docker and Docker-Compose are the only requirements here. Install docker locally (see Docker Desktop or your preferred installation). The minimum verified Docker version is 20.10.8 If your docker installation did not include docker-compose , make sure to get at least version 1.29.0 . Installation instructions can be found here . Create a .fides folder in the root directory of your project (or in the same directory as your docker compose file).","title":"System Requirements"},{"location":"quickstart/docker/#docker-setup","text":"This is a reference file that you can copy/paste into a local docker-compose.yml file, which should be created in your project's root folder. It will create a database and spin up the fidesctl webserver. Make sure that you don't have anything else running on port 5432 or 8080 before using this file. docker-compose.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 services: fidesctl: image: ethyca/fidesctl command: fidesctl webserver healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://0.0.0.0:8000/health\"] interval: 5s timeout: 5s retries: 5 depends_on: fidesctl-db: condition: service_healthy expose: - 8080 ports: - \"8080:8080\" environment: FIDESCTL_TEST_MODE: \"True\" FIDESCTL__CLI__SERVER_HOST: \"fidesctl\" FIDESCTL__CLI__SERVER_PORT: \"8080\" FIDESCTL__API__DATABASE_HOST: \"fidesctl-db\" volumes: - type: bind source: . target: /fides read_only: False fidesctl-db: image: postgres:12 healthcheck: test: [\"CMD-SHELL\", \"pg_isready -U postgres\"] interval: 5s timeout: 5s retries: 5 volumes: - postgres-fidesctl:/var/lib/postgresql/data expose: - 5432 ports: - \"5432:5432\" environment: - POSTGRES_USER=postgres - POSTGRES_PASSWORD=fidesctl - POSTGRES_DB=fidesctl volumes: postgres-fidesctl: Now you can start interacting with your installation. Run the following commands to get going: docker-compose up -d -> This will spin up the docker-compose file in the background. docker-compose run --rm fidesctl /bin/bash -> This opens a shell within the fidesctl container. fidesctl init -> This will create a default configuration file at ./.fides/fidesctl.toml . 1 2 3 4 5 6 7 8 Created a fidesctl config file: ./.fides/fidesctl.toml To learn more about configuring fidesctl, see: https://ethyca.github.io/fides/installation/configuration/ ---------- For example policies and help getting started, see: https://ethyca.github.io/fides/guides/policies/ ---------- Fidesctl initialization complete. fidesctl status -> This confirms that your fidesctl CLI can reach the server and everything is ready to go! Once your installation is running, you can use fidesctl from the shell to get a list of all the possible CLI commands .","title":"Docker Setup"},{"location":"quickstart/docker/#next-steps","text":"You're now ready to start enforcing privacy with Fidesctl! See the Tutorial page for a step-by-step guide on setting up a Fidesctl data privacy workflow.","title":"Next Steps"},{"location":"quickstart/local_full/","text":"Running Fidesctl Locally (Full Installation) Fidesctl can be spun up locally in its entirety without relying on Docker, but it is a bit more complicated. If you'd like something simpler, please see the Running Fidesctl in Docker guide for the recommended setup experience or Running Fidesctl Locally (Standalone) for the simplest possible installation. System Requirements See the Prerequisites and Dependencies page for more information. Fidesctl installation The next step is to install fidesctl via pip with the required extras: 1 pip install \"fidesctl[webserver]\" For more information on pip installing fidesctl as well as the other potential extras, see the Installation from PyPI guide. Database installation Due to environmental differences, there is no specific guide on running/configuring your own Postgres database outside of the version constraints mentioned in the System Requirements section above. Make sure to note your database credentials and use them to generate a SQLAlchemy Connection String . This will be used in the database_url configuration value mentioned below. Initializing Fidesctl With Fidesctl installed, it's time to initialize the project so we have some place to start adding resource manifests and tweaking our configuration. Initialize Fidesctl 1 fidesctl init Expected Output 1 2 3 4 5 6 7 8 9 10 11 12 Initializing Fidesctl... ---------- Created a './.fides' directory. ---------- Created a fidesctl config file: ./.fides/fidesctl.toml To learn more about configuring fidesctl, see: https://ethyca.github.io/fides/installation/configuration/ ---------- For example policies and help getting started, see: https://ethyca.github.io/fides/guides/policies/ ---------- Fidesctl initialization complete. Configuring Fidesctl See our Configuration guide for more information on how to configure fidesctl. Running the webserver Now that we've spun up our database and set our configuration values, it's time to start our webserver. In a shell, run the following command: 1 fidesctl webserver The fidesctl webserver will now be accessible at localhost:8080 , you can test this by going to localhost:8080/health and localhost:8080/docs . Using the CLI Now that the database and webserver are running, it's time to verify that the whole installation is working properly. Run the command fidesctl status to make sure that the CLI can talk to the webserver. The output should look something like this: 1 2 3 root@2da501a72f8f:/fides/fidesctl# fidesctl status Getting server status... Server is reachable and the client/server application versions match. That's it! Your local installation of fidesctl is completely up and running. Next Steps See the Tutorial page for a step-by-step guide on setting up a Fides data privacy workflow.","title":"Running Fidesctl Locally (Full Installation)"},{"location":"quickstart/local_full/#running-fidesctl-locally-full-installation","text":"Fidesctl can be spun up locally in its entirety without relying on Docker, but it is a bit more complicated. If you'd like something simpler, please see the Running Fidesctl in Docker guide for the recommended setup experience or Running Fidesctl Locally (Standalone) for the simplest possible installation.","title":"Running Fidesctl Locally (Full Installation)"},{"location":"quickstart/local_full/#system-requirements","text":"See the Prerequisites and Dependencies page for more information.","title":"System Requirements"},{"location":"quickstart/local_full/#fidesctl-installation","text":"The next step is to install fidesctl via pip with the required extras: 1 pip install \"fidesctl[webserver]\" For more information on pip installing fidesctl as well as the other potential extras, see the Installation from PyPI guide.","title":"Fidesctl installation"},{"location":"quickstart/local_full/#database-installation","text":"Due to environmental differences, there is no specific guide on running/configuring your own Postgres database outside of the version constraints mentioned in the System Requirements section above. Make sure to note your database credentials and use them to generate a SQLAlchemy Connection String . This will be used in the database_url configuration value mentioned below.","title":"Database installation"},{"location":"quickstart/local_full/#initializing-fidesctl","text":"With Fidesctl installed, it's time to initialize the project so we have some place to start adding resource manifests and tweaking our configuration. Initialize Fidesctl 1 fidesctl init Expected Output 1 2 3 4 5 6 7 8 9 10 11 12 Initializing Fidesctl... ---------- Created a './.fides' directory. ---------- Created a fidesctl config file: ./.fides/fidesctl.toml To learn more about configuring fidesctl, see: https://ethyca.github.io/fides/installation/configuration/ ---------- For example policies and help getting started, see: https://ethyca.github.io/fides/guides/policies/ ---------- Fidesctl initialization complete.","title":"Initializing Fidesctl"},{"location":"quickstart/local_full/#configuring-fidesctl","text":"See our Configuration guide for more information on how to configure fidesctl.","title":"Configuring Fidesctl"},{"location":"quickstart/local_full/#running-the-webserver","text":"Now that we've spun up our database and set our configuration values, it's time to start our webserver. In a shell, run the following command: 1 fidesctl webserver The fidesctl webserver will now be accessible at localhost:8080 , you can test this by going to localhost:8080/health and localhost:8080/docs .","title":"Running the webserver"},{"location":"quickstart/local_full/#using-the-cli","text":"Now that the database and webserver are running, it's time to verify that the whole installation is working properly. Run the command fidesctl status to make sure that the CLI can talk to the webserver. The output should look something like this: 1 2 3 root@2da501a72f8f:/fides/fidesctl# fidesctl status Getting server status... Server is reachable and the client/server application versions match. That's it! Your local installation of fidesctl is completely up and running.","title":"Using the CLI"},{"location":"quickstart/local_full/#next-steps","text":"See the Tutorial page for a step-by-step guide on setting up a Fides data privacy workflow.","title":"Next Steps"},{"location":"quickstart/local_standalone/","text":"Running Fidesctl Locally (Standalone) This method of running fidesctl requires zero dependencies outside of Python and a default pip installation of fidesctl. It is intended as the fastest possible quick start and is not designed for production-grade deployments. In standalone mode most CLI commands will not work as they require webserver connectivity for persistence, and so those commands are not available. Crucially though, the core evaluation functionality is still present. To run in standalone mode, use one of the following methods: CLI flag 1 fidesctl --local <subcommand> fidesctl.toml 1 2 [cli] local_mode = true For more information on running a full fidesctl installation, see the Running Fidesctl Locally (Full Installation) or Running Fidesctl in Docker pages. System Requirements See the Python section of the Prerequisites and Dependencies page for more information. Fidesctl Installation The next step is to install fidesctl via pip: 1 pip install fidesctl For more information on installing fidesctl with pip, as well as the other potential extras, see the Installation from PyPI guide. Verifying the Installation Now that we have fidesctl installed, let's verify the installation: Command 1 fidesctl --version Expected Output 1 fidesctl, version 1.0.0 Initializing Fidesctl With Fidesctl installed, it's time to initialize fidesctl for a project, so we have some place to start adding resource manifests and tweaking our configuration. Switch to your project's root directory, and initialize fidesctl: Initialize Fidesctl 1 fidesctl init Expected Output 1 2 3 4 5 6 7 8 9 10 11 12 Initializing Fidesctl... ---------- Created a './.fides' directory. ---------- Created a fidesctl config file: ./.fides/fidesctl.toml To learn more about configuring fidesctl, see: https://ethyca.github.io/fides/installation/configuration/ ---------- For example policies and help getting started, see: https://ethyca.github.io/fides/guides/policies/ ---------- Fidesctl initialization complete. That's it! Your local standalone installation of fidesctl is up and running. Next Steps See the Tutorial page for a step-by-step guide on setting up a Fides data privacy workflow.","title":"Running Fidesctl Locally (Standalone)"},{"location":"quickstart/local_standalone/#running-fidesctl-locally-standalone","text":"This method of running fidesctl requires zero dependencies outside of Python and a default pip installation of fidesctl. It is intended as the fastest possible quick start and is not designed for production-grade deployments. In standalone mode most CLI commands will not work as they require webserver connectivity for persistence, and so those commands are not available. Crucially though, the core evaluation functionality is still present. To run in standalone mode, use one of the following methods: CLI flag 1 fidesctl --local <subcommand> fidesctl.toml 1 2 [cli] local_mode = true For more information on running a full fidesctl installation, see the Running Fidesctl Locally (Full Installation) or Running Fidesctl in Docker pages.","title":"Running Fidesctl Locally (Standalone)"},{"location":"quickstart/local_standalone/#system-requirements","text":"See the Python section of the Prerequisites and Dependencies page for more information.","title":"System Requirements"},{"location":"quickstart/local_standalone/#fidesctl-installation","text":"The next step is to install fidesctl via pip: 1 pip install fidesctl For more information on installing fidesctl with pip, as well as the other potential extras, see the Installation from PyPI guide.","title":"Fidesctl Installation"},{"location":"quickstart/local_standalone/#verifying-the-installation","text":"Now that we have fidesctl installed, let's verify the installation: Command 1 fidesctl --version Expected Output 1 fidesctl, version 1.0.0","title":"Verifying the Installation"},{"location":"quickstart/local_standalone/#initializing-fidesctl","text":"With Fidesctl installed, it's time to initialize fidesctl for a project, so we have some place to start adding resource manifests and tweaking our configuration. Switch to your project's root directory, and initialize fidesctl: Initialize Fidesctl 1 fidesctl init Expected Output 1 2 3 4 5 6 7 8 9 10 11 12 Initializing Fidesctl... ---------- Created a './.fides' directory. ---------- Created a fidesctl config file: ./.fides/fidesctl.toml To learn more about configuring fidesctl, see: https://ethyca.github.io/fides/installation/configuration/ ---------- For example policies and help getting started, see: https://ethyca.github.io/fides/guides/policies/ ---------- Fidesctl initialization complete. That's it! Your local standalone installation of fidesctl is up and running.","title":"Initializing Fidesctl"},{"location":"quickstart/local_standalone/#next-steps","text":"See the Tutorial page for a step-by-step guide on setting up a Fides data privacy workflow.","title":"Next Steps"},{"location":"quickstart/overview/","text":"Quick Start The fastest way to get fidesctl running is to launch it using the fidesctl standlone) guide. For guides on setting up a complete installation, see the Docker instructions here or the local full-installation instructions here .","title":"Overview"},{"location":"quickstart/overview/#quick-start","text":"The fastest way to get fidesctl running is to launch it using the fidesctl standlone) guide. For guides on setting up a complete installation, see the Docker instructions here or the local full-installation instructions here .","title":"Quick Start"},{"location":"schemas/config/","text":"1","title":"Config"},{"location":"tutorial/","text":"Tutorial Overview In this tutorial you will learn how to use fidesctl to solve a real-world data privacy problem. These steps closely follow the example found in the ethyca/fidesdemo repository here . You will run a local instance of a basic web app to demonstrate the use of Fidesctl as part of a \"real\" project that uses: Flask to run a web server simulating a basic e-commerce application PostgreSQL as the application's database SQLAlchemy to connect to the database fidesctl to declare privacy manifests and evaluate policies The app itself is the Flask tutorial app , but modified to simulate an e-commerce marketplace. This helps to highlight some basic examples of data categories that might be stored in a \"real\" user-facing application. Setup Instructions System Requirements Before beginning, ensure you have the following software installed and configured to your liking: Docker (v12+) Python (v3.7+) Make pg_config (required for the Python project. Installed via Homebrew with brew install libpq or brew install postgres .) Installation Clone the ethyca/fidesdemo repository to your machine. Checkout the repository's tutorial-start tag : 1 git checkout tutorial-start Each step in this tutorial will explain the changes made in each commit of the fidesdemo repository. You can follow along by checking out each one, or by building everything yourself and comparing your work to each commit's changeset. Navigate to the repository directory in your command line, and run: 1 make install This will create the project's virtual environment, and set up all required containers, databases, and dependencies. If you prefer, you may execute the project's test suite by running: 1 make test About the Example Application (\"Flaskr\") This example application is meant to simulate a basic e-commerce marketplace where users can create accounts and purchase products from one another. Using the web app you can: Register a new user Login as a user Post a \"product\" for sale Delete/update products you've posted Purchase a product (no products are actually for sale) The schema itself is designed to highlight a few very simple examples of how identifiable data might get stored in a web application like this one. The sample data below shows what this looks like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 flaskr = # SELECT * FROM users; id | created_at | email | password | first_name | last_name ----+---------------------+-------------------+------------------------------------+------------+----------- 1 | 2020 -01-01 00 :00:00 | admin@example.com | pbkdf2:sha256:260000 $O87nanbSkl ... | Admin | User 2 | 2020 -01-03 00 :00:00 | user@example.com | pbkdf2:sha256:260000 $PGcBy5NzZe ... | Example | User ( 2 rows ) flaskr = # SELECT * FROM products; id | created_at | seller_id | name | description | price ----+---------------------+-----------+-------------------+--------------------------------------+------- 1 | 2020 -01-01 12 :00:00 | 1 | Example Product 1 | A description for example product #1 | 10 2 | 2020 -01-02 12 :00:00 | 1 | Example Product 2 | A description for example product #2 | 20 3 | 2020 -01-03 12 :00:00 | 2 | Example Product 3 | A description for example product #3 | 50 ( 3 rows ) flaskr = # SELECT * FROM purchases; id | created_at | product_id | buyer_id | street_1 | street_2 | city | state | zip ----+---------------------+------------+----------+----------------+----------+-------------+-------+------- 1 | 2020 -01-04 12 :00:00 | 1 | 2 | 123 Example St | Apt 123 | Exampletown | NY | 12345 ( 1 row ) Check Your Progress After running the commands outlined in the Installation section, your app should resemble the state of the ethyca/fidesdemo repository at the tutorial-start tag. Next: Add Fidesctl to the App Work within the sample app prior to the installation and configuration of the Fides developer tools to add fidesctl .","title":"Overview"},{"location":"tutorial/#tutorial-overview","text":"In this tutorial you will learn how to use fidesctl to solve a real-world data privacy problem. These steps closely follow the example found in the ethyca/fidesdemo repository here . You will run a local instance of a basic web app to demonstrate the use of Fidesctl as part of a \"real\" project that uses: Flask to run a web server simulating a basic e-commerce application PostgreSQL as the application's database SQLAlchemy to connect to the database fidesctl to declare privacy manifests and evaluate policies The app itself is the Flask tutorial app , but modified to simulate an e-commerce marketplace. This helps to highlight some basic examples of data categories that might be stored in a \"real\" user-facing application.","title":"Tutorial Overview"},{"location":"tutorial/#setup-instructions","text":"","title":"Setup Instructions"},{"location":"tutorial/#system-requirements","text":"Before beginning, ensure you have the following software installed and configured to your liking: Docker (v12+) Python (v3.7+) Make pg_config (required for the Python project. Installed via Homebrew with brew install libpq or brew install postgres .)","title":"System Requirements"},{"location":"tutorial/#installation","text":"Clone the ethyca/fidesdemo repository to your machine. Checkout the repository's tutorial-start tag : 1 git checkout tutorial-start Each step in this tutorial will explain the changes made in each commit of the fidesdemo repository. You can follow along by checking out each one, or by building everything yourself and comparing your work to each commit's changeset. Navigate to the repository directory in your command line, and run: 1 make install This will create the project's virtual environment, and set up all required containers, databases, and dependencies. If you prefer, you may execute the project's test suite by running: 1 make test","title":"Installation"},{"location":"tutorial/#about-the-example-application-flaskr","text":"This example application is meant to simulate a basic e-commerce marketplace where users can create accounts and purchase products from one another. Using the web app you can: Register a new user Login as a user Post a \"product\" for sale Delete/update products you've posted Purchase a product (no products are actually for sale) The schema itself is designed to highlight a few very simple examples of how identifiable data might get stored in a web application like this one. The sample data below shows what this looks like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 flaskr = # SELECT * FROM users; id | created_at | email | password | first_name | last_name ----+---------------------+-------------------+------------------------------------+------------+----------- 1 | 2020 -01-01 00 :00:00 | admin@example.com | pbkdf2:sha256:260000 $O87nanbSkl ... | Admin | User 2 | 2020 -01-03 00 :00:00 | user@example.com | pbkdf2:sha256:260000 $PGcBy5NzZe ... | Example | User ( 2 rows ) flaskr = # SELECT * FROM products; id | created_at | seller_id | name | description | price ----+---------------------+-----------+-------------------+--------------------------------------+------- 1 | 2020 -01-01 12 :00:00 | 1 | Example Product 1 | A description for example product #1 | 10 2 | 2020 -01-02 12 :00:00 | 1 | Example Product 2 | A description for example product #2 | 20 3 | 2020 -01-03 12 :00:00 | 2 | Example Product 3 | A description for example product #3 | 50 ( 3 rows ) flaskr = # SELECT * FROM purchases; id | created_at | product_id | buyer_id | street_1 | street_2 | city | state | zip ----+---------------------+------------+----------+----------------+----------+-------------+-------+------- 1 | 2020 -01-04 12 :00:00 | 1 | 2 | 123 Example St | Apt 123 | Exampletown | NY | 12345 ( 1 row )","title":"About the Example Application (\"Flaskr\")"},{"location":"tutorial/#check-your-progress","text":"After running the commands outlined in the Installation section, your app should resemble the state of the ethyca/fidesdemo repository at the tutorial-start tag.","title":"Check Your Progress"},{"location":"tutorial/#next-add-fidesctl-to-the-app","text":"Work within the sample app prior to the installation and configuration of the Fides developer tools to add fidesctl .","title":"Next: Add Fidesctl to the App"},{"location":"tutorial/add/","text":"Add Fidesctl to the App In this step you will incorporate fidesctl , which will enable you to declare your system , dataset , and policy resources as manifest YAML files. Add the fidesctl Dependency Open the requirements.txt file and add the fidesctl dependency by including the following line: 1 fidesctl>=1.0.0 Then, install the dependencies by running: 1 pip install -r requirements.txt Initializing Fidesctl With fidesctl installed, it's time to initialize the project so we have some place to start adding resource manifests and tweaking our configuration. Run the following command and follow the prompts to get your local fidesctl instance initialized. Initialize Fidesctl 1 fidesctl init Expected Output 1 2 3 4 5 6 7 8 9 10 11 12 Initializing Fidesctl... ---------- Created a './.fides' directory. ---------- Created a fidesctl config file: ./.fides/fidesctl.toml To learn more about configuring fidesctl, see: https://ethyca.github.io/fides/installation/configuration/ ---------- For example policies and help getting started, see: https://ethyca.github.io/fides/guides/policies/ ---------- Fidesctl initialization complete. Configuring Fidesctl See our Configuration guide for more information on how to configure fidesctl. Run Fidesctl via Docker Now that the dependency is included in the project and the configuration is in place, the fidesctl server needs to be told to run. The app uses docker-compose to orchestrate resources, so include fidesctl as a service by adding the following configuration after the database service: 1 2 3 4 5 6 7 8 9 10 11 fidesctl : image : ethyca/fidesctl:latest depends_on : - db command : fidesctl webserver expose : - 8080 ports : - \"8080:8080\" environment : - FIDESCTL__API__DATABASE_URL=postgres:postgres@db:5432/fidesctl See the fidesctl installation guide for a more detailed fidesctl server setup walkthrough, and the docker-compose documentation for an explanation of the above configuration options. Add Makefile Commands This step is optional, but the commands added to the Makefile here will be referenced later in this tutorial. The above changes will enable fidesctl CLI commands to be run within the project's virtual environment. You can simplify usage of the fidesctl CLI by adding commands to the Makefile like the following: 1 2 3 4 5 6 7 8 9 10 11 fidesctl-init-db : compose - up @echo \"Initializing fidesctl db..\" ./venv/bin/fidesctl db init fidesctl-evaluate : compose - up @echo \"Evaluating policy with fidesctl...\" ./venv/bin/fidesctl evaluate --dry fides_resources fidesctl-generate-dataset : compose - up @echo \"Generating dataset with fidesctl...\" ./venv/bin/fidesctl generate dataset db postgresql://postgres:postgres@localhost:5432/flaskr example.yml Note: There are additional Makefile changes included in the fidesdemo repository, but they are only intended to enable cleaner usage of this project for demonstration purposes. Check Your Progress After making the above changes, your app should resemble the state of the ethyca/fidesdemo repository at the fidesops-start tag. Next: Annotate the Resources Now that the fidesctl tools are available to use within the app's virtual environment, the next step is to configure fidesctl to work with the specifics of this app. This can be done by creating manifest files to annotate the resources .","title":"Add Fidesctl to the App"},{"location":"tutorial/add/#add-fidesctl-to-the-app","text":"In this step you will incorporate fidesctl , which will enable you to declare your system , dataset , and policy resources as manifest YAML files.","title":"Add Fidesctl to the App"},{"location":"tutorial/add/#add-the-fidesctl-dependency","text":"Open the requirements.txt file and add the fidesctl dependency by including the following line: 1 fidesctl>=1.0.0 Then, install the dependencies by running: 1 pip install -r requirements.txt","title":"Add the fidesctl Dependency"},{"location":"tutorial/add/#initializing-fidesctl","text":"With fidesctl installed, it's time to initialize the project so we have some place to start adding resource manifests and tweaking our configuration. Run the following command and follow the prompts to get your local fidesctl instance initialized. Initialize Fidesctl 1 fidesctl init Expected Output 1 2 3 4 5 6 7 8 9 10 11 12 Initializing Fidesctl... ---------- Created a './.fides' directory. ---------- Created a fidesctl config file: ./.fides/fidesctl.toml To learn more about configuring fidesctl, see: https://ethyca.github.io/fides/installation/configuration/ ---------- For example policies and help getting started, see: https://ethyca.github.io/fides/guides/policies/ ---------- Fidesctl initialization complete.","title":"Initializing Fidesctl"},{"location":"tutorial/add/#configuring-fidesctl","text":"See our Configuration guide for more information on how to configure fidesctl.","title":"Configuring Fidesctl"},{"location":"tutorial/add/#run-fidesctl-via-docker","text":"Now that the dependency is included in the project and the configuration is in place, the fidesctl server needs to be told to run. The app uses docker-compose to orchestrate resources, so include fidesctl as a service by adding the following configuration after the database service: 1 2 3 4 5 6 7 8 9 10 11 fidesctl : image : ethyca/fidesctl:latest depends_on : - db command : fidesctl webserver expose : - 8080 ports : - \"8080:8080\" environment : - FIDESCTL__API__DATABASE_URL=postgres:postgres@db:5432/fidesctl See the fidesctl installation guide for a more detailed fidesctl server setup walkthrough, and the docker-compose documentation for an explanation of the above configuration options.","title":"Run Fidesctl via Docker"},{"location":"tutorial/add/#add-makefile-commands","text":"This step is optional, but the commands added to the Makefile here will be referenced later in this tutorial. The above changes will enable fidesctl CLI commands to be run within the project's virtual environment. You can simplify usage of the fidesctl CLI by adding commands to the Makefile like the following: 1 2 3 4 5 6 7 8 9 10 11 fidesctl-init-db : compose - up @echo \"Initializing fidesctl db..\" ./venv/bin/fidesctl db init fidesctl-evaluate : compose - up @echo \"Evaluating policy with fidesctl...\" ./venv/bin/fidesctl evaluate --dry fides_resources fidesctl-generate-dataset : compose - up @echo \"Generating dataset with fidesctl...\" ./venv/bin/fidesctl generate dataset db postgresql://postgres:postgres@localhost:5432/flaskr example.yml Note: There are additional Makefile changes included in the fidesdemo repository, but they are only intended to enable cleaner usage of this project for demonstration purposes.","title":"Add Makefile Commands"},{"location":"tutorial/add/#check-your-progress","text":"After making the above changes, your app should resemble the state of the ethyca/fidesdemo repository at the fidesops-start tag.","title":"Check Your Progress"},{"location":"tutorial/add/#next-annotate-the-resources","text":"Now that the fidesctl tools are available to use within the app's virtual environment, the next step is to configure fidesctl to work with the specifics of this app. This can be done by creating manifest files to annotate the resources .","title":"Next: Annotate the Resources"},{"location":"tutorial/dataset/","text":"Annotate the Dataset Making the fidesctl tools available within the app's virtual environment is just the beginning. Next, configure fidesctl for this app by annotating its resources using manifest files. First, create a fides_resources directory at the project root. This is where the manifest files will be stored. Note: In a production app this directory can have any name, but it's a best practice to create a specific directory to house the fidesctl manifest files. Fundamentally, the data ecosystem is built on data that is stored somewhere . In fidesctl, Datasets are used for granular, field-level annotations of exactly what data your systems are storing and where that data is stored. For example, an app might declare one dataset for a Postgres application database, a second dataset for a Mongo orders collection, and a third dataset for some CSV files in cloud storage. The Dataset resource provides a database-agnostic way to annotate the fields stored in these systems with Data Categories, providing a metadata layer consumable by other tooling. This app contains a single PostgreSQL dataset. Create a dataset resource to annotate it by adding a flaskr_postgres_dataset.yml file to the fides_resources directory. To annotate this dataset correctly, go through each column of each table and answer the question: \"What data categories are stored here?\" For this project, the file should contain the following configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 dataset : - fides_key : flaskr_postgres_dataset name : Flaskr Example PostgreSQL Database description : Application database for Flaskr example app collections : - name : products fields : - name : created_at data_categories : [ system.operations ] - name : description data_categories : [ user.provided.identifiable ] - name : id data_categories : [ system.operations ] - name : name data_categories : [ user.provided.identifiable ] - name : price data_categories : [ user.provided.identifiable ] - name : seller_id data_categories : [ user.derived.identifiable.unique_id ] - name : purchases fields : - name : buyer_id data_categories : [ user.derived.identifiable.unique_id ] - name : city data_categories : [ user.provided.identifiable.contact.city ] - name : created_at data_categories : [ system.operations ] - name : id data_categories : [ system.operations ] - name : product_id data_categories : [ system.operations ] - name : state data_categories : [ user.provided.identifiable.contact.state ] - name : street_1 data_categories : [ user.provided.identifiable.contact.street ] - name : street_2 data_categories : [ user.provided.identifiable.contact.street ] - name : zip data_categories : [ user.provided.identifiable.contact.postal_code ] - name : users fields : - name : created_at data_categories : [ system.operations ] - name : email data_categories : [ user.provided.identifiable.contact.email ] - name : first_name data_categories : [ user.provided.identifiable.name ] - name : id data_categories : [ user.derived.identifiable.unique_id ] - name : last_name data_categories : [ user.provided.identifiable.name ] - name : password data_categories : [ user.provided.identifiable.credentials.password ] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized As an alternative to manually authoring the resource file, you can also use the generate CLI command. The CLI will connect to the database and automatically generate a non-annotated resource YAML file in the specified location, based on the database schema. For this project, the command is: 1 2 3 ./venv/bin/fidesctl generate dataset db \\ postgresql://postgres:postgres@localhost:5432/flaskr \\ fides_resources/flaskr_postgres_dataset.yml Understanding the Dataset Resource This YAML serves as the foundation of fideslang , the Fides language; it answers \" What data and kinds of data do we have? \" and \" How is our data organized? \". The language is built on declaring the types of data found in storage for your organization. In traditional SQL, fidesctl defines the following: \"datasets\" as database schemas \"collections\" as database tables \"fields\" as database columns For NoSQL datasets, fidesctl defines the following: \"dataset\" \"collection\" as a logical grouping of data fields (ie: in MongoDB, this is called a \"Collection\") \"fields\" as a reference to an individual data element (ie: in MongoDB, this is called a \"field\") Additionally, fideslang has attributes that describe what kind of data is contained in this dataset. We use the following attributes to describe the data: Name Type Description name String The name of this field description String A description of what this field contains data_categories List[FidesKey] The types of sensitive data, as defined by the taxonomy, that can be found in this field data_qualifier FidesKey The level of deidentification for the dataset For more detail on Dataset resources, see the full Dataset resource documentation . PRO TIP As you're progressing with the tutorial, we recommend installing our fidesctl VS Code extension , which will validate the syntax in real-time as you're writing your resource files! Maintaining a Dataset Resource As apps add more databases and other services to store potentially sensitive data, it is recommended that updating this resource file becomes a part of the development process when building a new feature. Next: Annotate the System Resource With the underlying database resource declared, you must now include the database in an application-level System resource annotation .","title":"Annotate the Dataset"},{"location":"tutorial/dataset/#annotate-the-dataset","text":"Making the fidesctl tools available within the app's virtual environment is just the beginning. Next, configure fidesctl for this app by annotating its resources using manifest files. First, create a fides_resources directory at the project root. This is where the manifest files will be stored. Note: In a production app this directory can have any name, but it's a best practice to create a specific directory to house the fidesctl manifest files. Fundamentally, the data ecosystem is built on data that is stored somewhere . In fidesctl, Datasets are used for granular, field-level annotations of exactly what data your systems are storing and where that data is stored. For example, an app might declare one dataset for a Postgres application database, a second dataset for a Mongo orders collection, and a third dataset for some CSV files in cloud storage. The Dataset resource provides a database-agnostic way to annotate the fields stored in these systems with Data Categories, providing a metadata layer consumable by other tooling. This app contains a single PostgreSQL dataset. Create a dataset resource to annotate it by adding a flaskr_postgres_dataset.yml file to the fides_resources directory. To annotate this dataset correctly, go through each column of each table and answer the question: \"What data categories are stored here?\" For this project, the file should contain the following configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 dataset : - fides_key : flaskr_postgres_dataset name : Flaskr Example PostgreSQL Database description : Application database for Flaskr example app collections : - name : products fields : - name : created_at data_categories : [ system.operations ] - name : description data_categories : [ user.provided.identifiable ] - name : id data_categories : [ system.operations ] - name : name data_categories : [ user.provided.identifiable ] - name : price data_categories : [ user.provided.identifiable ] - name : seller_id data_categories : [ user.derived.identifiable.unique_id ] - name : purchases fields : - name : buyer_id data_categories : [ user.derived.identifiable.unique_id ] - name : city data_categories : [ user.provided.identifiable.contact.city ] - name : created_at data_categories : [ system.operations ] - name : id data_categories : [ system.operations ] - name : product_id data_categories : [ system.operations ] - name : state data_categories : [ user.provided.identifiable.contact.state ] - name : street_1 data_categories : [ user.provided.identifiable.contact.street ] - name : street_2 data_categories : [ user.provided.identifiable.contact.street ] - name : zip data_categories : [ user.provided.identifiable.contact.postal_code ] - name : users fields : - name : created_at data_categories : [ system.operations ] - name : email data_categories : [ user.provided.identifiable.contact.email ] - name : first_name data_categories : [ user.provided.identifiable.name ] - name : id data_categories : [ user.derived.identifiable.unique_id ] - name : last_name data_categories : [ user.provided.identifiable.name ] - name : password data_categories : [ user.provided.identifiable.credentials.password ] data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized As an alternative to manually authoring the resource file, you can also use the generate CLI command. The CLI will connect to the database and automatically generate a non-annotated resource YAML file in the specified location, based on the database schema. For this project, the command is: 1 2 3 ./venv/bin/fidesctl generate dataset db \\ postgresql://postgres:postgres@localhost:5432/flaskr \\ fides_resources/flaskr_postgres_dataset.yml","title":"Annotate the Dataset"},{"location":"tutorial/dataset/#understanding-the-dataset-resource","text":"This YAML serves as the foundation of fideslang , the Fides language; it answers \" What data and kinds of data do we have? \" and \" How is our data organized? \". The language is built on declaring the types of data found in storage for your organization. In traditional SQL, fidesctl defines the following: \"datasets\" as database schemas \"collections\" as database tables \"fields\" as database columns For NoSQL datasets, fidesctl defines the following: \"dataset\" \"collection\" as a logical grouping of data fields (ie: in MongoDB, this is called a \"Collection\") \"fields\" as a reference to an individual data element (ie: in MongoDB, this is called a \"field\") Additionally, fideslang has attributes that describe what kind of data is contained in this dataset. We use the following attributes to describe the data: Name Type Description name String The name of this field description String A description of what this field contains data_categories List[FidesKey] The types of sensitive data, as defined by the taxonomy, that can be found in this field data_qualifier FidesKey The level of deidentification for the dataset For more detail on Dataset resources, see the full Dataset resource documentation .","title":"Understanding the Dataset Resource"},{"location":"tutorial/dataset/#pro-tip","text":"As you're progressing with the tutorial, we recommend installing our fidesctl VS Code extension , which will validate the syntax in real-time as you're writing your resource files!","title":"PRO TIP"},{"location":"tutorial/dataset/#maintaining-a-dataset-resource","text":"As apps add more databases and other services to store potentially sensitive data, it is recommended that updating this resource file becomes a part of the development process when building a new feature.","title":"Maintaining a Dataset Resource"},{"location":"tutorial/dataset/#next-annotate-the-system-resource","text":"With the underlying database resource declared, you must now include the database in an application-level System resource annotation .","title":"Next: Annotate the System Resource"},{"location":"tutorial/google/","text":"Add Google Analytics To better understand the behavior of the app's users, add Google Analytics to the app and a fidesctl System resource to annotate it. Define the App's Google Analytics Identifier Open the flaskr/__init__.py file in your favorite editor, and define the GOOGLE_ANALYTICS_ID constant below line 7: 1 GOOGLE_ANALYTICS_ID = \"UA-xxxxxxxxx-y\" In the create_app function defined on line 11, include the Google Analytics ID value in the application's configuration by adding the following line below line 17: 1 GOOGLE_ANALYTICS_ID = GOOGLE_ANALYTICS_ID , Add the Google Analytics Script Open the flaskr/templates/base.html file in your favorite editor, and include the following at the beginning of the <head> tag: 1 2 3 4 5 6 7 8 9 10 {% if config['GOOGLE_ANALYTICS_ID'] %} <!-- Global site tag (gtag.js) - Google Analytics --> < script async src = \"https://www.googletagmanager.com/gtag/js?id={{ config['GOOGLE_ANALYTICS_ID'] }}\" ></ script > < script > window . dataLayer = window . dataLayer || []; function gtag (){ dataLayer . push ( arguments );} gtag ( \"js\" , new Date ()); gtag ( \"config\" , \"{{ config['GOOGLE_ANALYTICS_ID'] }}\" ); </ script > {% endif %} Annotate a Fidesctl System Resource To ensure that the app's policies can account for the data collected by Google Analytics, define a new fidesctl System resource by adding a google_analytics_system.yml file to the fides_resources directory. This System resource annotation should reflect the uses of the Google Analytics features configured in this app's implementation. Some things to think about might be: What fields are being tracked? (See the field reference documentation for a list of all possible fields) What data_use value would be appropriate for this app? ( provide vs. improve ) For this System resource, the file should contain the following configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 system : - fides_key : google_analytics_system name : Google Analytics description : Hosted third party analytics to track and analyze user behaviour system_type : Third Party privacy_declarations : # See the Google Analytics documentation for a description of the possible # fields collected by the tracker, including page URL, referrer, cookie ID, etc. # https://developers.google.com/analytics/devguides/collection/analyticsjs/field-reference - name : Track & report on page views data_categories : - user.derived.identifiable.browsing_history - user.derived.identifiable.device.cookie_id - user.derived.identifiable.telemetry - user.derived.identifiable.location - user.derived.nonidentifiable data_use : improve data_subjects : - customer data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized # Google Analytics collects the user's IP address and derives geographic dimensions server-side. # See https://developers.google.com/analytics/devguides/reporting/realtime/dimsmets/geonetwork - name : Derive user geographic location data_categories : - user.derived.identifiable.device.ip_address - user.derived.identifiable.location - user.derived.identifiable data_use : improve data_subjects : - customer # With \"IP Anonymization\" disabled, IP Addresses will remain identifiable. # See https://developers.google.com/analytics/devguides/collection/gtagjs/ip-anonymization data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified There are two privacy_declaration s defined: The use of pseudonymized behavioral data to analyze app usage The use of user IP addresses to derive their geographic location The two declarations reflect the separate purposes for which data is collected and used by Google Analytics here. They are meant to align with the app's actual usage of its specific Google Analytics implementation; there are many other data uses for Google Analytics, but this app does not leverage them. Check Your Progress After making the above changes, your app should resemble the state of the ethyca/fidesdemo repository at the fidesctl-add-google-analytics tag . Next: Manage Google Analytics with Fidesctl Google Analytics is implemented and working correctly, but - oh no! - executing make fidesctl-evaluate shows a failure: 1 2 3 4 5 6 7 8 { \"fides_key\" : \"4e739b1b_732e_43b1_8747_e833905dfc4c_1635789050\" , \"status\" : \"FAIL\" , \"details\" : [ \"Declaration (Derive user geographic location) of System (google_analytics_system) failed Rule (Minimize User Identifiable Data) from Policy (flaskr_policy)\" ], \"message\" : null } In the final step, enable the fidesctl policy already in place to pass by updating Google Analytics .","title":"Add Google Analytics"},{"location":"tutorial/google/#add-google-analytics","text":"To better understand the behavior of the app's users, add Google Analytics to the app and a fidesctl System resource to annotate it.","title":"Add Google Analytics"},{"location":"tutorial/google/#define-the-apps-google-analytics-identifier","text":"Open the flaskr/__init__.py file in your favorite editor, and define the GOOGLE_ANALYTICS_ID constant below line 7: 1 GOOGLE_ANALYTICS_ID = \"UA-xxxxxxxxx-y\" In the create_app function defined on line 11, include the Google Analytics ID value in the application's configuration by adding the following line below line 17: 1 GOOGLE_ANALYTICS_ID = GOOGLE_ANALYTICS_ID ,","title":"Define the App's Google Analytics Identifier"},{"location":"tutorial/google/#add-the-google-analytics-script","text":"Open the flaskr/templates/base.html file in your favorite editor, and include the following at the beginning of the <head> tag: 1 2 3 4 5 6 7 8 9 10 {% if config['GOOGLE_ANALYTICS_ID'] %} <!-- Global site tag (gtag.js) - Google Analytics --> < script async src = \"https://www.googletagmanager.com/gtag/js?id={{ config['GOOGLE_ANALYTICS_ID'] }}\" ></ script > < script > window . dataLayer = window . dataLayer || []; function gtag (){ dataLayer . push ( arguments );} gtag ( \"js\" , new Date ()); gtag ( \"config\" , \"{{ config['GOOGLE_ANALYTICS_ID'] }}\" ); </ script > {% endif %}","title":"Add the Google Analytics Script"},{"location":"tutorial/google/#annotate-a-fidesctl-system-resource","text":"To ensure that the app's policies can account for the data collected by Google Analytics, define a new fidesctl System resource by adding a google_analytics_system.yml file to the fides_resources directory. This System resource annotation should reflect the uses of the Google Analytics features configured in this app's implementation. Some things to think about might be: What fields are being tracked? (See the field reference documentation for a list of all possible fields) What data_use value would be appropriate for this app? ( provide vs. improve ) For this System resource, the file should contain the following configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 system : - fides_key : google_analytics_system name : Google Analytics description : Hosted third party analytics to track and analyze user behaviour system_type : Third Party privacy_declarations : # See the Google Analytics documentation for a description of the possible # fields collected by the tracker, including page URL, referrer, cookie ID, etc. # https://developers.google.com/analytics/devguides/collection/analyticsjs/field-reference - name : Track & report on page views data_categories : - user.derived.identifiable.browsing_history - user.derived.identifiable.device.cookie_id - user.derived.identifiable.telemetry - user.derived.identifiable.location - user.derived.nonidentifiable data_use : improve data_subjects : - customer data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized # Google Analytics collects the user's IP address and derives geographic dimensions server-side. # See https://developers.google.com/analytics/devguides/reporting/realtime/dimsmets/geonetwork - name : Derive user geographic location data_categories : - user.derived.identifiable.device.ip_address - user.derived.identifiable.location - user.derived.identifiable data_use : improve data_subjects : - customer # With \"IP Anonymization\" disabled, IP Addresses will remain identifiable. # See https://developers.google.com/analytics/devguides/collection/gtagjs/ip-anonymization data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified There are two privacy_declaration s defined: The use of pseudonymized behavioral data to analyze app usage The use of user IP addresses to derive their geographic location The two declarations reflect the separate purposes for which data is collected and used by Google Analytics here. They are meant to align with the app's actual usage of its specific Google Analytics implementation; there are many other data uses for Google Analytics, but this app does not leverage them.","title":"Annotate a Fidesctl System Resource"},{"location":"tutorial/google/#check-your-progress","text":"After making the above changes, your app should resemble the state of the ethyca/fidesdemo repository at the fidesctl-add-google-analytics tag .","title":"Check Your Progress"},{"location":"tutorial/google/#next-manage-google-analytics-with-fidesctl","text":"Google Analytics is implemented and working correctly, but - oh no! - executing make fidesctl-evaluate shows a failure: 1 2 3 4 5 6 7 8 { \"fides_key\" : \"4e739b1b_732e_43b1_8747_e833905dfc4c_1635789050\" , \"status\" : \"FAIL\" , \"details\" : [ \"Declaration (Derive user geographic location) of System (google_analytics_system) failed Rule (Minimize User Identifiable Data) from Policy (flaskr_policy)\" ], \"message\" : null } In the final step, enable the fidesctl policy already in place to pass by updating Google Analytics .","title":"Next: Manage Google Analytics with Fidesctl"},{"location":"tutorial/pass/","text":"Manage Google Analytics with Fidesctl By default, Google Analytics disables \"IP Anonymization\" (see the documentation for more information). The \"Minimize User Identifiable Data\" fidesctl Policy resource created earlier in this tutorial is configured to reject data collection of this nature. POP QUIZ There are two options to remedy this situation, and to get the make fidesctl-evaluate command to pass. Which option is best? Modify the \"Minimize User Identifiable Data\" policy resource to accept data collection of this nature Modify the Google Analytics implementation such that it becomes compliant with the \"Minimize User Identifiable Data\" policy Click to see the correct answer Option 2 is the best path forward: the Google Analytics implementation should be modified, not the \"Minimize User Identifiable Data\" policy resource. The policy resource's configuration is dictated by the app's Privacy Policy, and changes could lead to larger compliance issues throughout the system. Enable IP Anonymization Open the flaskr/templates/base.html file in your favorite editor, and add the following line just above the closing <script> tag in the Google Analytics script: 1 2 3 4 5 6 7 8 9 10 11 {% if config['GOOGLE_ANALYTICS_ID'] %} <!-- Global site tag (gtag.js) - Google Analytics --> <script async src=\"https://www.googletagmanager.com/gtag/js?id={{ config['GOOGLE_ANALYTICS_ID'] }}\"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\"js\", new Date()); gtag(\"config\", \"{{ config['GOOGLE_ANALYTICS_ID'] }}\"); + gtag(\"config\", \"{{ config['GOOGLE_ANALYTICS_ID'] }}\", { 'anonymize_ip': true }); </script> {% endif %} Update the Google Analytics System Resource Now that the data collection practices in the Google Analytics script have changed, the associated fidesctl System resource should be updated accordingly. Open the fides_resources/google_analytics_system.yml file in your favorite editor, and modify the last line (the data_qualifier configuration) so that it reads: 1 data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized By removing the final identified key of the Fides taxonomy, the updated nature of the data collection practices used in this System resource now aligns with the actual behavior of the updated Google Analytics script. Evaluate the Fidesctl Policies Execute the make fidesctl-evaluate command one final time. You should see the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Evaluating policy with fidesctl... ./venv/bin/fidesctl evaluate --dry fides_resources Loading resource manifests from: fides_resources Taxonomy successfully created. ---------- Processing dataset resources... WOULD CREATE 0 dataset resources. WOULD UPDATE 0 dataset resources. WOULD SKIP 1 dataset resources. ---------- Processing system resources... WOULD CREATE 0 system resources. WOULD UPDATE 1 system resources. WOULD SKIP 1 system resources. ---------- Processing policy resources... WOULD CREATE 0 policy resources. WOULD UPDATE 0 policy resources. WOULD SKIP 1 policy resources. ---------- Loading resource manifests from: fides_resources Taxonomy successfully created. Evaluating the following policies: flaskr_policy ---------- Checking for missing resources... Executing evaluations... Evaluation passed! The fidesctl policy evaluation passes! Check Your Progress After making the above changes, your app should resemble the state of the ethyca/fidesdemo repository at the fidesctl-demo tag.","title":"Manage Google Analytics with Fidesctl"},{"location":"tutorial/pass/#manage-google-analytics-with-fidesctl","text":"By default, Google Analytics disables \"IP Anonymization\" (see the documentation for more information). The \"Minimize User Identifiable Data\" fidesctl Policy resource created earlier in this tutorial is configured to reject data collection of this nature.","title":"Manage Google Analytics with Fidesctl"},{"location":"tutorial/pass/#pop-quiz","text":"There are two options to remedy this situation, and to get the make fidesctl-evaluate command to pass. Which option is best? Modify the \"Minimize User Identifiable Data\" policy resource to accept data collection of this nature Modify the Google Analytics implementation such that it becomes compliant with the \"Minimize User Identifiable Data\" policy Click to see the correct answer Option 2 is the best path forward: the Google Analytics implementation should be modified, not the \"Minimize User Identifiable Data\" policy resource. The policy resource's configuration is dictated by the app's Privacy Policy, and changes could lead to larger compliance issues throughout the system.","title":"POP QUIZ"},{"location":"tutorial/pass/#enable-ip-anonymization","text":"Open the flaskr/templates/base.html file in your favorite editor, and add the following line just above the closing <script> tag in the Google Analytics script: 1 2 3 4 5 6 7 8 9 10 11 {% if config['GOOGLE_ANALYTICS_ID'] %} <!-- Global site tag (gtag.js) - Google Analytics --> <script async src=\"https://www.googletagmanager.com/gtag/js?id={{ config['GOOGLE_ANALYTICS_ID'] }}\"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\"js\", new Date()); gtag(\"config\", \"{{ config['GOOGLE_ANALYTICS_ID'] }}\"); + gtag(\"config\", \"{{ config['GOOGLE_ANALYTICS_ID'] }}\", { 'anonymize_ip': true }); </script> {% endif %}","title":"Enable IP Anonymization"},{"location":"tutorial/pass/#update-the-google-analytics-system-resource","text":"Now that the data collection practices in the Google Analytics script have changed, the associated fidesctl System resource should be updated accordingly. Open the fides_resources/google_analytics_system.yml file in your favorite editor, and modify the last line (the data_qualifier configuration) so that it reads: 1 data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized By removing the final identified key of the Fides taxonomy, the updated nature of the data collection practices used in this System resource now aligns with the actual behavior of the updated Google Analytics script.","title":"Update the Google Analytics System Resource"},{"location":"tutorial/pass/#evaluate-the-fidesctl-policies","text":"Execute the make fidesctl-evaluate command one final time. You should see the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Evaluating policy with fidesctl... ./venv/bin/fidesctl evaluate --dry fides_resources Loading resource manifests from: fides_resources Taxonomy successfully created. ---------- Processing dataset resources... WOULD CREATE 0 dataset resources. WOULD UPDATE 0 dataset resources. WOULD SKIP 1 dataset resources. ---------- Processing system resources... WOULD CREATE 0 system resources. WOULD UPDATE 1 system resources. WOULD SKIP 1 system resources. ---------- Processing policy resources... WOULD CREATE 0 policy resources. WOULD UPDATE 0 policy resources. WOULD SKIP 1 policy resources. ---------- Loading resource manifests from: fides_resources Taxonomy successfully created. Evaluating the following policies: flaskr_policy ---------- Checking for missing resources... Executing evaluations... Evaluation passed! The fidesctl policy evaluation passes!","title":"Evaluate the Fidesctl Policies"},{"location":"tutorial/pass/#check-your-progress","text":"After making the above changes, your app should resemble the state of the ethyca/fidesdemo repository at the fidesctl-demo tag.","title":"Check Your Progress"},{"location":"tutorial/policy/","text":"Write a Policy Fidesctl's privacy declarations provide rich metadata about systems, the data categories they process, and the uses of that data. Policies allow you to enforce constraints on these declarations and decide what combinations to allow or reject at your company, thus providing a layer of automation to control data privacy at the source. Define a single Policy by creating a flaskr_policy.yml file in the fides_resources directory. For this project, the file should contain the following configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 policy : - fides_key : flaskr_policy name : Flaskr Privacy Policy description : A privacy policy for the example Flask app rules : - fides_key : minimize_user_identifiable_data name : Minimize User Identifiable Data description : Reject collecting any user identifiable data for uses other than system operations data_categories : matches : ANY values : - user.provided.identifiable - user.derived.identifiable data_uses : matches : ANY values : - improve - personalize - advertising - third_party_sharing - collect - train_ai_system data_subjects : matches : ANY values : - customer data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - fides_key : reject_sensitive_data name : Reject Sensitive Data description : Reject collecting sensitive user data for any use data_categories : matches : ANY values : - user.provided.identifiable.biometric - user.provided.identifiable.childrens - user.provided.identifiable.genetic - user.provided.identifiable.health_and_medical - user.provided.identifiable.political_opinion - user.provided.identifiable.race - user.provided.identifiable.religious_belief - user.provided.identifiable.sexual_orientation data_uses : matches : ANY values : - provide - improve - personalize - advertising - third_party_sharing - collect - train_ai_system data_subjects : matches : ANY values : - customer data_qualifier : aggregated This demo application is built without any real controls on user data, so the Fides policy is relatively restrictive. The two rules can be interpreted respectfully as: Do not use identifiable data for anything other than the app's primary functions (after all, it's just a demo app!). Do not collect any sensitive data at all. As a safe default, this is the type of policy you might add to all projects. Later, you can make exceptions (if you are working on a project that requires these categories). Understanding the Policy The purpose of a privacy policy is to state what types of data are allowed for certain means of use. In fidesctl, a Policy is comprised of rules against which the system's privacy declarations are evaluated. Policies will evaluate the data subjects, data category, and data qualifier values against data use cases. This generates a boolean output to either allow or reject the process from proceeding. Policies use the following attributes: Name Type Description fides_key FidesKey An identifier label that must be unique within your organization. A fides_key can only contain alphanumeric characters and _ . data_categories List[DataRule] The types of sensitive data as defined by the taxonomy data_uses List[DataRule] The various categories of data processing and operations within your organization data_subjects List[DataRule] The individual persons to whom you data rule pertains data_qualifier String The acceptable or non-acceptable level of deidentification For more detail on Policy resources, see the full Policy resource documentation . Maintaining a Policy As global privacy laws change and businesses scale, a company's policies will evolve with them. We recommend that updating this resource file becomes a regular part of the development planning process when building a new feature. Check Your Progress After making the above changes and the changes in the previous two steps, your app should resemble the state of the ethyca/fidesdemo repository at the fidesctl-manifests tag . Next: Add Google Analytics Improve usage telemetry for this project by adding the nefarious tracker, Google Analytics .","title":"Write a Policy"},{"location":"tutorial/policy/#write-a-policy","text":"Fidesctl's privacy declarations provide rich metadata about systems, the data categories they process, and the uses of that data. Policies allow you to enforce constraints on these declarations and decide what combinations to allow or reject at your company, thus providing a layer of automation to control data privacy at the source. Define a single Policy by creating a flaskr_policy.yml file in the fides_resources directory. For this project, the file should contain the following configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 policy : - fides_key : flaskr_policy name : Flaskr Privacy Policy description : A privacy policy for the example Flask app rules : - fides_key : minimize_user_identifiable_data name : Minimize User Identifiable Data description : Reject collecting any user identifiable data for uses other than system operations data_categories : matches : ANY values : - user.provided.identifiable - user.derived.identifiable data_uses : matches : ANY values : - improve - personalize - advertising - third_party_sharing - collect - train_ai_system data_subjects : matches : ANY values : - customer data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified - fides_key : reject_sensitive_data name : Reject Sensitive Data description : Reject collecting sensitive user data for any use data_categories : matches : ANY values : - user.provided.identifiable.biometric - user.provided.identifiable.childrens - user.provided.identifiable.genetic - user.provided.identifiable.health_and_medical - user.provided.identifiable.political_opinion - user.provided.identifiable.race - user.provided.identifiable.religious_belief - user.provided.identifiable.sexual_orientation data_uses : matches : ANY values : - provide - improve - personalize - advertising - third_party_sharing - collect - train_ai_system data_subjects : matches : ANY values : - customer data_qualifier : aggregated This demo application is built without any real controls on user data, so the Fides policy is relatively restrictive. The two rules can be interpreted respectfully as: Do not use identifiable data for anything other than the app's primary functions (after all, it's just a demo app!). Do not collect any sensitive data at all. As a safe default, this is the type of policy you might add to all projects. Later, you can make exceptions (if you are working on a project that requires these categories).","title":"Write a Policy"},{"location":"tutorial/policy/#understanding-the-policy","text":"The purpose of a privacy policy is to state what types of data are allowed for certain means of use. In fidesctl, a Policy is comprised of rules against which the system's privacy declarations are evaluated. Policies will evaluate the data subjects, data category, and data qualifier values against data use cases. This generates a boolean output to either allow or reject the process from proceeding. Policies use the following attributes: Name Type Description fides_key FidesKey An identifier label that must be unique within your organization. A fides_key can only contain alphanumeric characters and _ . data_categories List[DataRule] The types of sensitive data as defined by the taxonomy data_uses List[DataRule] The various categories of data processing and operations within your organization data_subjects List[DataRule] The individual persons to whom you data rule pertains data_qualifier String The acceptable or non-acceptable level of deidentification For more detail on Policy resources, see the full Policy resource documentation .","title":"Understanding the Policy"},{"location":"tutorial/policy/#maintaining-a-policy","text":"As global privacy laws change and businesses scale, a company's policies will evolve with them. We recommend that updating this resource file becomes a regular part of the development planning process when building a new feature.","title":"Maintaining a Policy"},{"location":"tutorial/policy/#check-your-progress","text":"After making the above changes and the changes in the previous two steps, your app should resemble the state of the ethyca/fidesdemo repository at the fidesctl-manifests tag .","title":"Check Your Progress"},{"location":"tutorial/policy/#next-add-google-analytics","text":"Improve usage telemetry for this project by adding the nefarious tracker, Google Analytics .","title":"Next: Add Google Analytics"},{"location":"tutorial/system/","text":"Annotate the System Now that you've built out the underlying database that describes how and what type of data is stored, include the database in application-level \"systems\", another critical fidesctl resource. This app contains a single Flaskr Web Application system resource. Create a system resource to annotate it by adding a flaskr_system.yml file to the fides_resources directory. Writing a Fides privacy declaration requires answering the questions: \"What data is this system processing?\" , \"Why is the system processing this data?\" , \"Whose data is involved?\" , and \"How is the data protected?\" Fides answers these questions with a privacy_declaration that describes the data categories, use, subjects, and qualifier. This application is quite simple, so it only has a single use: to provide the service to users. In order to do so, it requires some identifiable data (name, email, contact, etc.) and it derives some data as well (unique IDs). For this project, the file should contain the following configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 system : - fides_key : flaskr_system name : Flaskr Web Application description : An example Flask web app that simulates an e-commerce application system_type : Application privacy_declarations : - name : Provide e-commerce operations to example customers data_categories : - user.provided.identifiable - user.derived.identifiable - system.operations data_use : provide.system.operations data_subjects : - customer data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified dataset_references : - flaskr_postgres_dataset Privacy Declarations can be read colloquially as \"This system uses sensitive data types of data_categories for data_subjects with the purpose of data_use at a deidentification level of data_qualifier \". In a production app, create as many systems as are necessary to cover all relevant business applications. Understanding Systems In fidesctl, Systems are used to model things that process data for organizations (applications, services, 3rd party APIs, etc.) and describe how these datasets are used for business functions. These groupings are not mutually exclusive; they answer \" How and why are these datasets being used? \" The System resource groups the lowest level of data (your datasets) with your business use cases, and associates qualitative attributes describing the type of data being used. Systems use the following attributes: Name Type Description data_categories List[FidesKey] The types of sensitive data as defined by the taxonomy data_subjects List[FidesKey] The individual persons whose data resides in your datasets data_use List[FidesKey] The various categories of data processing and operations within your organization data_qualifier List[FidesKey] The level of deidentification for the dataset dataset_refereneces List[FidesKey] The fides_key (s) of the dataset fields used in this Privacy Declaration For more detail on System resources, see the full System resource documentation . Maintaining a System Resource As use cases evolve, your systems' data subjects, data categories, and data uses will change as well. We recommend that updating this resource file becomes a regular part of the development planning process when building a new feature. PRO TIP As more systems are added to a data ecosystem, consider grouping systems into another Fides resource type, called a Registry . Next: Write a Policy With database and system resources declared, you must now enforce your data constraints by writing a Policy .","title":"Annotate the System"},{"location":"tutorial/system/#annotate-the-system","text":"Now that you've built out the underlying database that describes how and what type of data is stored, include the database in application-level \"systems\", another critical fidesctl resource. This app contains a single Flaskr Web Application system resource. Create a system resource to annotate it by adding a flaskr_system.yml file to the fides_resources directory. Writing a Fides privacy declaration requires answering the questions: \"What data is this system processing?\" , \"Why is the system processing this data?\" , \"Whose data is involved?\" , and \"How is the data protected?\" Fides answers these questions with a privacy_declaration that describes the data categories, use, subjects, and qualifier. This application is quite simple, so it only has a single use: to provide the service to users. In order to do so, it requires some identifiable data (name, email, contact, etc.) and it derives some data as well (unique IDs). For this project, the file should contain the following configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 system : - fides_key : flaskr_system name : Flaskr Web Application description : An example Flask web app that simulates an e-commerce application system_type : Application privacy_declarations : - name : Provide e-commerce operations to example customers data_categories : - user.provided.identifiable - user.derived.identifiable - system.operations data_use : provide.system.operations data_subjects : - customer data_qualifier : aggregated.anonymized.unlinked_pseudonymized.pseudonymized.identified dataset_references : - flaskr_postgres_dataset Privacy Declarations can be read colloquially as \"This system uses sensitive data types of data_categories for data_subjects with the purpose of data_use at a deidentification level of data_qualifier \". In a production app, create as many systems as are necessary to cover all relevant business applications.","title":"Annotate the System"},{"location":"tutorial/system/#understanding-systems","text":"In fidesctl, Systems are used to model things that process data for organizations (applications, services, 3rd party APIs, etc.) and describe how these datasets are used for business functions. These groupings are not mutually exclusive; they answer \" How and why are these datasets being used? \" The System resource groups the lowest level of data (your datasets) with your business use cases, and associates qualitative attributes describing the type of data being used. Systems use the following attributes: Name Type Description data_categories List[FidesKey] The types of sensitive data as defined by the taxonomy data_subjects List[FidesKey] The individual persons whose data resides in your datasets data_use List[FidesKey] The various categories of data processing and operations within your organization data_qualifier List[FidesKey] The level of deidentification for the dataset dataset_refereneces List[FidesKey] The fides_key (s) of the dataset fields used in this Privacy Declaration For more detail on System resources, see the full System resource documentation .","title":"Understanding Systems"},{"location":"tutorial/system/#maintaining-a-system-resource","text":"As use cases evolve, your systems' data subjects, data categories, and data uses will change as well. We recommend that updating this resource file becomes a regular part of the development planning process when building a new feature. PRO TIP As more systems are added to a data ecosystem, consider grouping systems into another Fides resource type, called a Registry .","title":"Maintaining a System Resource"},{"location":"tutorial/system/#next-write-a-policy","text":"With database and system resources declared, you must now enforce your data constraints by writing a Policy .","title":"Next: Write a Policy"}]}